{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNc5r1HYEmVSdjWcZiqZo91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/VQ_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "869d7522-1e4e-4cdd-9a26-df8e6e0bd4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 64.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ghQ8RSExs_A4",
        "outputId": "d536cf35-abec-463f-d067-56b743bc18f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY\n",
            "From (redirected): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY&confirm=t&uuid=30279fa9-2144-4354-926d-0f7b1b4ad1cb\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:19<00:00, 37.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9UKkkuorG_b9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 # 128 512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title chatgpt quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_emb, emb_dim, beta=0.5, decay=0.99):\n",
        "        super().__init__()\n",
        "        self.num_emb, self.emb_dim = num_emb, emb_dim\n",
        "        self.beta, self.decay = beta, decay\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-3**.5, 3**.5))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-1./num_emb, 1./num_emb))\n",
        "\n",
        "        # Register buffers for EMA updates.\n",
        "        self.register_buffer('ema_cluster_size', torch.zeros(num_emb))\n",
        "        self.register_buffer('ema_w', self.embeddings.data.clone())\n",
        "        # self.ema_cluster_size = nn.Parameter(torch.zeros(num_emb), requires_grad=False)\n",
        "        # self.ema_w = nn.Parameter(self.embeddings.data.clone(), requires_grad=False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Save the original shape and flatten the input to (batch_size * ..., emb_dim)\n",
        "        input_shape = z.shape\n",
        "        # flat_z = z.view(-1, self.emb_dim)\n",
        "\n",
        "        # flat_z = z.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "        # print(flat_z.shape)\n",
        "\n",
        "        # distances = (torch.sum(flat_z ** 2, dim=1, keepdim=True) + torch.sum(self.embeddings ** 2, dim=1) - 2 * torch.matmul(flat_z, self.embeddings.t()))\n",
        "        distances = (torch.sum(flat_z**2, dim=1, keepdim=True) + torch.sum(self.embeddings**2, dim=1) - 2*torch.matmul(flat_z, self.embeddings.T))\n",
        "        enc_ind = torch.argmin(distances, dim=1)\n",
        "        encs = F.one_hot(enc_ind, self.num_emb).type(flat_z.dtype)\n",
        "        # Quantise the input by replacing with the nearest embedding.\n",
        "        z_q = torch.matmul(encs, self.embeddings).view(input_shape)\n",
        "\n",
        "        commitment_loss = self.beta * F.mse_loss(z_q.detach(), z) # commitment loss.\n",
        "        # loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q-z.detach())**2) # comitment, codebook\n",
        "\n",
        "        if self.training:\n",
        "            # EMA update for cluster size.\n",
        "            ema_cluster_size = torch.sum(encs, dim=0)\n",
        "            self.ema_cluster_size = self.ema_cluster_size * self.decay + (1 - self.decay) * ema_cluster_size\n",
        "            # Laplace smoothing to avoid zero counts.\n",
        "            n = torch.sum(self.ema_cluster_size)\n",
        "            self.ema_cluster_size = ((self.ema_cluster_size + self.epsilon) / (n + self.num_emb * self.epsilon)) * n\n",
        "\n",
        "            # EMA update for the embedding weights.\n",
        "            # dw = torch.matmul(encs.t(), flat_z)\n",
        "            dw = encs.T @ flat_z.detach()\n",
        "            # print('vq fwd', encs.T, flat_z)\n",
        "            self.ema_w = self.ema_w * self.decay + (1 - self.decay) * dw\n",
        "\n",
        "            # Update embeddings with the EMA values.\n",
        "            self.embeddings.data = self.ema_w / self.ema_cluster_size.unsqueeze(1)\n",
        "\n",
        "        # # Compute perplexity of the encs.\n",
        "        # avg_probs = torch.mean(encs, dim=0)\n",
        "        # perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + self.epsilon)))\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # print(z_q.shape)\n",
        "        # flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "\n",
        "        # z_q = z_q.transpose(1,2).reshape(*bchw)\n",
        "        # return loss, z_q, perplexity, enc_ind\n",
        "        return commitment_loss, z_q, enc_ind\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "x = torch.randn(2, emb_dim, 5, 7)\n",
        "vq = VectorQuantizerEMA(num_emb, emb_dim, beta=0.5) # chat gpt\n",
        "loss, z_q, enc_ind = vq(x)\n",
        "print(z_q.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yfLogBYztcuM",
        "outputId": "38699279-ef8a-43b4-a68f-3a2dfb8babca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 5, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "fsq = FSQ(levels = [5,4,3,2])\n",
        "# print(fsq.codebook)\n",
        "batch_size, seq_len = 2, 4\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "x = torch.linspace(-2,2,7, device=device).repeat(4,1).T\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n",
        "# la = fsq.indexes_to_codes(lact)\n",
        "# print(la)\n"
      ],
      "metadata": {
        "id": "Mje-yFj88WlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d932a124-0c9f-4ba0-e0d1-291364eb1f42",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
            "        [-0.5000, -0.3333, -1.0000, -1.0000],\n",
            "        [-0.5000, -0.3333,  0.0000, -1.0000],\n",
            "        [ 0.0000, -0.3333,  0.0000, -1.0000],\n",
            "        [ 0.5000,  0.3333,  0.0000,  1.0000],\n",
            "        [ 0.5000,  0.3333,  1.0000,  1.0000],\n",
            "        [ 1.0000,  1.0000,  1.0000,  1.0000]], device='cuda:0')\n",
            "tensor([  0.,  30.,  32.,  56.,  87.,  89., 119.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title vqvae me\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=32, z_dim=3):\n",
        "        super().__init__()\n",
        "        d_list=[16, 3] # 849126\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        kernel = 3\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, 16, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 14\n",
        "            nn.Conv2d(in_ch, d_model, kernel, 2, kernel//2), nn.BatchNorm2d(d_model), act,# nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(d_model, z_dim, kernel, 2, kernel//2), act\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, d_model, kernel, stride=2, padding=kernel//2, output_padding=1), nn.BatchNorm2d(d_model), act,\n",
        "            # nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(d_model, in_ch, kernel, 2, padding=kernel//2, output_padding=1)\n",
        "        )\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=20, emb_dim=z_dim, beta=0.5) # chat gpt\n",
        "        self.vq = FSQ(levels = z_dim*[8])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        commitment_loss, quantised, encoding_indices = self.vq(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(quantised)\n",
        "        return x, commitment_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.quantise(x)\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "d_model=32\n",
        "z_dim=3\n",
        "model = VQVAE(in_ch, d_model, z_dim).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "\n",
        "x = torch.randn((2, in_ch, 64, 64), device=device)\n",
        "# out, _ = model(x)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iRZMIxPsMTV",
        "outputId": "81c91189-1eff-4004-e14f-a78a10199a20",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3654\n",
            "torch.Size([2, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title res & attn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.lin(out) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.self(self.norm1(x))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        return x.transpose(1,2).reshape(*bchw)\n",
        "\n",
        "\n",
        "# class GLUMBConv(nn.Module):\n",
        "#     def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "#         super().__init__()\n",
        "#         mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "#         self.inverted_depth_conv = nn.Sequential(\n",
        "#             nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "#             nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "#         )\n",
        "#         self.point_conv = nn.Sequential(\n",
        "#             nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch), # \"ln2d\"\n",
        "#         )\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.inverted_depth_conv(x)\n",
        "#         x, gate = torch.chunk(x, 2, dim=1)\n",
        "#         x = x * nn.SiLU()(gate)\n",
        "#         x = self.point_conv(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# model = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = model(x, cond, mask)\n",
        "# print(out.shape)\n",
        "# # print(out)\n",
        "\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        # self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)))\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CtmsyKatYT7h"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title encoder me\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=8192, emb_dim=out_ch, beta=0.5) # chat gpt\n",
        "        # self.vq = FSQ(levels = z_dim*[32])\n",
        "\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # nn.Conv2d(in_ch, ch_list[0], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(in_ch, ch_list[0]),\n",
        "            # ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[1], ch_list[1]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[0]*2**2, ch_list[1]),\n",
        "            nn.AvgPool2d(2,2),\n",
        "            # nn.MaxPool2d(2,2),\n",
        "            ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[2], ch_list[2]),\n",
        "            AttentionBlock(ch_list[1], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[2]),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[2]*2**2, ch_list[3]),\n",
        "            nn.AvgPool2d(2,2),\n",
        "            ResBlock(ch_list[2], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            ResBlock(ch_list[3], out_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # nn.Conv2d(out_ch, ch_list[-1], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(out_ch, ch_list[3]),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            ResBlock(ch_list[3], ch_list[2]),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            # ResBlock(ch_list[3], ch_list[2]*2**2),\n",
        "            # # AttentionBlock(ch_list[2]*2**2, d_head),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[1]),\n",
        "            AttentionBlock(ch_list[2], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[0]),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            # ResBlock(ch_list[1], ch_list[0]*2**2),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[1], ch_list[0]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "            ResBlock(ch_list[0], in_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[0]), nn.SiLU(), nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[0]), nn.SiLU(), zero_module(nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)) # zero\n",
        "        )\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     # print(x.shape)\n",
        "    #     commitment_loss, x, _ = self.vq(x)\n",
        "    #     # print(x.shape)\n",
        "    #     x = self.decoder(x)\n",
        "    #     return x, commitment_loss\n",
        "\n",
        "    # def encode(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return x\n",
        "\n",
        "    # def decode(self, x):\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # return self.quantise(x)\n",
        "        return x\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        # x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "\n",
        "batch=2\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "# out, _ = model(x)\n",
        "# print(out.shape)\n",
        "# x = model.quantise(x)\n",
        "# print(x.shape)\n",
        "# print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuXDKUIACl91",
        "outputId": "f131a025-0153-4661-c4e7-9b7ff3986bf4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        if out_ch//in_ch > 1: self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1: self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        elif in_ch==out_ch: self.func = lambda x: x\n",
        "    def forward(self, x): return self.func(x) # [b,c,h,w] -> [b,o,h,w]\n",
        "\n",
        "class PixelShortcut(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "    def forward(self, x): return self.net(x) # [b,c,h,w] -> [b,o,r*h,r*w]\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=r)\n",
        "    def forward(self, x):\n",
        "        return self.block(x) + self.shortcut_block(x)\n"
      ],
      "metadata": {
        "id": "JYMQDoL578HQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "conv = nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2)\n",
        "print(conv.weight.data.shape)\n"
      ],
      "metadata": {
        "id": "E-VeKdVamz2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # encoder mult=[1,2,4,4,8,8] # depth_list=[0,4,8,2,2,2]\n",
        "        # decoder mult=[1,2,4,4,8,8] # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            UpDownBlock(in_ch, width_list[0], r=1/2),\n",
        "            ResBlock(width_list[0]),\n",
        "            UpDownBlock(width_list[0], width_list[-1], r=1/2),\n",
        "            AttentionBlock(width_list[-1], d_head=4),\n",
        "            UpDownBlock(width_list[-1], out_ch, r=1),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            UpDownBlock(in_ch, width_list[-1], r=1),\n",
        "            AttentionBlock(width_list[-1], d_head=4),\n",
        "            UpDownBlock(width_list[-1], width_list[0], r=2),\n",
        "            ResBlock(width_list[0]),\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpDownBlock(width_list[0], out_ch, r=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea2ba66-7aa1-4ca8-bee0-954a059bee5a",
        "id": "y7CtfbCdIpCG"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83655\n",
            "torch.Size([2, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def train(model, optim, dataloader, scheduler=None):\n",
        "    model.train()\n",
        "    # for i, (x, _) in enumerate(dataloader):\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x_, commitment_loss = model(x)\n",
        "            # loss = commitment_loss + F.mse_loss(x, x_)\n",
        "\n",
        "            x_ = model(x)\n",
        "            loss = F.mse_loss(x, x_)\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if i%10 == 0:\n",
        "        #     with torch.no_grad():\n",
        "        #         state = buffer[12][40][0]\n",
        "        #         transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        #         # transform = transforms.Compose([transforms.ToTensor()])\n",
        "        #         x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "        #         out = model(x).squeeze(0)\n",
        "        #         sx = model.encode(x).squeeze(0)\n",
        "        #         out = model.decode(sx).squeeze(0)\n",
        "        #         imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "        #         imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if i % 10 == 0: print(loss.item())\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # z = torch.randn(1,z_dim,8,8).to(device)\n",
        "        # _, z, _ = model.vq(z)\n",
        "        # z = model.quantise(z)\n",
        "\n",
        "        # out = model.decode(z)\n",
        "        # imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        state = buffer[12][40][0]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        # transform = transforms.Compose([transforms.ToTensor()])\n",
        "        x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "\n",
        "        # out, _ = model(x)\n",
        "        out = model(x)\n",
        "        imshow(torchvision.utils.make_grid(x.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(out.cpu()))\n"
      ],
      "metadata": {
        "id": "rJQDMix9EpYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "0ccc8835-52ec-4435-ebbb-cd6d5bba59c7",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d591651dfc51>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.randn(1,z_dim,16,16).to(device)\n",
        "with torch.no_grad(): out = model.decode(z)\n",
        "imshow(torchvision.utils.make_grid(out))\n",
        "\n",
        "\n",
        "# out, _ = model(x)\n",
        "# imshow(torchvision.utils.make_grid(out))"
      ],
      "metadata": {
        "id": "lXLAI-bVQ6G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "kI3uaeX-r73O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title vqvae from CompVis\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25, remap=remap, sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self, ddconfig, lossconfig, embed_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "psJQyxGNkOlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title encoder from CompVis\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/cin-ldm-vq-f8.yaml\n",
        "# ddconfig:\n",
        "#     double_z: false\n",
        "#     z_channels: 4\n",
        "#     resolution: 256\n",
        "#     in_channels: 3\n",
        "#     out_ch: 3\n",
        "#     ch: 128\n",
        "#     ch_mult: 1,2,3,4\n",
        "#     num_res_blocks: 2\n",
        "#     attn_resolutions: 32\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/ffhq-ldm-vq-4.yaml\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/celebahq-ldm-vq-4.yaml\n",
        "# embed_dim: 3\n",
        "# n_embed: 8192 = 2^13 ~ 16^3\n",
        "# ddconfig:\n",
        "#   double_z: false\n",
        "#   z_channels: 3\n",
        "#   resolution: 256\n",
        "#   in_channels: 3\n",
        "#   out_ch: 3\n",
        "#   ch: 128\n",
        "#   ch_mult: 1,2,4\n",
        "#   num_res_blocks: 2\n",
        "#   attn_resolutions: []\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch=128, out_ch=3, ch_mult=(1,2,3,4), num_res_blocks=2,\n",
        "                 attn_resolutions=32, dropout=0.0, resamp_with_conv=True, in_channels=3,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult) # 1,1,2,3,4\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level] # 128 * 1,1,2,3,4\n",
        "            block_out = ch*ch_mult[i_level] # 128 * 1,2,3,4\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1: # downsample at all except last\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "encoder:\n",
        "convin k3s1\n",
        "res attn (),\n",
        "norm act convout k3s1\n",
        "\n",
        "self.conv_in = torch.nn.Conv2d(in_channels, self.ch, 3, 1, padding=3//2)\n",
        "\n",
        "res res down res res down\n",
        "res att res\n",
        "\n",
        "self.conv_out = nn.Sequential(\n",
        "    nn.GroupNorm(32, ch), nn.SiLU(), torch.nn.Conv2d(block_in, z_channels, 3, 1, padding=3//2)\n",
        ")\n",
        "\n",
        "me: down res res down res res\n",
        "down res att down res att = lvl lvl\n",
        "\n",
        "decoder:\n",
        "conv_in\n",
        "res att res\n",
        "res res up res res up\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XgDIoz8Jm8B6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CompVis model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nWb4L-uhmL4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# airalcorn2\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema: dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else: dictionary_loss = None\n",
        "\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "# rosinality\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "# CompVis\n",
        "from einops import rearrange\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "# x = torch.randn(2, 3, 4, 4)\n",
        "# vq = VectorQuantizer(emb_dim, num_emb, use_ema, decay)\n",
        "# vq = Quantize(emb_dim, num_emb, decay=0.99, eps=1e-5)\n",
        "vq = VectorQuantizer2(num_emb, emb_dim, beta=0.5, sane_index_shape=False) # CompVis\n",
        "out = vq(x)\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5bYiW-RnBdAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5)\n",
        "        shift = torch.tan(offset / half_l)\n",
        "        return torch.tanh(z + shift) * half_l - offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "print(fsq.codebook)\n",
        "\n",
        "batch_size, seq_len = 1, 1\n",
        "x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6gGjNCKog8za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ac9ebe-1cdf-44a0-c47b-f2db2aa9f497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1., -1., -1.],\n",
            "        [ 0., -1., -1.],\n",
            "        [ 1., -1., -1.],\n",
            "        [-1.,  0., -1.],\n",
            "        [ 0.,  0., -1.],\n",
            "        [ 1.,  0., -1.],\n",
            "        [-1.,  1., -1.],\n",
            "        [ 0.,  1., -1.],\n",
            "        [ 1.,  1., -1.],\n",
            "        [-1., -1.,  0.],\n",
            "        [ 0., -1.,  0.],\n",
            "        [ 1., -1.,  0.],\n",
            "        [-1.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.],\n",
            "        [ 1.,  0.,  0.],\n",
            "        [-1.,  1.,  0.],\n",
            "        [ 0.,  1.,  0.],\n",
            "        [ 1.,  1.,  0.]], device='cuda:0')\n",
            "tensor([[[0., 1., 0.]]], device='cuda:0')\n",
            "tensor([[16]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tried no round, LogitNormalCDF\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "# class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "#     def __init__(self, levels):\n",
        "#         super().__init__()\n",
        "#         self.eps = eps\n",
        "#         self.levels = torch.tensor(levels, device=device)\n",
        "#         # level in levels\n",
        "\n",
        "    # linear, normal,\n",
        "    # center = LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3)\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     ind = torch.argmin((z-center).abs())\n",
        "\n",
        "\n",
        "    # threshold = LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=3)[1:-1]\n",
        "\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     center[ind]\n",
        "\n",
        "\n",
        "# linear, normal,\n",
        "\n",
        "center = [LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3) for level in levels]\n",
        "# print(center)\n",
        "# def forward(self, z):\n",
        "z = torch.linspace(-2,2,7).repeat(3,1).T\n",
        "\n",
        "z = F.sigmoid(z)\n",
        "# ind = [torch.argmin((z-c).abs()) for c in center]\n",
        "# print(ind)\n",
        "\n",
        "threshold = [LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=1)[1:-1] for level in levels]\n",
        "print(threshold)\n",
        "\n",
        "def get_vjp(v):\n",
        "    return torch.autograd.grad(y, x, v)\n",
        "out = torch.vmap(get_vjp)(I_N)\n",
        "\n",
        "\n",
        "# def forward(self, z):\n",
        "#     z = F.sigmoid(z)\n",
        "#     center[ind]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bhCqoI0_fX1B",
        "outputId": "bb25265f-3355-4888-a4e0-1c0a3385c3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([0.0828, 0.3426, 0.6574, 0.9172]), tensor([0.2441, 0.7559]), tensor([0.5000])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title lucidrains vector_quantize_pytorch.py GroupedResidualVQ\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/vector_quantize_pytorch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from math import ceil\n",
        "from functools import partial, cache\n",
        "from itertools import zip_longest\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import Module, ModuleList\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "\n",
        "from einx import get_at\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def first(it):\n",
        "    return it[0]\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_tuple(t, length = 1):\n",
        "    return t if isinstance(t, tuple) else ((t,) * length)\n",
        "\n",
        "def unique(arr):\n",
        "    return list({*arr})\n",
        "\n",
        "def round_up_multiple(num, mult):\n",
        "    return ceil(num / mult) * mult\n",
        "\n",
        "# distributed helpers\n",
        "\n",
        "def is_distributed():\n",
        "    return dist.is_initialized() and dist.get_world_size() > 1\n",
        "\n",
        "def get_maybe_sync_seed(device, max_size = 10_000):\n",
        "    rand_int = torch.randint(0, max_size, (), device = device)\n",
        "\n",
        "    if is_distributed():\n",
        "        dist.all_reduce(rand_int)\n",
        "\n",
        "    return rand_int.item()\n",
        "\n",
        "# the mlp for generating the neural implicit codebook\n",
        "# from Huijben et al. https://arxiv.org/abs/2401.14732\n",
        "\n",
        "class MLP(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_hidden = None,\n",
        "        depth = 4,             # they used 4 layers in the paper\n",
        "        l2norm_output = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_hidden = default(dim_hidden, dim)\n",
        "\n",
        "        self.proj_in = nn.Linear(2 * dim, dim)\n",
        "\n",
        "        layers = ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Linear(dim, dim_hidden),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(dim_hidden, dim)\n",
        "            ))\n",
        "\n",
        "        self.layers = layers\n",
        "        self.l2norm_output = l2norm_output\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        codes,\n",
        "        *,\n",
        "        condition\n",
        "    ):\n",
        "        one_headed = codes.ndim == 2\n",
        "\n",
        "        if one_headed:\n",
        "            codes = rearrange(codes, 'c d -> 1 c d')\n",
        "\n",
        "        heads, num_codes, batch, seq_len = codes.shape[0], codes.shape[-2], condition.shape[0], condition.shape[-2]\n",
        "\n",
        "        codes = repeat(codes, 'h c d -> h b n c d', n = seq_len, b = batch)\n",
        "        condition = repeat(condition, 'b n d -> h b n c d', c = num_codes, h = heads)\n",
        "\n",
        "        x = torch.cat((condition, codes), dim = -1)\n",
        "        x = self.proj_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x) + x\n",
        "\n",
        "        if self.l2norm_output:\n",
        "            x = F.normalize(x, dim = -1)\n",
        "\n",
        "        if not one_headed:\n",
        "            return x\n",
        "\n",
        "        return rearrange(x, '1 ... -> ...')\n",
        "\n",
        "# main class\n",
        "\n",
        "class ResidualVQ(Module):\n",
        "    \"\"\" Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_quantizers: int | None = None,\n",
        "        codebook_size: int | tuple[int, ...],\n",
        "        codebook_dim = None,\n",
        "        shared_codebook = False,\n",
        "        heads = 1,\n",
        "        quantize_dropout = False,\n",
        "        quantize_dropout_cutoff_index = 0,\n",
        "        quantize_dropout_multiple_of = 1,\n",
        "        accept_image_fmap = False,\n",
        "        implicit_neural_codebook = False, # QINCo from https://arxiv.org/abs/2401.14732\n",
        "        mlp_kwargs: dict = dict(),\n",
        "        **vq_kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert heads == 1, 'residual vq is not compatible with multi-headed codes'\n",
        "        assert exists(num_quantizers) or isinstance(codebook_size, tuple)\n",
        "\n",
        "        codebook_dim = default(codebook_dim, dim)\n",
        "        codebook_input_dim = codebook_dim * heads\n",
        "\n",
        "        requires_projection = codebook_input_dim != dim\n",
        "        self.project_in = nn.Linear(dim, codebook_input_dim) if requires_projection else nn.Identity()\n",
        "        self.project_out = nn.Linear(codebook_input_dim, dim) if requires_projection else nn.Identity()\n",
        "        self.has_projections = requires_projection\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.implicit_neural_codebook = implicit_neural_codebook\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            vq_kwargs.update(\n",
        "                learnable_codebook = True,\n",
        "                ema_update = False\n",
        "            )\n",
        "\n",
        "        if shared_codebook:\n",
        "            vq_kwargs.update(\n",
        "                manual_ema_update = True,\n",
        "                manual_in_place_optimizer_update = True\n",
        "            )\n",
        "\n",
        "        # take care of maybe different codebook sizes across depth\n",
        "\n",
        "        codebook_sizes = cast_tuple(codebook_size, num_quantizers)\n",
        "\n",
        "        num_quantizers = default(num_quantizers, len(codebook_sizes))\n",
        "        assert len(codebook_sizes) == num_quantizers\n",
        "\n",
        "        self.num_quantizers = num_quantizers\n",
        "\n",
        "        self.codebook_sizes = codebook_sizes\n",
        "        self.uniform_codebook_size = len(unique(codebook_sizes)) == 1\n",
        "\n",
        "        # define vq across layers\n",
        "\n",
        "        self.layers = ModuleList([VectorQuantize(dim = codebook_dim, codebook_size = layer_codebook_size, codebook_dim = codebook_dim, accept_image_fmap = accept_image_fmap, **vq_kwargs) for layer_codebook_size in codebook_sizes])\n",
        "\n",
        "        assert all([not vq.has_projections for vq in self.layers])\n",
        "\n",
        "        self.quantize_dropout = quantize_dropout and num_quantizers > 1\n",
        "\n",
        "        assert quantize_dropout_cutoff_index >= 0\n",
        "\n",
        "        self.quantize_dropout_cutoff_index = quantize_dropout_cutoff_index\n",
        "        self.quantize_dropout_multiple_of = quantize_dropout_multiple_of  # encodec paper proposes structured dropout, believe this was set to 4\n",
        "\n",
        "        # setting up the MLPs for implicit neural codebooks\n",
        "\n",
        "        self.mlps = None\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            self.mlps = ModuleList([MLP(dim = codebook_dim, l2norm_output = first(self.layers).use_cosine_sim, **mlp_kwargs) for _ in range(num_quantizers - 1)])\n",
        "        else:\n",
        "            self.mlps = (None,) * (num_quantizers - 1)\n",
        "\n",
        "        # sharing codebook logic\n",
        "\n",
        "        self.shared_codebook = shared_codebook\n",
        "\n",
        "        if not shared_codebook:\n",
        "            return\n",
        "\n",
        "        assert self.uniform_codebook_size\n",
        "\n",
        "        first_vq, *rest_vq = self.layers\n",
        "        codebook = first_vq._codebook\n",
        "\n",
        "        for vq in rest_vq:\n",
        "            vq._codebook = codebook\n",
        "\n",
        "    @property\n",
        "    def codebook_size(self):\n",
        "        return self.layers[0].codebook_size\n",
        "\n",
        "    @property\n",
        "    def codebook_dim(self):\n",
        "        return self.layers[0].codebook_dim\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        codebooks = [layer._codebook.embed for layer in self.layers]\n",
        "\n",
        "        codebooks = tuple(rearrange(codebook, '1 ... -> ...') for codebook in codebooks)\n",
        "\n",
        "        if not self.uniform_codebook_size:\n",
        "            return codebooks\n",
        "\n",
        "        codebooks = torch.stack(codebooks)\n",
        "        return codebooks\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "\n",
        "        batch, quantize_dim = indices.shape[0], indices.shape[-1]\n",
        "\n",
        "        # may also receive indices in the shape of 'b h w q' (accept_image_fmap)\n",
        "\n",
        "        indices, ps = pack([indices], 'b * q')\n",
        "\n",
        "        # because of quantize dropout, one can pass in indices that are coarse\n",
        "        # and the network should be able to reconstruct\n",
        "\n",
        "        if quantize_dim < self.num_quantizers:\n",
        "            assert self.quantize_dropout > 0., 'quantize dropout must be greater than 0 if you wish to reconstruct from a signal with less fine quantizations'\n",
        "            indices = F.pad(indices, (0, self.num_quantizers - quantize_dim), value = -1)\n",
        "\n",
        "        # take care of quantizer dropout\n",
        "\n",
        "        mask = indices == -1.\n",
        "        indices = indices.masked_fill(mask, 0) # have it fetch a dummy code to be masked out later\n",
        "\n",
        "        if not self.implicit_neural_codebook and self.uniform_codebook_size:\n",
        "\n",
        "            all_codes = get_at('q [c] d, b n q -> q b n d', self.codebooks, indices)\n",
        "\n",
        "        else:\n",
        "            # else if using implicit neural codebook, or non uniform codebook sizes, codes will need to be derived layer by layer\n",
        "\n",
        "            code_transform_mlps = (None, *self.mlps)\n",
        "\n",
        "            all_codes = []\n",
        "            quantized_out = 0.\n",
        "\n",
        "            for codes, indices, maybe_transform_mlp in zip(self.codebooks, indices.unbind(dim = -1), code_transform_mlps):\n",
        "\n",
        "                if exists(maybe_transform_mlp):\n",
        "                    codes = maybe_transform_mlp(codes, condition = quantized_out)\n",
        "                    layer_codes = get_at('b n [c] d, b n -> b n d', codes, indices)\n",
        "                else:\n",
        "                    layer_codes = get_at('[c] d, b n -> b n d', codes, indices)\n",
        "\n",
        "                all_codes.append(layer_codes)\n",
        "                quantized_out += layer_codes\n",
        "\n",
        "            all_codes = torch.stack(all_codes)\n",
        "\n",
        "        # mask out any codes that were dropout-ed\n",
        "\n",
        "        all_codes = all_codes.masked_fill(rearrange(mask, 'b n q -> q b n 1'), 0.)\n",
        "\n",
        "        # if (accept_image_fmap = True) then return shape (quantize, batch, height, width, dimension)\n",
        "\n",
        "        all_codes, = unpack(all_codes, ps, 'q b * d')\n",
        "\n",
        "        return all_codes\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        codes = self.get_codes_from_indices(indices)\n",
        "        codes_summed = reduce(codes, 'q ... -> ...', 'sum')\n",
        "        return self.project_out(codes_summed)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        mask = None,\n",
        "        indices: Tensor | list[Tensor] | None = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        rand_quantize_dropout_fixed_seed = None\n",
        "    ):\n",
        "        num_quant, quant_dropout_multiple_of, return_loss, device = self.num_quantizers, self.quantize_dropout_multiple_of, exists(indices), x.device\n",
        "\n",
        "        x = self.project_in(x)\n",
        "\n",
        "        assert not (self.accept_image_fmap and exists(indices))\n",
        "\n",
        "        quantized_out = 0.\n",
        "        residual = x\n",
        "\n",
        "        all_losses = []\n",
        "        all_indices = []\n",
        "\n",
        "        if isinstance(indices, list):\n",
        "            indices = torch.stack(indices)\n",
        "\n",
        "        if return_loss:\n",
        "            assert not torch.any(indices == -1), 'some of the residual vq indices were dropped out. please use indices derived when the module is in eval mode to derive cross entropy loss'\n",
        "            ce_losses = []\n",
        "\n",
        "        should_quantize_dropout = self.training and self.quantize_dropout and not return_loss\n",
        "\n",
        "        # sample a layer index at which to dropout further residual quantization\n",
        "        # also prepare null indices and loss\n",
        "\n",
        "        if should_quantize_dropout:\n",
        "\n",
        "            # check if seed is manually passed in\n",
        "\n",
        "            if not exists(rand_quantize_dropout_fixed_seed):\n",
        "                rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device)\n",
        "\n",
        "            rand = random.Random(rand_quantize_dropout_fixed_seed)\n",
        "\n",
        "            rand_quantize_dropout_index = rand.randrange(self.quantize_dropout_cutoff_index, num_quant)\n",
        "\n",
        "            if quant_dropout_multiple_of != 1:\n",
        "                rand_quantize_dropout_index = round_up_multiple(rand_quantize_dropout_index + 1, quant_dropout_multiple_of) - 1\n",
        "\n",
        "            null_indices_shape = (x.shape[0], *x.shape[-2:]) if self.accept_image_fmap else tuple(x.shape[:2])\n",
        "            null_indices = torch.full(null_indices_shape, -1., device = device, dtype = torch.long)\n",
        "            null_loss = torch.full((1,), 0., device = device, dtype = x.dtype)\n",
        "\n",
        "        # setup the mlps for implicit neural codebook\n",
        "\n",
        "        maybe_code_transforms = (None,) * len(self.layers)\n",
        "\n",
        "        if self.implicit_neural_codebook:\n",
        "            maybe_code_transforms = (None, *self.mlps)\n",
        "\n",
        "        # save all inputs across layers, for use during expiration at end under shared codebook setting\n",
        "\n",
        "        all_residuals = []\n",
        "\n",
        "        # go through the layers\n",
        "\n",
        "        for quantizer_index, (vq, maybe_mlp) in enumerate(zip(self.layers, maybe_code_transforms)):\n",
        "\n",
        "            if should_quantize_dropout and quantizer_index > rand_quantize_dropout_index:\n",
        "                all_indices.append(null_indices)\n",
        "                all_losses.append(null_loss)\n",
        "                continue\n",
        "\n",
        "            layer_indices = None\n",
        "            if return_loss:\n",
        "                layer_indices = indices[..., quantizer_index]\n",
        "\n",
        "            # setup the transform code function to be passed into VectorQuantize forward\n",
        "\n",
        "            if exists(maybe_mlp):\n",
        "                maybe_mlp = partial(maybe_mlp, condition = quantized_out)\n",
        "\n",
        "            # save for expiration\n",
        "\n",
        "            all_residuals.append(residual)\n",
        "\n",
        "            # vector quantize forward\n",
        "\n",
        "            quantized, *rest = vq(\n",
        "                residual,\n",
        "                mask = mask,\n",
        "                indices = layer_indices,\n",
        "                sample_codebook_temp = sample_codebook_temp,\n",
        "                freeze_codebook = freeze_codebook,\n",
        "                codebook_transform_fn = maybe_mlp\n",
        "            )\n",
        "\n",
        "            residual = residual - quantized.detach()\n",
        "            quantized_out = quantized_out + quantized\n",
        "\n",
        "            if return_loss:\n",
        "                ce_loss = rest[0]\n",
        "                ce_losses.append(ce_loss)\n",
        "                continue\n",
        "\n",
        "            embed_indices, loss = rest\n",
        "\n",
        "            all_indices.append(embed_indices)\n",
        "            all_losses.append(loss)\n",
        "\n",
        "        # if shared codebook, update ema only at end\n",
        "\n",
        "        if self.training and self.shared_codebook:\n",
        "            shared_layer = first(self.layers)\n",
        "            shared_layer._codebook.update_ema()\n",
        "            shared_layer.update_in_place_optimizer()\n",
        "            shared_layer.expire_codes_(torch.cat(all_residuals, dim = -2))\n",
        "\n",
        "        # project out, if needed\n",
        "\n",
        "        quantized_out = self.project_out(quantized_out)\n",
        "\n",
        "        # whether to early return the cross entropy loss\n",
        "\n",
        "        if return_loss:\n",
        "            return quantized_out, sum(ce_losses)\n",
        "\n",
        "        # stack all losses and indices\n",
        "\n",
        "        all_losses, all_indices = map(partial(torch.stack, dim = -1), (all_losses, all_indices))\n",
        "\n",
        "        ret = (quantized_out, all_indices, all_losses)\n",
        "\n",
        "        if return_all_codes:\n",
        "            # whether to return all codes from all codebooks across layers\n",
        "            all_codes = self.get_codes_from_indices(all_indices)\n",
        "\n",
        "            # will return all codes in shape (quantizer, batch, sequence length, codebook dimension)\n",
        "            ret = (*ret, all_codes)\n",
        "\n",
        "        return ret\n",
        "\n",
        "# grouped residual vq\n",
        "\n",
        "class GroupedResidualVQ(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        groups = 1,\n",
        "        accept_image_fmap = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.groups = groups\n",
        "        assert (dim % groups) == 0\n",
        "        dim_per_group = dim // groups\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.rvqs = ModuleList([])\n",
        "\n",
        "        for _ in range(groups):\n",
        "            self.rvqs.append(ResidualVQ(\n",
        "                dim = dim_per_group,\n",
        "                accept_image_fmap = accept_image_fmap,\n",
        "                **kwargs\n",
        "            ))\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        return torch.stack(tuple(rvq.codebooks for rvq in self.rvqs))\n",
        "\n",
        "    @property\n",
        "    def split_dim(self):\n",
        "        return 1 if self.accept_image_fmap else -1\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "        codes = tuple(rvq.get_codes_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.stack(codes)\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        outputs = tuple(rvq.get_output_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.cat(outputs, dim = self.split_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        indices = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        mask = None,\n",
        "    ):\n",
        "        shape, split_dim, device = x.shape, self.split_dim, x.device\n",
        "        assert shape[split_dim] == self.dim\n",
        "\n",
        "        # split the feature dimension into groups\n",
        "\n",
        "        x = x.chunk(self.groups, dim = split_dim)\n",
        "\n",
        "        indices = default(indices, tuple())\n",
        "        return_ce_loss = len(indices) > 0\n",
        "        assert len(indices) == 0 or len(indices) == self.groups\n",
        "\n",
        "        forward_kwargs = dict(\n",
        "            return_all_codes = return_all_codes,\n",
        "            sample_codebook_temp = sample_codebook_temp,\n",
        "            mask = mask,\n",
        "            freeze_codebook = freeze_codebook,\n",
        "            rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device) if self.training else None\n",
        "        )\n",
        "\n",
        "        # invoke residual vq on each group\n",
        "\n",
        "        out = tuple(rvq(chunk, indices = chunk_indices, **forward_kwargs) for rvq, chunk, chunk_indices in zip_longest(self.rvqs, x, indices))\n",
        "        out = tuple(zip(*out))\n",
        "\n",
        "        # if returning cross entropy loss to rvq codebooks\n",
        "\n",
        "        if return_ce_loss:\n",
        "            quantized, ce_losses = out\n",
        "            return torch.cat(quantized, dim = split_dim), sum(ce_losses)\n",
        "\n",
        "        # otherwise, get all the zipped outputs and combine them\n",
        "\n",
        "        quantized, all_indices, commit_losses, *maybe_all_codes = out\n",
        "\n",
        "        quantized = torch.cat(quantized, dim = split_dim)\n",
        "        all_indices = torch.stack(all_indices)\n",
        "        commit_losses = torch.stack(commit_losses)\n",
        "\n",
        "        ret = (quantized, all_indices, commit_losses, *maybe_all_codes)\n",
        "        return ret\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "axZFsNiThSmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CompVis vqvae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "\n",
        "# class VQModel(pl.LightningModule):\n",
        "class VQModel(nn.Module):\n",
        "    def __init__(self, n_embed, embed_dim,):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        z_channels = 4 # 4?\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n",
        "        self.quant_conv = torch.nn.Conv2d(z_channels, embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, z_channels, 1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Dr2bajYVyjbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title airalcorn2 vqvae\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/vqvae.py\n",
        "# Ported from: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb.\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_hiddens,\n",
        "                        out_channels=num_residual_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_residual_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "        # ResNet V1-style.\n",
        "        return torch.relu(h)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
        "        # off with a ReLU.\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_hiddens,\n",
        "                out_channels=num_hiddens,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=embedding_dim,\n",
        "            out_channels=num_hiddens,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
        "\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
        "        super().__init__()\n",
        "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
        "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        # Weight for the exponential moving average.\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            dictionary_loss = None\n",
        "\n",
        "        # See third term of Equation (3).\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        # Straight-through gradient. See Section 3.2.\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "        embedding_dim,\n",
        "        num_embeddings,\n",
        "        use_ema,\n",
        "        decay,\n",
        "        epsilon,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "        self.vq = VectorQuantizer(\n",
        "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
        "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_recon = self.decoder(z_quantized)\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }\n",
        "\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/train_vqvae.py\n",
        "        # out = model(imgs)\n",
        "        # recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
        "        # total_recon_error += recon_error.item()\n",
        "        # loss = recon_error + beta * out[\"commitment_loss\"]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cS-HV67C8S_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rosinality from sonet\n",
        "# https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import distributed as dist_fn\n",
        "\n",
        "\n",
        "# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Borrowed from https://github.com/deepmind/sonnet and ported it to PyTorch\n",
        "\n",
        "\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channel, channel, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel, in_channel, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv(input)\n",
        "        out += input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channel, out_channel, channel, n_res_block, n_res_channel, stride\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = [nn.Conv2d(in_channel, channel, 3, padding=1)]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks.extend(\n",
        "                [\n",
        "                    nn.ConvTranspose2d(channel, channel // 2, 4, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.ConvTranspose2d(\n",
        "                        channel // 2, out_channel, 4, stride=2, padding=1\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks.append(\n",
        "                nn.ConvTranspose2d(channel, out_channel, 4, stride=2, padding=1)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel=3,\n",
        "        channel=128,\n",
        "        n_res_block=2,\n",
        "        n_res_channel=32,\n",
        "        embed_dim=64,\n",
        "        n_embed=512,\n",
        "        decay=0.99,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
        "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
        "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
        "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
        "        self.dec_t = Decoder(\n",
        "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
        "        )\n",
        "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
        "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
        "        self.upsample_t = nn.ConvTranspose2d(\n",
        "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.dec = Decoder(\n",
        "            embed_dim + embed_dim,\n",
        "            in_channel,\n",
        "            channel,\n",
        "            n_res_block,\n",
        "            n_res_channel,\n",
        "            stride=4,\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec, diff\n",
        "\n",
        "    def encode(self, input):\n",
        "        enc_b = self.enc_b(input)\n",
        "        enc_t = self.enc_t(enc_b)\n",
        "\n",
        "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
        "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        diff_t = diff_t.unsqueeze(0)\n",
        "\n",
        "        dec_t = self.dec_t(quant_t)\n",
        "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
        "\n",
        "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
        "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "        diff_b = diff_b.unsqueeze(0)\n",
        "\n",
        "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
        "\n",
        "    def decode(self, quant_t, quant_b):\n",
        "        upsample_t = self.upsample_t(quant_t)\n",
        "        quant = torch.cat([upsample_t, quant_b], 1)\n",
        "        dec = self.dec(quant)\n",
        "\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_t, code_b):\n",
        "        quant_t = self.quantize_t.embed_code(code_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        quant_b = self.quantize_b.embed_code(code_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "15v0h5A74X2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CompVis stable-diffusion model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GRVHtJAs0bXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwCLJCY_dFD6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title CompVis taming-transformers vqvae quantize.py\n",
        "# https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n",
        "    ____________________________________________\n",
        "    Discretization bottleneck part of the VQ-VAE.\n",
        "    Inputs:\n",
        "    - n_e : number of embeddings\n",
        "    - e_dim : dimension of embedding\n",
        "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
        "    _____________________________________________\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: this class contains a bug regarding beta; see VectorQuantizer2 for\n",
        "    # a fix and use legacy=False to apply that fix. VectorQuantizer2 can be\n",
        "    # used wherever VectorQuantizer has been used before and is additionally\n",
        "    # more efficient.\n",
        "    def __init__(self, n_e, e_dim, beta):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Inputs the output of the encoder network z and maps it to a discrete\n",
        "        one-hot vector that is the index of the closest embedding vector e_j\n",
        "        z (continuous) -> z_q (discrete)\n",
        "        z.shape = (batch, channel, height, width)\n",
        "        quantization pipeline:\n",
        "            1. get encoder input (B,C,H,W)\n",
        "            2. flatten input to (B*H*W,C)\n",
        "        \"\"\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
        "\n",
        "        ## could possible replace this here\n",
        "        # #\\start...\n",
        "        # find closest encodings\n",
        "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
        "\n",
        "        min_encodings = torch.zeros(\n",
        "            min_encoding_indices.shape[0], self.n_e).to(z)\n",
        "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
        "\n",
        "        # dtype min encodings: torch.float32\n",
        "        # min_encodings shape: torch.Size([2048, 512])\n",
        "        # min_encoding_indices.shape: torch.Size([2048, 1])\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
        "        #.........\\end\n",
        "\n",
        "        # with:\n",
        "        # .........\\start\n",
        "        #min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        #z_q = self.embedding(min_encoding_indices)\n",
        "        # ......\\end......... (TODO)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
        "            torch.mean((z_q - z.detach()) ** 2)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # perplexity\n",
        "        e_mean = torch.mean(min_encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        # TODO: check for more easy handling with nn.Embedding\n",
        "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
        "        min_encodings.scatter_(1, indices[:,None], 1)\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
        "\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class GumbelQuantize(nn.Module):\n",
        "    \"\"\"\n",
        "    credit to @karpathy: https://github.com/karpathy/deep-vector-quantization/blob/main/model.py (thanks!)\n",
        "    Gumbel Softmax trick quantizer\n",
        "    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016\n",
        "    https://arxiv.org/abs/1611.01144\n",
        "    \"\"\"\n",
        "    def __init__(self, num_hiddens, embedding_dim, n_embed, straight_through=True, kl_weight=5e-4, temp_init=1.0, use_vqinterface=True, remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.straight_through = straight_through\n",
        "        self.temperature = temp_init\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "        self.proj = nn.Conv2d(num_hiddens, n_embed, 1)\n",
        "        self.embed = nn.Embedding(n_embed, embedding_dim)\n",
        "\n",
        "        self.use_vqinterface = use_vqinterface\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, return_logits=False):\n",
        "        # force hard = True when we are in eval mode, as we must quantize. actually, always true seems to work\n",
        "        hard = self.straight_through if self.training else True\n",
        "        temp = self.temperature if temp is None else temp\n",
        "\n",
        "        logits = self.proj(z)\n",
        "        if self.remap is not None:\n",
        "            # continue only with used logits\n",
        "            full_zeros = torch.zeros_like(logits)\n",
        "            logits = logits[:,self.used,...]\n",
        "\n",
        "        soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=hard)\n",
        "        if self.remap is not None:\n",
        "            # go back to all entries but unused set to zero\n",
        "            full_zeros[:,self.used,...] = soft_one_hot\n",
        "            soft_one_hot = full_zeros\n",
        "        z_q = einsum('b n h w, n d -> b d h w', soft_one_hot, self.embed.weight)\n",
        "\n",
        "        # + kl divergence to the prior loss\n",
        "        qy = F.softmax(logits, dim=1)\n",
        "        diff = self.kl_weight * torch.sum(qy * torch.log(qy * self.n_embed + 1e-10), dim=1).mean()\n",
        "\n",
        "        ind = soft_one_hot.argmax(dim=1)\n",
        "        if self.remap is not None:\n",
        "            ind = self.remap_to_used(ind)\n",
        "        if self.use_vqinterface:\n",
        "            if return_logits:\n",
        "                return z_q, diff, (None, None, ind), logits\n",
        "            return z_q, diff, (None, None, ind)\n",
        "        return z_q, diff, ind\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        b, h, w, c = shape\n",
        "        assert b*h*w == indices.shape[0]\n",
        "        indices = rearrange(indices, '(b h w) -> b h w', b=b, h=h, w=w)\n",
        "        if self.remap is not None:\n",
        "            indices = self.unmap_to_all(indices)\n",
        "        one_hot = F.one_hot(indices, num_classes=self.n_embed).permute(0, 3, 1, 2).float()\n",
        "        z_q = einsum('b n h w, n d -> b d h w', one_hot, self.embed.weight)\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class VectorQuantizer2(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n",
        "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
        "    \"\"\"\n",
        "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
        "    # backwards compatibility we use the buggy version by default, but you can\n",
        "    # specify legacy=False to fix it.\n",
        "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\", sane_index_shape=False, legacy=True):\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.legacy = legacy\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_e\n",
        "\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
        "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
        "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "        perplexity = None\n",
        "        min_encodings = None\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        # loss = torch.mean((z_q.detach()-z)**2) + self.beta * torch.mean((z_q - z.detach()) ** 2) # legacy\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        if self.remap is not None:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
        "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
        "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
        "        if self.sane_index_shape:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(\n",
        "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        if self.remap is not None:\n",
        "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
        "            indices = self.unmap_to_all(indices)\n",
        "            indices = indices.reshape(-1) # flatten again\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "class EmbeddingEMA(nn.Module):\n",
        "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        weight = torch.randn(num_tokens, codebook_dim)\n",
        "        self.weight = nn.Parameter(weight, requires_grad = False)\n",
        "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False)\n",
        "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False)\n",
        "        self.update = True\n",
        "\n",
        "    def forward(self, embed_id):\n",
        "        return F.embedding(embed_id, self.weight)\n",
        "\n",
        "    def cluster_size_ema_update(self, new_cluster_size):\n",
        "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
        "\n",
        "    def embed_avg_ema_update(self, new_embed_avg):\n",
        "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
        "\n",
        "    def weight_update(self, num_tokens):\n",
        "        n = self.cluster_size.sum()\n",
        "        smoothed_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
        "            )\n",
        "        #normalize embedding average with smoothed cluster size\n",
        "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
        "        self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "\n",
        "class EMAVectorQuantizer(nn.Module):\n",
        "    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n",
        "                remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.num_tokens = num_tokens\n",
        "        self.beta = beta\n",
        "        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        #z, 'b c h w -> b h w c'\n",
        "        z = rearrange(z, 'b c h w -> b h w c')\n",
        "        z_flattened = z.reshape(-1, self.codebook_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n",
        "            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n",
        "            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n'\n",
        "\n",
        "\n",
        "        encoding_indices = torch.argmin(d, dim=1)\n",
        "\n",
        "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        if self.training and self.embedding.update:\n",
        "            #EMA cluster size\n",
        "            encodings_sum = encodings.sum(0)\n",
        "            self.embedding.cluster_size_ema_update(encodings_sum)\n",
        "            #EMA embedding average\n",
        "            embed_sum = encodings.transpose(0,1) @ z_flattened\n",
        "            self.embedding.embed_avg_ema_update(embed_sum)\n",
        "            #normalize embed_avg and update weight\n",
        "            self.embedding.weight_update(self.num_tokens)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * F.mse_loss(z_q.detach(), z)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        #z_q, 'b h w c -> b c h w'\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w')\n",
        "        return z_q, loss, (perplexity, encodings, encoding_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CompVis stable-diffusion autoencoder.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 batch_resize_range=None,\n",
        "                 scheduler_config=None,\n",
        "                 lr_g_factor=1.0,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
        "                                        remap=remap,\n",
        "                                        sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        self.batch_resize_range = batch_resize_range\n",
        "        if self.batch_resize_range is not None:\n",
        "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "        self.scheduler_config = scheduler_config\n",
        "        self.lr_g_factor = lr_g_factor\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Missing Keys: {missing}\")\n",
        "            print(f\"Unexpected Keys: {unexpected}\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        if self.batch_resize_range is not None:\n",
        "            lower_size = self.batch_resize_range[0]\n",
        "            upper_size = self.batch_resize_range[1]\n",
        "            if self.global_step <= 4:\n",
        "                # do the first few batches with max size to avoid later oom\n",
        "                new_resize = upper_size\n",
        "            else:\n",
        "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
        "            if new_resize != x.shape[2]:\n",
        "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
        "            x = x.detach()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_d = self.learning_rate\n",
        "        lr_g = self.lr_g_factor*self.learning_rate\n",
        "        print(\"lr_d\", lr_d)\n",
        "        print(\"lr_g\", lr_g)\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quantize.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr_g, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
        "                                    lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "        if self.scheduler_config is not None:\n",
        "            scheduler = instantiate_from_config(self.scheduler_config)\n",
        "\n",
        "            print(\"Setting up LambdaLR scheduler...\")\n",
        "            scheduler = [\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "            ]\n",
        "            return [opt_ae, opt_disc], scheduler\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if only_inputs:\n",
        "            log[\"inputs\"] = x\n",
        "            return log\n",
        "        xrec, _ = self(x)\n",
        "        if x.shape[1] > 3:\n",
        "            # colorize with random projection\n",
        "            assert xrec.shape[1] > 3\n",
        "            x = self.to_rgb(x)\n",
        "            xrec = self.to_rgb(xrec)\n",
        "        log[\"inputs\"] = x\n",
        "        log[\"reconstructions\"] = xrec\n",
        "        if plot_ema:\n",
        "            with self.ema_scope():\n",
        "                xrec_ema, _ = self(x)\n",
        "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
        "                log[\"reconstructions_ema\"] = xrec_ema\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path}\")\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if not only_inputs:\n",
        "            xrec, posterior = self(x)\n",
        "            if x.shape[1] > 3:\n",
        "                # colorize with random projection\n",
        "                assert xrec.shape[1] > 3\n",
        "                x = self.to_rgb(x)\n",
        "                xrec = self.to_rgb(xrec)\n",
        "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
        "            log[\"reconstructions\"] = xrec\n",
        "        log[\"inputs\"] = x\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7CSqxzfG_2rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): #\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# # https://arxiv.org/pdf/1707.02937\n",
        "# nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), 'out'), nn.PixelShuffle(r))\n",
        "# nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), 'in'))\n",
        "\n",
        "# https://github.com/fastai/fastai/blob/main/fastai/layers.py#L368\n",
        "def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
        "    \"ICNR init of `x`, with `scale` and `init` function\"\n",
        "    ni,nf,h,w = x.shape\n",
        "    ni2 = int(ni/(scale**2))\n",
        "    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
        "    k = k.contiguous().view(ni2, nf, -1)\n",
        "    k = k.repeat(1, 1, scale**2)\n",
        "    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
        "\n",
        "class PixelShuffle_ICNR(nn.Sequential):\n",
        "    \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`.\"\n",
        "    def __init__(self, ni, nf=None, scale=2, blur=False, norm_type=NormType.Weight, act_cls=defaults.activation):\n",
        "        super().__init__()\n",
        "        nf = ifnone(nf, ni)\n",
        "        layers = [ConvLayer(ni, nf*(scale**2), ks=1, norm_type=norm_type, act_cls=act_cls, bias_std=0),\n",
        "                  nn.PixelShuffle(scale)]\n",
        "        if norm_type == NormType.Weight:\n",
        "            layers[0][0].weight_v.data.copy_(icnr_init(layers[0][0].weight_v.data))\n",
        "            layers[0][0].weight_g.data.copy_(((layers[0][0].weight_v.data**2).sum(dim=[1,2,3])**0.5)[:,None,None,None])\n",
        "        else:\n",
        "            layers[0][0].weight.data.copy_(icnr_init(layers[0][0].weight.data))\n",
        "        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XOuGUdMZaxB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        # self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=1/2)\n",
        "        # self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=1/2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        # self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "        self.out_shortcut = PixelShortcut(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        # self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        # print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        # self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "        self.in_shortcut = PixelShortcut(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a0f32c-e5d9-4790-a96b-dcd62065fc47",
        "cellView": "form",
        "id": "w_FyFDkua0lA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88119\n",
            "torch.Size([2, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title efficientvit nn/ops.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.utils import get_same_padding, list_sum, resize, val2list, val2tuple\n",
        "\n",
        "__all__ = [\n",
        "    \"ConvLayer\",\n",
        "    \"UpSampleLayer\",\n",
        "    \"ConvPixelUnshuffleDownSampleLayer\",\n",
        "    \"PixelUnshuffleChannelAveragingDownSampleLayer\",\n",
        "    \"ConvPixelShuffleUpSampleLayer\",\n",
        "    \"ChannelDuplicatingPixelUnshuffleUpSampleLayer\",\n",
        "    \"LinearLayer\",\n",
        "    \"IdentityLayer\",\n",
        "    \"DSConv\",\n",
        "    \"MBConv\",\n",
        "    \"FusedMBConv\",\n",
        "    \"ResBlock\",\n",
        "    \"LiteMLA\",\n",
        "    \"EfficientViTBlock\",\n",
        "    \"ResidualBlock\",\n",
        "    \"DAGBlock\",\n",
        "    \"OpSequential\",\n",
        "]\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Layers                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        dilation=1,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        dropout=0,\n",
        "        norm=\"bn2d\",\n",
        "        act_func=\"relu\",\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        padding = get_same_padding(kernel_size)\n",
        "        padding *= dilation\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            stride=(stride, stride),\n",
        "            padding=padding,\n",
        "            dilation=(dilation, dilation),\n",
        "            groups=groups,\n",
        "            bias=use_bias,\n",
        "        )\n",
        "        self.norm = build_norm(norm, num_features=out_channels)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode=\"bicubic\",\n",
        "        size: Optional[int | tuple[int, int] | list[int]] = None,\n",
        "        factor=2,\n",
        "        align_corners=False,\n",
        "    ):\n",
        "        super(UpSampleLayer, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.size = val2list(size, 2) if size is not None else None\n",
        "        self.factor = None if self.size is not None else factor\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if (self.size is not None and tuple(x.shape[-2:]) == self.size) or self.factor == 1:\n",
        "            return x\n",
        "        if x.dtype in [torch.float16, torch.bfloat16]:\n",
        "            x = x.float()\n",
        "        return resize(x, self.size, self.factor, self.mode, self.align_corners)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        assert out_channels % out_ratio == 0\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels // out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert in_channels * factor**2 % out_channels == 0\n",
        "        self.group_size = in_channels * factor**2 // out_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, self.out_channels, self.group_size, H, W)\n",
        "        x = x.mean(dim=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels * out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InterpolateConvUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "        mode: str = \"nearest\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        self.mode = mode\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert out_channels * factor**2 % in_channels == 0\n",
        "        self.repeats = out_channels * factor**2 // in_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.repeat_interleave(self.repeats, dim=1)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        use_bias=True,\n",
        "        dropout=0,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    ):\n",
        "        super(LinearLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.linear = nn.Linear(in_features, out_features, use_bias)\n",
        "        self.norm = build_norm(norm, num_features=out_features)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def _try_squeeze(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() > 2:\n",
        "            x = torch.flatten(x, start_dim=1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self._try_squeeze(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Blocks                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class DSConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super(DSConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.depth_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            in_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=in_channels,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", \"relu6\", None),\n",
        "    ):\n",
        "        super(MBConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            1,\n",
        "            stride=1,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "            use_bias=use_bias[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FusedMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.spatial_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=groups,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.spatial_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLUMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(None, None, \"ln2d\"),\n",
        "        act_func=(\"silu\", \"silu\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels * 2,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels * 2,\n",
        "            mid_channels * 2,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels * 2,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=None,\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[2],\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        gate = self.glu_act(gate)\n",
        "        x = x * gate\n",
        "\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.conv1 = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.conv2 = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LiteMLA(nn.Module):\n",
        "    r\"\"\"Lightweight multi-scale linear attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        heads: Optional[int] = None,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=8,\n",
        "        use_bias=False,\n",
        "        norm=(None, \"bn2d\"),\n",
        "        act_func=(None, None),\n",
        "        kernel_func=\"relu\",\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        eps=1.0e-15,\n",
        "    ):\n",
        "        super(LiteMLA, self).__init__()\n",
        "        self.eps = eps\n",
        "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
        "\n",
        "        total_dim = heads * dim\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.qkv = ConvLayer(\n",
        "            in_channels,\n",
        "            3 * total_dim,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.aggreg = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        3 * total_dim,\n",
        "                        3 * total_dim,\n",
        "                        scale,\n",
        "                        padding=get_same_padding(scale),\n",
        "                        groups=3 * total_dim,\n",
        "                        bias=use_bias[0],\n",
        "                    ),\n",
        "                    nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
        "                )\n",
        "                for scale in scales\n",
        "            ]\n",
        "        )\n",
        "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
        "\n",
        "        self.proj = ConvLayer(\n",
        "            total_dim * (1 + len(scales)),\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        if qkv.dtype == torch.float16:\n",
        "            qkv = qkv.float()\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        # lightweight linear attention\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        # linear matmul\n",
        "        trans_k = k.transpose(-1, -2)\n",
        "\n",
        "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
        "        vk = torch.matmul(v, trans_k)\n",
        "        out = torch.matmul(vk, q)\n",
        "        if out.dtype == torch.bfloat16:\n",
        "            out = out.float()\n",
        "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
        "        original_dtype = att_map.dtype\n",
        "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
        "            att_map = att_map.float()\n",
        "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
        "        att_map = att_map.to(original_dtype)\n",
        "        out = torch.matmul(v, att_map)  # b h d n\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # generate multi-scale q, k, v\n",
        "        qkv = self.qkv(x)\n",
        "        multi_scale_qkv = [qkv]\n",
        "        for op in self.aggreg:\n",
        "            multi_scale_qkv.append(op(qkv))\n",
        "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
        "\n",
        "        H, W = list(qkv.size())[-2:]\n",
        "        if H * W > self.dim:\n",
        "            out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
        "        else:\n",
        "            out = self.relu_quadratic_att(qkv)\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=32,\n",
        "        expand_ratio: float = 4,\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        norm: str = \"bn2d\",\n",
        "        act_func: str = \"hswish\",\n",
        "        context_module: str = \"LiteMLA\",\n",
        "        local_module: str = \"MBConv\",\n",
        "    ):\n",
        "        super(EfficientViTBlock, self).__init__()\n",
        "        if context_module == \"LiteMLA\":\n",
        "            self.context_module = ResidualBlock(\n",
        "                LiteMLA(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    heads_ratio=heads_ratio,\n",
        "                    dim=dim,\n",
        "                    norm=(None, norm),\n",
        "                    scales=scales,\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
        "        if local_module == \"MBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                MBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        elif local_module == \"GLUMBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                GLUMBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.context_module(x)\n",
        "        x = self.local_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Functional Blocks                                 #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        main: Optional[nn.Module],\n",
        "        shortcut: Optional[nn.Module],\n",
        "        post_act=None,\n",
        "        pre_norm: Optional[nn.Module] = None,\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.main = main\n",
        "        self.shortcut = shortcut\n",
        "        self.post_act = build_act(post_act)\n",
        "\n",
        "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.pre_norm is None:\n",
        "            return self.main(x)\n",
        "        else:\n",
        "            return self.main(self.pre_norm(x))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.main is None:\n",
        "            res = x\n",
        "        elif self.shortcut is None:\n",
        "            res = self.forward_main(x)\n",
        "        else:\n",
        "            res = self.forward_main(x) + self.shortcut(x)\n",
        "            if self.post_act:\n",
        "                res = self.post_act(res)\n",
        "        return res\n",
        "\n",
        "\n",
        "class DAGBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inputs: dict[str, nn.Module],\n",
        "        merge: str,\n",
        "        post_input: Optional[nn.Module],\n",
        "        middle: nn.Module,\n",
        "        outputs: dict[str, nn.Module],\n",
        "    ):\n",
        "        super(DAGBlock, self).__init__()\n",
        "\n",
        "        self.input_keys = list(inputs.keys())\n",
        "        self.input_ops = nn.ModuleList(list(inputs.values()))\n",
        "        self.merge = merge\n",
        "        self.post_input = post_input\n",
        "\n",
        "        self.middle = middle\n",
        "\n",
        "        self.output_keys = list(outputs.keys())\n",
        "        self.output_ops = nn.ModuleList(list(outputs.values()))\n",
        "\n",
        "    def forward(self, feature_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
        "        feat = [op(feature_dict[key]) for key, op in zip(self.input_keys, self.input_ops)]\n",
        "        if self.merge == \"add\":\n",
        "            feat = list_sum(feat)\n",
        "        elif self.merge == \"cat\":\n",
        "            feat = torch.concat(feat, dim=1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        if self.post_input is not None:\n",
        "            feat = self.post_input(feat)\n",
        "        feat = self.middle(feat)\n",
        "        for key, op in zip(self.output_keys, self.output_ops):\n",
        "            feature_dict[key] = op(feat)\n",
        "        return feature_dict\n",
        "\n",
        "\n",
        "class OpSequential(nn.Module):\n",
        "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
        "        super(OpSequential, self).__init__()\n",
        "        valid_op_list = []\n",
        "        for op in op_list:\n",
        "            if op is not None:\n",
        "                valid_op_list.append(op)\n",
        "        self.op_list = nn.ModuleList(valid_op_list)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for op in self.op_list:\n",
        "            x = op(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rkrEVsXxSs_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from omegaconf import MISSING, OmegaConf\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.nn.ops import (\n",
        "    ChannelDuplicatingPixelUnshuffleUpSampleLayer,\n",
        "    ConvLayer,\n",
        "    ConvPixelShuffleUpSampleLayer,\n",
        "    ConvPixelUnshuffleDownSampleLayer,\n",
        "    EfficientViTBlock,\n",
        "    IdentityLayer,\n",
        "    InterpolateConvUpSampleLayer,\n",
        "    OpSequential,\n",
        "    PixelUnshuffleChannelAveragingDownSampleLayer,\n",
        "    ResBlock,\n",
        "    ResidualBlock,\n",
        ")\n",
        "\n",
        "__all__ = [\"DCAE\", \"dc_ae_f32c32\", \"dc_ae_f64c128\", \"dc_ae_f128c512\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EncoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: str = \"trms2d\"\n",
        "    act: str = \"silu\"\n",
        "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
        "    downsample_match_channel: bool = True\n",
        "    downsample_shortcut: Optional[str] = \"averaging\"\n",
        "    out_norm: Optional[str] = None\n",
        "    out_act: Optional[str] = None\n",
        "    out_shortcut: Optional[str] = \"averaging\"\n",
        "    double_latent: bool = False\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DecoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    in_shortcut: Optional[str] = \"duplicating\"\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: Any = \"trms2d\"\n",
        "    act: Any = \"silu\"\n",
        "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
        "    upsample_match_channel: bool = True\n",
        "    upsample_shortcut: str = \"duplicating\"\n",
        "    out_norm: str = \"trms2d\"\n",
        "    out_act: str = \"relu\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DCAEConfig:\n",
        "    in_channels: int = 3\n",
        "    latent_channels: int = 32\n",
        "    encoder: EncoderConfig = field(\n",
        "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    decoder: DecoderConfig = field(\n",
        "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    use_quant_conv: bool = False\n",
        "\n",
        "    pretrained_path: Optional[str] = None\n",
        "    pretrained_source: str = \"dc-ae\"\n",
        "\n",
        "    scaling_factor: Optional[float] = None\n",
        "\n",
        "\n",
        "def build_block(\n",
        "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str]\n",
        ") -> nn.Module:\n",
        "    if block_type == \"ResBlock\":\n",
        "        assert in_channels == out_channels\n",
        "        main_block = ResBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=(True, False),\n",
        "            norm=(None, norm),\n",
        "            act_func=(act, None),\n",
        "        )\n",
        "        block = ResidualBlock(main_block, IdentityLayer())\n",
        "    elif block_type == \"EViT_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=())\n",
        "    elif block_type == \"EViTS5_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,))\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_stage_main(\n",
        "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int\n",
        ") -> list[nn.Module]:\n",
        "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
        "    stage = []\n",
        "    for d in range(depth):\n",
        "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
        "        block = build_block(\n",
        "            block_type=current_block_type,\n",
        "            in_channels=width if d > 0 else input_width,\n",
        "            out_channels=width,\n",
        "            norm=norm,\n",
        "            act=act,\n",
        "        )\n",
        "        stage.append(block)\n",
        "    return stage\n",
        "\n",
        "\n",
        "def build_downsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"Conv\":\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif block_type == \"ConvPixelUnshuffle\":\n",
        "        block = ConvPixelUnshuffleDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_upsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"ConvPixelShuffle\":\n",
        "        block = ConvPixelShuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    elif block_type == \"InterpolateConv\":\n",
        "        block = InterpolateConvUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str):\n",
        "    if factor == 1:\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        block = build_downsample_block(\n",
        "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], shortcut: Optional[str]\n",
        "):\n",
        "    block = OpSequential(\n",
        "        [\n",
        "            build_norm(norm),\n",
        "            build_act(act),\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str]):\n",
        "    block = ConvLayer(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=True,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, factor: int, upsample_block_type: str, norm: Optional[str], act: Optional[str]\n",
        "):\n",
        "    layers: list[nn.Module] = [\n",
        "        build_norm(norm, in_channels),\n",
        "        build_act(act),\n",
        "    ]\n",
        "    if factor == 1:\n",
        "        layers.append(\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            )\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        layers.append(\n",
        "            build_upsample_block(\n",
        "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
        "    return OpSequential(layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, cfg: EncoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "\n",
        "        self.project_in = build_encoder_project_in_block(\n",
        "            in_channels=cfg.in_channels,\n",
        "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            downsample_block_type=cfg.downsample_block_type,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            stage = build_stage_main(\n",
        "                width=width, depth=depth, block_type=block_type, norm=cfg.norm, act=cfg.act, input_width=width\n",
        "            )\n",
        "\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                downsample_block = build_downsample_block(\n",
        "                    block_type=cfg.downsample_block_type,\n",
        "                    in_channels=width,\n",
        "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
        "                    shortcut=cfg.downsample_shortcut,\n",
        "                )\n",
        "                stage.append(downsample_block)\n",
        "            self.stages.append(OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_encoder_project_out_block(\n",
        "            in_channels=cfg.width_list[-1],\n",
        "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "            shortcut=cfg.out_shortcut,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in self.stages:\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, cfg: DecoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
        "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
        "\n",
        "        self.project_in = build_decoder_project_in_block(\n",
        "            in_channels=cfg.latent_channels,\n",
        "            out_channels=cfg.width_list[-1],\n",
        "            shortcut=cfg.in_shortcut,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
        "            stage = []\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                upsample_block = build_upsample_block(\n",
        "                    block_type=cfg.upsample_block_type,\n",
        "                    in_channels=cfg.width_list[stage_id + 1],\n",
        "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
        "                    shortcut=cfg.upsample_shortcut,\n",
        "                )\n",
        "                stage.append(upsample_block)\n",
        "\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
        "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
        "            stage.extend(\n",
        "                build_stage_main(\n",
        "                    width=width,\n",
        "                    depth=depth,\n",
        "                    block_type=block_type,\n",
        "                    norm=norm,\n",
        "                    act=act,\n",
        "                    input_width=(\n",
        "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "            self.stages.insert(0, OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_decoder_project_out_block(\n",
        "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            out_channels=cfg.in_channels,\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            upsample_block_type=cfg.upsample_block_type,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in reversed(self.stages):\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, cfg: DCAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.encoder = Encoder(cfg.encoder)\n",
        "        self.decoder = Decoder(cfg.decoder)\n",
        "\n",
        "        if self.cfg.pretrained_path is not None:\n",
        "            self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.cfg.pretrained_source == \"dc-ae\":\n",
        "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
        "            self.load_state_dict(state_dict)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def spatial_compression_ratio(self) -> int:\n",
        "        return 2 ** (self.decoder.num_stages - 1)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor, global_step: int) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, torch.tensor(0), {}\n",
        "\n",
        "\n",
        "def dc_ae_f32c32(name: str, pretrained_path: str) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f32c32-in-1.0\", \"dc-ae-f32c32-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[0,4,8,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[0,5,10,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]\"\n",
        "        )\n",
        "    elif name in [\"dc-ae-f32c32-sana-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
        "            \"encoder.downsample_block_type=Conv \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
        "            \"decoder.upsample_block_type=InterpolateConv \"\n",
        "            \"decoder.norm=trms2d decoder.act=silu \"\n",
        "            \"scaling_factor=0.41407\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f64c128(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f64c128-in-1.0\", \"dc-ae-f64c128-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=128 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048] encoder.depth_list=[0,4,8,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048] decoder.depth_list=[0,5,10,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f128c512(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f128c512-in-1.0\", \"dc-ae-f128c512-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=512 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048,2048] encoder.depth_list=[0,4,8,2,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048,2048] decoder.depth_list=[0,5,10,2,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EfX78Q2-Sqaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
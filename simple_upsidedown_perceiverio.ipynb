{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/simple_upsidedown_perceiverio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "k2Zy7r2Rb9vm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_62CZt9hfI1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "442cc6b9-8c42-46d0-c252-4bfec39f680b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.18)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.18"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220616_042209-1htnq6ex</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/bobdole/simple_upsidedown_perceiverio/runs/1htnq6ex\" target=\"_blank\">fast-spaceship-3</a></strong> to <a href=\"https://wandb.ai/bobdole/simple_upsidedown_perceiverio\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: perceiver-pytorch in /usr/local/lib/python3.7/dist-packages (0.8.3)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from perceiver-pytorch) (0.4.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from perceiver-pytorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->perceiver-pytorch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "import random, torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader as TorchDataLoader\n",
        "from torch.distributions.categorical import Categorical\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install gym[box2d]\n",
        "import gym\n",
        "# https://github.com/bprabhakar/upside-down-reinforcement-learning\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# https://wandb.ai/quickstart/pytorch\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # \n",
        "wandb.init(project=\"simple_upsidedown_perceiverio\", entity=\"bobdole\")\n",
        "\n",
        "!pip install perceiver-pytorch\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else: return observation, 0, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Behavior Net"
      ],
      "metadata": {
        "id": "Iexwn5NuU_9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Behavior Net\n",
        "from torch import nn\n",
        "#https://github.com/bprabhakar/upside-down-reinforcement-learning/blob/master/model.py\n",
        "class BehaviorNet(nn.Module):\n",
        "    \"\"\" Policy network takes state and target commands as input and returns probability distribution over all actions.\"\"\"\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(BehaviorNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim + 2, 32),  # 2 = command_dim\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.LogSoftmax(dim=-1)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"Input: features = [state command_steps command_reward] x batch_size\n",
        "        Output: action_probs = [action_dim] x batch_size\"\"\"\n",
        "        logprobs = self.model(features)\n",
        "        return logprobs\n"
      ],
      "metadata": {
        "id": "MgJkPwZ9dahb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIO"
      ],
      "metadata": {
        "id": "fPv66auOw8e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from perceiver_pytorch import PerceiverIO\n",
        "# # https://github.com/lucidrains/perceiver-pytorch\n",
        "class BehaviorNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(BehaviorNet, self).__init__()\n",
        "        self.model = PerceiverIO(\n",
        "            dim = state_dim+2,                    # dimension of sequence to be encoded\n",
        "            queries_dim = action_dim,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 64,           # 128 number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 64,            # 128 latent dimension\n",
        "            cross_heads = 1,             # 1 number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # 8 number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # 64 number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # 64 number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device)\n",
        "        self.queries = torch.zeros(1, action_dim, device=device)\n",
        "\n",
        "    def preprocess(self, X):\n",
        "        if X.dim()==1:\n",
        "            X=X.unsqueeze(dim=0)\n",
        "        X=X.flatten(start_dim=1, end_dim=-1) #(start_dim=1)\n",
        "        X=X.unsqueeze(dim=1)\n",
        "        return X\n",
        "\n",
        "    def postprocess(self, logits):\n",
        "        if logits.dim()==3:\n",
        "            logits=logits.squeeze(dim=1)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, features): #[512,10]\n",
        "        features = self.preprocess(features) #[512,1,10]\n",
        "        logits = self.model(features, queries = self.queries) # \n",
        "        logprobs = self.postprocess(logits) #[512, 4]\n",
        "        return logprobs\n"
      ],
      "metadata": {
        "id": "N2pdQHFVx9Xq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Replay Buffer"
      ],
      "metadata": {
        "id": "xKy41nR-Vatr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay Buffer\n",
        "import heapq\n",
        "import random\n",
        "import numpy as np\n",
        "# https://github.com/bprabhakar/upside-down-reinforcement-learning/blob/master/replay_buffer.py\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Implemented as a priority queue, where the priority value is set to be episode's total reward. Note that unlike usual RL buffers,\n",
        "    we store entire 'trajectories' together, instead of just transitions.\"\"\"\n",
        "    def __init__(self, size, seed=None):\n",
        "        self.size = size\n",
        "        self.buffer = [] # initialized as a regular list; use heapq functions\n",
        "        self.rg = np.random.RandomState(seed)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.buffer[key]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def add_episode(self, S, A, R, S_): # all inputs are numpy arrays; num_rows = timesteps; states, actions, rewards, next states\n",
        "        episode = (S, A, R, S_)\n",
        "        episode_reward = np.sum(R)\n",
        "        if S.shape[0] > 1: # ignore episodes that only last 1 step\n",
        "            item = (episode_reward, episode) # -1 for desc ordering\n",
        "            if len(self.buffer) < self.size:\n",
        "                heapq.heappush(self.buffer, item) \n",
        "            else:\n",
        "                _ = heapq.heappushpop(self.buffer, item) # ignore the popped obj\n",
        "    \n",
        "    def top_episodes(self, K): # Returns K episodes with highest total ep rewards.\n",
        "        episodes = [x[1] for x in self.buffer[-K:]] # buffer has (-reward, episode)\n",
        "        return episodes\n",
        "\n",
        "    def sample_episodes(self, K): # Returns random K episodes.\n",
        "        sampled_items = random.choices(self.buffer, k=K)\n",
        "        episodes = [x[1] for x in sampled_items] # buffer has (-reward, episode)\n",
        "        return episodes\n",
        "\n",
        "def generate_command(tgt_horizon, tgt_reward_mean, tgt_reward_std):\n",
        "    tgt_horizon = min(tgt_horizon, 200)\n",
        "    tgt_reward = round(np.random.random_sample()*tgt_reward_std + tgt_reward_mean, 0)\n",
        "    return tgt_horizon, tgt_reward\n",
        "\n"
      ],
      "metadata": {
        "id": "kTf4a3U6dop7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### utils"
      ],
      "metadata": {
        "id": "7uFcceE7VeR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader as TorchDataLoader\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "\n",
        "# https://github.com/bprabhakar/upside-down-reinforcement-learning/blob/master/utils.py\n",
        "def augment_state(state, command, command_scale):\n",
        "\t\"\"\" Appends scaled command values (horizon, reward) to the original state vector.\"\"\"\n",
        "\ttgt_horizon, tgt_return = command\n",
        "\thorizon_scale, return_scale = command_scale\n",
        "\ttgt_horizon *= horizon_scale\n",
        "\ttgt_return *= return_scale\n",
        "\tstate_ = np.append(state, [tgt_horizon, tgt_return])\n",
        "\treturn state_\n",
        "\n",
        "class BehaviorDataset(TorchDataset):\n",
        "    \"\"\" Samples behavior segments for supervised learning from given input episodes.\"\"\"\n",
        "    def __init__(self, episodes, size, horizon_scale, return_scale):\n",
        "        super(BehaviorDataset, self).__init__()\n",
        "        self.episodes = episodes\n",
        "        self.horizon_scale = horizon_scale\n",
        "        self.return_scale = return_scale\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size # just returning a placeholder number for now\n",
        "\n",
        "    def __getitem__(self, idx): # get episode\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()[0]\n",
        "        episode = random.choice(self.episodes) # randomly sample an episode\n",
        "        S, A, R, S_ = episode\n",
        "        # extract behavior segment\n",
        "        episode_len = S.shape[0]\n",
        "        start_index = np.random.choice(episode_len - 1) # ensures cmd_steps >= 1\n",
        "        command_horizon = (episode_len - start_index - 1)\n",
        "        command_return = np.sum(R[start_index:])\n",
        "        command = command_horizon, command_return\n",
        "        command_scale = self.horizon_scale, self.return_scale\n",
        "        # construct sample\n",
        "        features = augment_state(S[start_index,:], command, command_scale)\n",
        "        label = A[start_index]               # action taken\n",
        "        sample = {\n",
        "            'features': torch.tensor(features, dtype=torch.float), \n",
        "            'label': torch.tensor(label, dtype=torch.long) # categorical val\n",
        "        }        \n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "07ru7G9hd0cl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize replay buffer and warm-up using random policy"
      ],
      "metadata": {
        "id": "VMItZrYMWZSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KmTJlypqfI1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "9a80c3d8424d4fce9c44987479bc708f",
            "ace4d9e1ba6e462ba38d645653552710",
            "0b9e9a7cfdbf4edca9c57f391bdf3a2d",
            "05918e8642f541a0a99320714fae8074",
            "7c7c32360c1a42cabc95cbdfb984b3c2",
            "60a67a80c1bb431487c5aa61a9815c9e",
            "4a05454ffed040b388afb1c5cc5e76e5",
            "0cfc2c18b926402f8370ba821bf03eb8",
            "4b54ef58e3164a8cafaa32a46c134125",
            "3a087d90bc774a6dba445c552170e2c9",
            "c0a96ba4791145c391c4405af8ff88e6"
          ]
        },
        "outputId": "d23ea14a-531d-438a-c165-5f27492980f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a80c3d8424d4fce9c44987479bc708f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# general hyperparams\n",
        "NUM_WARMUP_EPISODES = 10      # No of warm-up episodes at the beginning\n",
        "REPLAY_SIZE = 300             # Max size of the replay buffer (in episodes)\n",
        "RETURN_SCALE = 0.01           # Scaling factor for desired horizon input (reward)\n",
        "HORIZON_SCALE = 0.01          # Scaling factor for desired horizon input (steps)\n",
        "# training hyperparams\n",
        "BATCH_SIZE = 512              # No of (input, target) pairs/batch for training the behavior function\n",
        "NUM_UPDATES_PER_ITER  = 100   # No of gradient-based updates of the behavior function per step of UDRL training\n",
        "LEARNING_RATE = 1e-3          # LR for ADAM optimizer\n",
        "# generating episodes hyperparams\n",
        "NUM_EPISODES_PER_ITER = 10    # No of exploratory episodes generated per step of UDRL training\n",
        "LAST_FEW = 25                 # No of episodes from the end of the replay buffer used for sampling exploratory commands\n",
        "\n",
        "\n",
        "# Initialize replay buffer and warm-up using random policy\n",
        "replay_buffer = ReplayBuffer(REPLAY_SIZE)   # init replay buffer\n",
        "# env = gym.make(\"LunarLander-v2\")            # init gym env\n",
        "env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "for _ in tqdm(range(NUM_WARMUP_EPISODES)):\n",
        "    episode = {'states': [], 'actions': [], 'rewards': [], 'next_states': [],}\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        episode['states'].append(state)\n",
        "        action = env.action_space.sample()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        episode['actions'].append(action)\n",
        "        episode['next_states'].append(state)\n",
        "        episode['rewards'].append(reward)\n",
        "    # wandb.log({\"reward\": reward})\n",
        "    replay_buffer.add_episode(\n",
        "        np.array(episode['states'], dtype=np.float),\n",
        "        np.array(episode['actions'], dtype=np.int),\n",
        "        np.array(episode['rewards'], dtype=np.float),\n",
        "        np.array(episode['next_states'], dtype=np.float),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwww"
      ],
      "metadata": {
        "id": "phCHqn_ZViyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pksar20nfI1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0054b2-6966-431c-c86d-1fbee7d0fcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "print(state_dim, action_dim)\n",
        "policy = BehaviorNet(state_dim, action_dim)\n",
        "loss_func = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main learning loop"
      ],
      "metadata": {
        "id": "EJ-olSrtWZWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F0_VjqFfI1k"
      },
      "outputs": [],
      "source": [
        "# Main learning loop\n",
        "\n",
        "# while(1): # keep cycling indefinitely\n",
        "for _ in tqdm(range(200)):\n",
        "# for x in range(100):\n",
        "    # 1 - Train Policy Network by sampling behavior segments from buffer.\n",
        "    episodes_to_train = replay_buffer.sample_episodes(5)\n",
        "    train_dset = BehaviorDataset(episodes_to_train, \n",
        "                                size=BATCH_SIZE*NUM_UPDATES_PER_ITER, \n",
        "                                horizon_scale=HORIZON_SCALE, \n",
        "                                return_scale=RETURN_SCALE)\n",
        "    training_behaviors = TorchDataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    if not policy.training: policy.train();\n",
        "    for behavior_batch in training_behaviors: # this runs for NUM_UPDATES_PER_ITER rounds\n",
        "        policy.zero_grad()\n",
        "        logprobs = policy(behavior_batch['features'].to(device))\n",
        "        loss = loss_func(logprobs, behavior_batch['label'].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # wandb.log({\"batch_loss\": loss.cpu().detach()})\n",
        "\n",
        "    # 2 - Sample exploratory target commands for future exploration.\n",
        "    top_episodes = replay_buffer.top_episodes(LAST_FEW) # [(S,A,R,S_), ... ]\n",
        "    tgt_horizon = int(np.mean([x[0].shape[0] for x in top_episodes]))\n",
        "    tgt_reward_mean = np.mean([np.sum(x[2]) for x in top_episodes])\n",
        "    tgt_reward_std = np.std([np.sum(x[2]) for x in top_episodes])\n",
        "    # wandb.log({\"tgt_reward_mean\": tgt_reward_mean})\n",
        "\n",
        "    # 3 - Generate new trajectories using latest policy network and generated commands\n",
        "    # use the latest policy network & sampled commands to generate new trajectories & add them to the replay buffer\n",
        "    for ep in range(NUM_EPISODES_PER_ITER):\n",
        "        episode = {'states': [], 'actions': [], 'rewards': [], 'next_states': [],}\n",
        "        episode_reward = 0\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        command_horizon, command_reward = generate_command(tgt_horizon, tgt_reward_mean, tgt_reward_std)\n",
        "        # wandb.log({\"command_horizon\": command_horizon})\n",
        "        # wandb.log({\"command_reward\": command_reward})\n",
        "        while not done:\n",
        "            episode['states'].append(state)\n",
        "            state_ = augment_state(state, \n",
        "                                command=(command_horizon, command_reward), \n",
        "                                command_scale=(HORIZON_SCALE, RETURN_SCALE))\n",
        "            state_ = torch.tensor(state_, dtype=torch.float)\n",
        "            with torch.no_grad():\n",
        "                action_logprobs = policy(state_.to(device))\n",
        "                action_distribution = Categorical(logits=action_logprobs)\n",
        "                action = action_distribution.sample().item()\n",
        "            state, reward, done, info = env.step(action)\n",
        "            episode['actions'].append(action)\n",
        "            episode['next_states'].append(state)\n",
        "            command_horizon = max(1, command_horizon-1)\n",
        "            episode['rewards'].append(reward)\n",
        "            command_reward -= reward\n",
        "        wandb.log({\"reward\": reward})\n",
        "        replay_buffer.add_episode(\n",
        "            np.array(episode['states'], dtype=float),\n",
        "            np.array(episode['actions'], dtype=int),\n",
        "            np.array(episode['rewards'], dtype=float),\n",
        "            np.array(episode['next_states'], dtype=float),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### save"
      ],
      "metadata": {
        "id": "HlfcmiotWiSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"model.pth\"\n",
        "torch.save(policy.state_dict(), name)\n"
      ],
      "metadata": {
        "id": "Xir00acg3aqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_reward=200\n",
        "state_ = np.append(state, [max_reward, max_reward])\n",
        "\n",
        "# state_ = augment_state(state, command=(command_horizon, command_reward), command_scale=(HORIZON_SCALE, RETURN_SCALE))\n",
        "state_ = torch.tensor(state_, dtype=torch.float)\n",
        "with torch.no_grad():\n",
        "    action_logprobs = policy(state_.to(device))\n",
        "    action_distribution = Categorical(logits=action_logprobs)\n",
        "    action = action_distribution.sample().item()\n",
        "state, reward, done, info = env.step(action)\n",
        "print(reward)\n",
        "# episode_reward += reward\n",
        "\n",
        "\n",
        "# episodes = 5\n",
        "# for ep in range(episodes):\n",
        "#     obs = env.reset()\n",
        "#     done = False\n",
        "#     while not done:\n",
        "#         action, _states = policy(torch.from_numpy(obs).to(device))\n",
        "#         obs, rewards, done, info = env.step(action)\n",
        "#         # env.render()\n",
        "#         print(rewards)"
      ],
      "metadata": {
        "id": "GHo8gVi0jgKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "simple_upsidedown_perceiverio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a80c3d8424d4fce9c44987479bc708f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ace4d9e1ba6e462ba38d645653552710",
              "IPY_MODEL_0b9e9a7cfdbf4edca9c57f391bdf3a2d",
              "IPY_MODEL_05918e8642f541a0a99320714fae8074"
            ],
            "layout": "IPY_MODEL_7c7c32360c1a42cabc95cbdfb984b3c2"
          }
        },
        "ace4d9e1ba6e462ba38d645653552710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a67a80c1bb431487c5aa61a9815c9e",
            "placeholder": "​",
            "style": "IPY_MODEL_4a05454ffed040b388afb1c5cc5e76e5",
            "value": "100%"
          }
        },
        "0b9e9a7cfdbf4edca9c57f391bdf3a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cfc2c18b926402f8370ba821bf03eb8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b54ef58e3164a8cafaa32a46c134125",
            "value": 10
          }
        },
        "05918e8642f541a0a99320714fae8074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a087d90bc774a6dba445c552170e2c9",
            "placeholder": "​",
            "style": "IPY_MODEL_c0a96ba4791145c391c4405af8ff88e6",
            "value": " 10/10 [00:00&lt;00:00, 33.61it/s]"
          }
        },
        "7c7c32360c1a42cabc95cbdfb984b3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a67a80c1bb431487c5aa61a9815c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a05454ffed040b388afb1c5cc5e76e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cfc2c18b926402f8370ba821bf03eb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b54ef58e3164a8cafaa32a46c134125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a087d90bc774a6dba445c552170e2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a96ba4791145c391c4405af8ff88e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lucid_perceiverio_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ncK_m9R1Pxsb"
      ],
      "authorship_tag": "ABX9TyNP/wYVYyiIkpt8133GFP92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/lucid_perceiverio_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "D4ul4ka0PtZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/pdf/2107.14795.pdf\n",
        "# https://github.com/lucidrains/perceiver-pytorch\n",
        "!pip install einops\n",
        "from math import pi, log\n",
        "from functools import wraps\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hca5OBdLmv0",
        "outputId": "a5450397-12ca-4ce6-e28d-4c01b17bad9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### helpers"
      ],
      "metadata": {
        "id": "ncK_m9R1Pxsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvAaHBGgK2qY"
      },
      "outputs": [],
      "source": [
        "# helpers\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cache_fn(f):\n",
        "    cache = None\n",
        "    # @wraps(f)\n",
        "    def cached_fn(*args, _cache = True, **kwargs):\n",
        "        if not _cache:\n",
        "            return f(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = f(*args, **kwargs)\n",
        "        return cache\n",
        "    return cached_fn\n",
        "\n",
        "# helper classes\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        if exists(self.norm_context):\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context = normed_context)\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None):\n",
        "        h = self.heads\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIO"
      ],
      "metadata": {
        "id": "DsVq7W_oPzzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PerceiverIO class save\n",
        "class PerceiverIO(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        dim,\n",
        "        queries_dim,\n",
        "        logits_dim = None,\n",
        "        num_latents = 512,\n",
        "        latent_dim = 512,\n",
        "        cross_heads = 1,\n",
        "        latent_heads = 8,\n",
        "        cross_dim_head = 64,\n",
        "        latent_dim_head = 64,\n",
        "        weight_tie_layers = False,\n",
        "        decoder_ff = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([get_latent_attn(**cache_args), get_latent_ff(**cache_args)]))\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
        "\n",
        "    def forward(self, data, mask = None, queries = None):\n",
        "        b, *_, device = *data.shape, data.device\n",
        "        x = repeat(self.latents, 'n d -> b n d', b = b)\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        # cross attention only happens once for Perceiver IO\n",
        "        x = cross_attn(x, context = data, mask = mask) + x\n",
        "        x = cross_ff(x) + x\n",
        "        # layers\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "        if not exists(queries):\n",
        "            return x\n",
        "        # make sure queries contains batch dimension\n",
        "        if queries.ndim == 2:\n",
        "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
        "        # cross attend from decoder queries to latents\n",
        "        latents = self.decoder_cross_attn(queries, context = x)\n",
        "        if exists(self.decoder_ff):\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "        return self.to_logits(latents)\n",
        "\n",
        "def preprocess(X):\n",
        "    if X.dim()==1:\n",
        "        X=X.unsqueeze(dim=0)\n",
        "    X=X.flatten(start_dim=1, end_dim=-1) #(start_dim=1)\n",
        "    X=X.unsqueeze(dim=1)\n",
        "    return X\n",
        "\n",
        "def postprocess(logits):\n",
        "    if logits.dim()==3:\n",
        "        logits=logits.squeeze(dim=1)\n",
        "    return logits\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6_r7WRKMLcc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PerceiverIO model save\n",
        "model = PerceiverIO(\n",
        "    dim = 28*28,                    # dimension of sequence to be encoded\n",
        "    queries_dim = 10,            # dimension of decoder queries\n",
        "    logits_dim = None,            # dimension of final logits\n",
        "    depth = 6,                   # depth of net\n",
        "    num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 128,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        ").to(device)\n",
        "\n",
        "seq = torch.randn(5, 1, 28*28, device=device)\n",
        "queries = torch.zeros(1, 10, device=device)\n",
        "\n",
        "seq = preprocess(seq) #[512,1,10]\n",
        "logits = model(seq, queries = None) # \n",
        "# logits = model(seq, queries = queries) # \n",
        "logprobs = postprocess(logits) #[512, 4]\n",
        "print(logprobs.shape)\n",
        "\n",
        "# none\n",
        "# in forward torch.Size([128, 128]) 5\n",
        "# torch.Size([5, 128, 128])\n",
        "# torch.Size([5, 128, 128])\n",
        "# torch.Size([5, 128, 128])\n",
        "# torch.Size([5, 128, 128])\n",
        "\n",
        "# queries\n",
        "# in forward torch.Size([128, 128]) 5\n",
        "# torch.Size([5, 128, 128])\n",
        "# torch.Size([5, 128, 128])\n",
        "# torch.Size([5, 128, 128])\n",
        "# torch.Size([5, 10])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gh_4wccN-XTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d083618-8820-41d3-a112-850fa7c7486c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIOrnn"
      ],
      "metadata": {
        "id": "hqxWCz07-Yv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PerceiverIOrnn(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        dim,\n",
        "        queries_dim,\n",
        "        logits_dim = None,\n",
        "        num_latents = 512,\n",
        "        latent_dim = 512,\n",
        "        cross_heads = 1,\n",
        "        latent_heads = 8,\n",
        "        cross_dim_head = 64,\n",
        "        latent_dim_head = 64,\n",
        "        weight_tie_layers = False,\n",
        "        decoder_ff = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "        self.latents = torch.zeros(num_latents, latent_dim)\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([get_latent_attn(**cache_args), get_latent_ff(**cache_args)]))\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
        "\n",
        "    def forward(self, data, mask = None, queries = None, x = None):\n",
        "        b, *_, device = *data.shape, data.device\n",
        "        if x == None: x = repeat(self.latents, 'n d -> b n d', b = b).to(device)\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        # cross attention only happens once for Perceiver IO\n",
        "        x = cross_attn(x, context = data, mask = mask) + x\n",
        "        x = cross_ff(x) + x\n",
        "        # layers\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "        if not exists(queries):\n",
        "            return x\n",
        "        # make sure queries contains batch dimension\n",
        "        if queries.ndim == 2:\n",
        "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
        "        # cross attend from decoder queries to latents\n",
        "        latents = self.decoder_cross_attn(queries, context = x)\n",
        "        if exists(self.decoder_ff):\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "        # return self.to_logits(latents)\n",
        "        return x, self.to_logits(latents)\n",
        "\n",
        "def preprocess(X):\n",
        "    if X.dim()==1:\n",
        "        X=X.unsqueeze(dim=0)\n",
        "    X=X.flatten(start_dim=1, end_dim=-1) #(start_dim=1)\n",
        "    X=X.unsqueeze(dim=1)\n",
        "    return X\n",
        "\n",
        "def postprocess(logits):\n",
        "    if logits.dim()==3:\n",
        "        logits=logits.squeeze(dim=1)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "i8Lh8uAVLixh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "j5QWXVURP2n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# input_size – The number of expected features in the input x\n",
        "# hidden_size – The number of features in the hidden state h\n",
        "# num_layers – Number of recurrent layers\n",
        "\n",
        "# rnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers)\n",
        "# input = torch.randn(5, 3, 10) # batch, input_size\n",
        "# h0 = torch.randn(2, 3, 20)\n",
        "# c0 = torch.randn(2, 3, 20)\n",
        "# output, (hn, cn) = rnn(input, (h0, c0))\n",
        "\n",
        "# rnn = nn.LSTMCell(10, 20) # (input_size, hidden_size)\n",
        "# input = torch.randn(2, 3, 10) # (time_steps, batch, input_size)\n",
        "# hx = torch.randn(3, 20) # (batch, hidden_size)\n",
        "# cx = torch.randn(3, 20)\n",
        "# output = []\n",
        "# for i in range(input.size()[0]):\n",
        "#     hx, cx = rnn(input[i], (hx, cx))\n",
        "#     output.append(hx)\n",
        "# output = torch.stack(output, dim=0)\n",
        "\n",
        "\n",
        "# seq = torch.randn(5, 1, 28,28, device=device) # batch, rgb, h, w\n",
        "# # b, in_channels, *axis, device, dtype = *data.shape, data.device, data.dtype # 4 [224, 224] cpu torch.float32\n",
        "# b, in_channels, h,w, device, dtype = *data.shape, data.device, data.dtype # 4 [224, 224] cpu torch.float32\n",
        "\n",
        "# # in_shape # mario (240, 256)\n",
        "# self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "# hx = torch.zeros(1, 256).to(device)\n",
        "# a3c_hx1, a3c_cx1 = self.lstm(vec_st, (a3c_hx, a3c_cx)) # [1, 320], ([1, 256],[1, 256])\n",
        "\n",
        "input_size = 28\n",
        "seq = torch.randn(1, input_size, device=device)\n",
        "\n",
        "model = PerceiverIOrnn(\n",
        "    # dim = h*w,                    # dimension of sequence to be encoded\n",
        "    dim = input_size,                    # dimension of sequence to be encoded\n",
        "    queries_dim = 10,            # dimension of decoder queries\n",
        "    logits_dim = None,            # dimension of final logits\n",
        "    depth = 1,                   # depth of net\n",
        "    num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 128,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        ").to(device)\n",
        "\n",
        "seq = preprocess(seq) #[512,1,10]\n",
        "latent = model(seq, queries = None) # \n",
        "# x = nn.Parameter(torch.randn(num_latents=512, latent_dim=512))\n",
        "for x in range(5):\n",
        "    # seq = torch.randn(1, 28*28, device=device)\n",
        "    seq = torch.randn(1, input_size, device=device)\n",
        "    seq = preprocess(seq) #[512,1,10]\n",
        "    latent = model(seq, queries = None, x=latent) # \n",
        "queries = torch.zeros(1, 10, device=device)\n",
        "latent, logits = model(seq, queries = queries) # \n",
        "print(logits)\n",
        "print(logits.shape)\n",
        "logprobs = postprocess(logits) #[512, 4]\n",
        "print(logprobs.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abEd-dTVHLyr",
        "outputId": "a59799f0-2339-41de-ad61-4dbfaf97ef7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0194, -0.2600, -0.0958,  0.1818, -0.2351,  1.0639,  0.5648,\n",
            "           0.1742, -0.0940,  0.1996]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([1, 1, 10])\n",
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### rnn"
      ],
      "metadata": {
        "id": "YZvTsH25hJ7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# https://github.com/python-engineer/pytorch-examples/blob/master/rnn-lstm-gru/main.py\n",
        "\n",
        "train_data = torchvision.datasets.FashionMNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "test_data = torchvision.datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)#, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)#, shuffle=False)\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "\n"
      ],
      "metadata": {
        "id": "bJAiRZgYiGGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### rnn lstm"
      ],
      "metadata": {
        "id": "mC3sJdvjCleR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rnn lstm\n",
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        # out, _ = self.rnn(x, h0)\n",
        "        out, _ = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "# print(model)\n"
      ],
      "metadata": {
        "id": "OTCzpIxiTHqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PIORNN"
      ],
      "metadata": {
        "id": "vbOwfNZVzD0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PIORNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(PIORNN, self).__init__()\n",
        "        self.cell = PerceiverIOrnn(\n",
        "            dim = input_size,                    # dimension of sequence to be encoded\n",
        "            queries_dim = num_classes,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 32,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "            )#.to(device)\n",
        "    def postprocess(self, logits):\n",
        "        if logits.dim()==3:\n",
        "            logits=logits.squeeze(dim=1)\n",
        "        return logits\n",
        "    def forward(self, input): # input = torch.randn(64, 28, 28, device=device)\n",
        "        if input.dim()==2:\n",
        "            input=input.unsqueeze(0)\n",
        "        if input.dim() not in [2,3]: print(\"erm\")\n",
        "        b,h,w = input.shape\n",
        "        latent=None\n",
        "        for i in range(h):  # dim=1\n",
        "            seq = input[:, i, :]\n",
        "            seq = preprocess(seq) #[b,1,num_classes]\n",
        "            latent = self.cell(seq, queries = None, x=latent) # \n",
        "        queries = torch.zeros(1, 10, device=device)\n",
        "        latent, logits = self.cell(seq, queries = queries) # \n",
        "        logprobs = postprocess(logits) #\n",
        "        pred_probab = nn.Softmax(dim=1)(logprobs)\n",
        "        outputs = pred_probab # [b, num_classes]\n",
        "        # y_pred = pred_probab.argmax(1).float() # [b]\n",
        "        return outputs\n",
        "\n",
        "model = PIORNN(28, 10).to(device)\n",
        "input = torch.randn(64, 28, 28, device=device)\n",
        "\n",
        "output=model(input)\n",
        "# print(output.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dKOHAWONrHc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjsoOdP9Kikj",
        "outputId": "52e6f247-c7f9-41db-a4b8-79d8feed7ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.037300  [    0/60000]\n",
            "loss: 2.020780  [ 6400/60000]\n",
            "loss: 1.984653  [12800/60000]\n",
            "loss: 2.048312  [19200/60000]\n",
            "loss: 2.015564  [25600/60000]\n",
            "loss: 2.002955  [32000/60000]\n",
            "loss: 1.955463  [38400/60000]\n",
            "loss: 1.997072  [44800/60000]\n",
            "loss: 1.990889  [51200/60000]\n",
            "loss: 2.003390  [57600/60000]\n",
            "Accuracy of the network on the 10000 test images: 44.17 %\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(train_loader, model, loss_fn, optimizer):\n",
        "    n_total_steps = len(train_loader)\n",
        "    size = len(train_loader.dataset)\n",
        "    model.train()\n",
        "    # https://stackoverflow.com/questions/69428646/pytorch-gpu-what-am-i-forgetting-to-move-over-to-the-gpu\n",
        "    for batch, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device) # origin shape: [N, 1, 28, 28] resized: [N, 28, 28]\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # print(outputs.shape, labels.shape)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(images)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(test_loader, model, loss_fn):\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        for X, y in test_loader:\n",
        "            X = X.reshape(-1, sequence_length, input_size).to(device)\n",
        "            y = y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            n_samples += y.size(0)\n",
        "            n_correct += (predicted == y).sum().item()\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "        print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
        "\n",
        "# def test(test_loader, model, loss_fn):\n",
        "#     size = len(test_loader.dataset)\n",
        "#     num_batches = len(test_loader)\n",
        "#     model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for X, y in test_loader:\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             X = torch.squeeze(X)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "train(train_loader, model, loss_fn, optimizer)\n",
        "test(test_loader, model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 2\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    test(train_loader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "# print(\"Saved PyTorch Model State to model.pth\")\n",
        "# model = NeuralNetwork()\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# og rnn 10000 test images: 86.515 %"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYrg-SA7l1fi",
        "outputId": "69766d9b-3628-441e-e0c2-6f670e49c3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.965151  [    0/60000]\n",
            "loss: 2.002404  [ 6400/60000]\n",
            "loss: 1.935071  [12800/60000]\n",
            "loss: 1.966725  [19200/60000]\n",
            "loss: 1.973865  [25600/60000]\n",
            "loss: 2.000520  [32000/60000]\n",
            "loss: 1.962388  [38400/60000]\n",
            "loss: 1.981493  [44800/60000]\n",
            "loss: 1.905448  [51200/60000]\n",
            "loss: 1.963466  [57600/60000]\n",
            "Accuracy of the network on the 10000 test images: 45.20666666666666 %\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.957975  [    0/60000]\n",
            "loss: 2.008984  [ 6400/60000]\n",
            "loss: 1.916686  [12800/60000]\n",
            "loss: 1.966338  [19200/60000]\n",
            "loss: 1.970482  [25600/60000]\n",
            "loss: 1.999395  [32000/60000]\n",
            "loss: 1.963080  [38400/60000]\n",
            "loss: 1.973942  [44800/60000]\n",
            "loss: 1.935835  [51200/60000]\n",
            "loss: 1.947677  [57600/60000]\n",
            "Accuracy of the network on the 10000 test images: 46.373333333333335 %\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\",]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "import random\n",
        "n=random.randint(0,1000)\n",
        "print(n)\n",
        "x, y = test_data[n][0], test_data[n][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaJMDQQl8fF",
        "outputId": "9c1f6ed3-4755-42af-ec32-d77081204424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "331\n",
            "Predicted: \"Sneaker\", Actual: \"Sneaker\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/\n",
        "# https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification\n",
        "dataloader=valid_dataloader\n",
        "predictions_labels = []\n",
        "true_labels = []\n",
        "total_loss = 0\n",
        "model.train() # set model to train mode\n",
        "# for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "# for batch, (images, labels) in enumerate(dataloader):\n",
        "for batch, labels in enumerate(dataloader):\n",
        "    # true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "    # batch = {k:v.type(torch.long).to(device) for k,v in batch.items()} # move batch to device\n",
        "    model.zero_grad() # clear previously calculated gradients before backward pass.\n",
        "    # print(batch.items())\n",
        "    # print(batch.keys())\n",
        "    # print(batch['labels'])\n",
        "    # print(batch['attention_mask'])\n",
        "    # print(batch['input_ids'])\n",
        "    outputs = model(labels)\n",
        "    # outputs = model(**batch)\n",
        "    loss, logits = outputs[:2]\n",
        "    total_loss += loss.item() #`.item()` returns the Python value from the tensor.\n",
        "    loss.backward() # back propagate loss\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip norm of gradients to 1.0 to help prevent the \"exploding gradients\"\n",
        "    optimizer_.step()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "avg_epoch_loss = total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "lK5YTwjt_MsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
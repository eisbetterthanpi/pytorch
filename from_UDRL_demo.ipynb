{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/from_UDRL_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv4NQbMZSgeT"
      },
      "source": [
        "#### Setup some basic dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2vicGMBmduX"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y torch gym\n",
        "# !pip install torch==1.4.0+cpu gym[box2d]==0.15.4 tqdm sortedcontainers -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!pip install gym[box2d] tqdm sortedcontainers\n",
        "# https://arxiv.org/pdf/1912.02877.pdf\n",
        "# https://colab.research.google.com/drive/1ynS9g7YzFpNSwhva2_RDKYLjyGckCA8H\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # \n",
        "wandb.init(project=\"from_UDRL\", entity=\"bobdole\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCCOEthAqwrP"
      },
      "source": [
        "#### Replay Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Y2K3Ul0Op_jJ"
      },
      "outputs": [],
      "source": [
        "# Replay Utilities\n",
        "import copy, os, pickle\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from sortedcontainers import SortedListWithKey\n",
        "\n",
        "class Episode:\n",
        "    \"\"\"For any episode, this container has len(actions) == len(rewards) == len(states) - 1\n",
        "    This is because we initialize using the starting state.\n",
        "    The add() method adds the action just taken, the obtained reward, and the **next** state.\n",
        "    This makes accessing the episode data simple:\n",
        "    states[0] is the first state\n",
        "    actions[0] is the action taken in that state\n",
        "    rewards[0] reward obtained by taking the action, and so on\n",
        "    The last state added is never actually processed by the agent.\"\"\"\n",
        "    def __init__(self, init_state, desired_return, desired_horizon):\n",
        "        self.states = [init_state]\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.desired_return = desired_return\n",
        "        self.desired_horizon = desired_horizon\n",
        "\n",
        "    def add(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    @property\n",
        "    def total_reward(self):\n",
        "        return sum(self.rewards)\n",
        "\n",
        "    @property\n",
        "    def steps(self):\n",
        "        return len(self.actions)\n",
        "\n",
        "    @property\n",
        "    def return_gap(self):\n",
        "        return self.desired_return - self.total_reward\n",
        "\n",
        "    @property\n",
        "    def horizon_gap(self):\n",
        "        return self.desired_horizon - self.steps\n",
        "\n",
        "\n",
        "def get_reward(episode: Episode):\n",
        "    return episode.total_reward\n",
        "\n",
        "def make_replay(config):\n",
        "    if config.replay == 'highest':\n",
        "        replay = HighestReplay(max_size=config.replay_size)\n",
        "    elif config.replay == 'recent':\n",
        "        replay = RecentReplay(max_size=config.replay_size)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return replay\n",
        "\n",
        "\n",
        "class Replay(ABC):\n",
        "    def __init__(self):\n",
        "        self.episodes = []\n",
        "        self.known_returns = []\n",
        "        self.known_horizons = []\n",
        "\n",
        "    @abstractmethod\n",
        "    def add(self, episode):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def best_episode(self):\n",
        "        return max(self.episodes, key=get_reward)\n",
        "\n",
        "    def get_closest_horizon(self, desired_return):\n",
        "        idx = np.abs(np.asarray(self.known_returns) - desired_return).argmin()\n",
        "        return self.known_horizons[idx]\n",
        "\n",
        "    @property\n",
        "    def returns(self):\n",
        "        return [episode.total_reward for episode in self.episodes]\n",
        "\n",
        "\n",
        "class HighestReplay(Replay):\n",
        "    def __init__(self, max_size: int):\n",
        "        super().__init__()\n",
        "        self.episodes = SortedListWithKey(key=get_reward)\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def add(self, episode: Episode):\n",
        "        self.episodes.add(episode)\n",
        "        self.known_returns.append(episode.total_reward)\n",
        "        self.known_horizons.append(episode.steps)\n",
        "        if len(self.episodes) > self.max_size:\n",
        "            self.episodes.pop(0)\n",
        "\n",
        "\n",
        "def trailing_segments(episode: Episode, nprnd: np.random.RandomState):\n",
        "    steps = episode.steps\n",
        "    i = nprnd.randint(0, steps)\n",
        "    j = steps\n",
        "    return episode.states[i], sum(episode.rewards[i:j]), (j - i), episode.actions[i]\n",
        "\n",
        "def sample_batch(replay: Replay, batch_size: int, nprnd: np.random.RandomState):\n",
        "    idxs = nprnd.randint(0, len(replay.episodes), batch_size)\n",
        "    episodes = [replay.episodes[idx] for idx in idxs]\n",
        "    segments = [trailing_segments(episode, nprnd) for episode in episodes]\n",
        "    states, desired_rewards, horizons, actions = [], [], [], []\n",
        "    for state, desired_reward, horizon, action in segments:\n",
        "        states.append(state)\n",
        "        desired_rewards.append(desired_reward)\n",
        "        horizons.append(horizon)\n",
        "        actions.append(action)\n",
        "    states = np.array(states, dtype=np.float32)\n",
        "    desired_rewards = np.array(desired_rewards, dtype=np.float32)[:, None]\n",
        "    horizons = np.array(horizons, dtype=np.float32)[:, None]\n",
        "    actions = np.array(actions, dtype=np.float32)\n",
        "    return states, desired_rewards, horizons, actions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc1B-S01q-RI"
      },
      "source": [
        "### Behavior Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSQ2Nd4Ppprs"
      },
      "outputs": [],
      "source": [
        "# Behavior Function\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import orthogonal_\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Categorical:\n",
        "    def __init__(self, dim: int):\n",
        "        self.dim = dim\n",
        "        self.loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    @staticmethod\n",
        "    def distribution(probs):\n",
        "        return torch.distributions.Categorical(probs)\n",
        "\n",
        "    def sample(self, scores):\n",
        "        probs = F.softmax(scores, dim=1)\n",
        "        dist = self.distribution(probs)\n",
        "        sample = dist.sample().item()                     # item() forces single env\n",
        "        return sample\n",
        "\n",
        "    def mode(self, scores):\n",
        "        probs = F.softmax(scores, dim=1)\n",
        "        mode = probs.to('cpu').data.numpy()[0].argmax()   # [0] forces single env\n",
        "        return mode\n",
        "\n",
        "    def random_sample(self):\n",
        "        return torch.randint(0, self.dim, (1,)).item()    # (1,) & item() force single env\n",
        "\n",
        "    def clip(self, action):\n",
        "        return action\n",
        "\n",
        "\n",
        "class ScaledIntent:\n",
        "    def __init__(self, return_scale: float, horizon_scale: float, max_return: float):\n",
        "        self.return_scale = return_scale\n",
        "        self.horizon_scale = horizon_scale\n",
        "        self.max_return = max_return\n",
        "\n",
        "    def __call__(self, intent):\n",
        "        _intent = np.zeros_like(intent)\n",
        "        returns = np.minimum(intent[:, 0], self.max_return)\n",
        "        horizons = np.maximum(intent[:, 1], 1)\n",
        "        _intent[:, 0] = returns * self.return_scale\n",
        "        _intent[:, 1] = horizons * self.horizon_scale\n",
        "        intent = _intent.astype(np.float32)\n",
        "        return intent\n",
        "\n",
        "\n",
        "def make_behavior_fn(config, nprnd: np.random.RandomState, device: torch.device):\n",
        "    intent_transform = ScaledIntent(config.return_scale, config.horizon_scale, config.env_max_return)\n",
        "    behavior_fn = BehaviorFn(ProductNetwork(config),\n",
        "                             intent_transform=intent_transform,\n",
        "                             config=config,\n",
        "                             nprnd=nprnd,\n",
        "                             device=device)\n",
        "    return behavior_fn\n",
        "\n",
        "\n",
        "class BehaviorFn(nn.Module):\n",
        "    def __init__(self, net: nn.Module, intent_transform: ScaledIntent, config, nprnd: np.random.RandomState, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.intent_transform = intent_transform\n",
        "        self.nprnd = nprnd\n",
        "        self.device = device\n",
        "        self.state_dtype = np.float32\n",
        "        if config.action_type == 'discrete':\n",
        "            self.action_dist = Categorical(config.n_action)\n",
        "            self.action_dtype = np.int64\n",
        "        else:\n",
        "            raise NotImplementedError(config.action_type)\n",
        "\n",
        "    def forward(self, state, desired_reward, horizon, device=None):\n",
        "        if device is None: device = self.device\n",
        "        state = np.asarray(state)\n",
        "        intent = np.concatenate([desired_reward, horizon], axis=1)\n",
        "        transformed_intent = self.intent_transform(intent)\n",
        "        state = self.make_variable(state, dtype=self.state_dtype, device=device)\n",
        "        intent = self.make_variable(transformed_intent, dtype=self.state_dtype, device=device)\n",
        "        net_output = self.net(state, intent)\n",
        "        if hasattr(self, 'logstd'):\n",
        "            net_output = torch.cat((net_output, self.logstd.expand_as(net_output)), dim=-1)\n",
        "        return net_output\n",
        "\n",
        "    def loss(self, states, desired_rewards, horizons, actions):\n",
        "        outputs = self(states, desired_rewards, horizons)\n",
        "        targets = self.make_variable(actions, dtype=self.action_dtype)\n",
        "        loss = self.action_dist.loss(outputs, targets)\n",
        "        return loss\n",
        "\n",
        "    def make_variable(self, x, dtype, device=None):\n",
        "        if device is None: device = self.device\n",
        "        return torch.from_numpy(np.asarray(x, dtype=dtype)).to(device)\n",
        "\n",
        "\n",
        "class ProductNetwork(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        activation, n_state, n_output, net_arch = config.activation, config.n_state, config.n_action, config.net_arch\n",
        "        self.n_state = n_state\n",
        "        self.net_option = config.net_option\n",
        "        n_proj = net_arch[0]\n",
        "        if activation == 'relu':\n",
        "            activation = nn.ReLU\n",
        "            gain = np.sqrt(2)\n",
        "        elif activation == 'tanh':\n",
        "            activation = nn.Tanh\n",
        "            gain = 1.0\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.layer1 = FastWeightLayer(n_proj, n_state, 2, activation, option=self.net_option)\n",
        "        hids = []\n",
        "        n_last = n_proj\n",
        "        for n_current in net_arch[1:]:\n",
        "            hids += [nn.Linear(n_last, n_current), activation()]\n",
        "            n_last = n_current\n",
        "        self.hids = nn.Sequential(*hids)\n",
        "        self.op = nn.Linear(n_last, n_output)\n",
        "        self.init_params(gain)\n",
        "\n",
        "    def forward(self, state, intent):\n",
        "        out = self.layer1(state, intent)\n",
        "        out = self.hids(out) if len(self.hids) > 0 else out\n",
        "        out = self.op(out)\n",
        "        return out\n",
        "\n",
        "    def init_params(self, gain):\n",
        "        def init(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                orthogonal_(m.weight.data, gain=gain)\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "        def init_hyper(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                orthogonal_(m.weight.data, gain=1.0)\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "        self.layer1.apply(init_hyper)\n",
        "        self.hids.apply(init)\n",
        "\n",
        "\n",
        "class FastWeightLayer(nn.Module):\n",
        "    def __init__(self, size, x_size, c_size, activation, option):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.x_size = x_size\n",
        "        self.c_size = c_size\n",
        "        self.option = option\n",
        "        self.activation = activation\n",
        "        if option == 'bilinear':\n",
        "            self.Wlinear = nn.Linear(c_size, self.size * self.x_size)\n",
        "            self.blinear = nn.Linear(c_size, self.size)\n",
        "        elif option == 'gated':\n",
        "            self.xlinear = nn.Linear(x_size, size)\n",
        "            self.clinear = nn.Linear(c_size, size)\n",
        "        else:\n",
        "            raise NotImplementedError(option)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        if self.option == 'bilinear':\n",
        "            batch_size = x.shape[0]\n",
        "            W, b = self.Wlinear(c), self.blinear(c)\n",
        "            W = torch.reshape(W, (batch_size, self.x_size, self.size))\n",
        "            x = torch.reshape(x, (batch_size, 1, self.x_size))  # add a dimension for matmul, then remove it\n",
        "            output = self.activation()(torch.matmul(x, W).reshape((batch_size, self.size)) + b)\n",
        "        elif self.option == 'gated':\n",
        "            x_proj = self.activation()(self.xlinear(x))\n",
        "            c_proj = torch.sigmoid(self.clinear(c))\n",
        "            output = x_proj * c_proj\n",
        "        else:\n",
        "            raise NotImplementedError(self.option)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MynJcnRkrG2l"
      },
      "source": [
        "### Agent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDbs9BoGnbrX"
      },
      "outputs": [],
      "source": [
        "# Agent Implementation\n",
        "from collections import deque\n",
        "from typing import List, Tuple, Union\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from gym.core import Wrapper\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "class SeedEnv(Wrapper):\n",
        "    \"\"\" Every reset() set a new seed from a given seed range \"\"\"\n",
        "    def __init__(self, env, seed_range):\n",
        "        super().__init__(env)\n",
        "        self.seed_range = seed_range\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.seed(np.random.randint(*self.seed_range))\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "\n",
        "def get_stats(scalar_list: list) -> dict:\n",
        "    if len(scalar_list) == 0:\n",
        "        stats = {key: np.nan for key in ('max', 'mean', 'median', 'min', 'std')}\n",
        "        stats['size'] = 0\n",
        "    else:\n",
        "        stats = {'max': np.max(scalar_list), 'mean': np.mean(scalar_list), 'median': np.median(scalar_list),\n",
        "                 'min': np.min(scalar_list), 'std': np.std(scalar_list, ddof=1), 'size': len(scalar_list)}\n",
        "    return stats\n",
        "\n",
        "\n",
        "class UpsideDownAgent:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.msg = print if config.verbose else lambda *a, **k: None\n",
        "        self.device = torch.device('cuda:0' if config.use_gpu and torch.cuda.is_available() else 'cpu')\n",
        "        self.msg('Using device', self.device)\n",
        "        seed = config.seed\n",
        "        self.nprnd = np.random.RandomState(seed)\n",
        "        np.random.seed(Config.seed)\n",
        "        torch.manual_seed(seed)\n",
        "        self.train_env = SeedEnv(gym.make(config.env_name), seed_range=config.train_seeds)\n",
        "        self.test_env  = SeedEnv(gym.make(config.env_name), seed_range=config.eval_seeds)\n",
        "        self.replay: Replay = make_replay(config)\n",
        "        self.behavior_fn: BehaviorFn = make_behavior_fn(config, self.nprnd, self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.behavior_fn.parameters(), lr=config.learning_rate)\n",
        "        self.iters = 0\n",
        "        self.total_episodes = 0\n",
        "        self.total_steps = 0\n",
        "        self.best_onpolicy_mean = np.array(-np.inf)\n",
        "        self.best_greedy_mean = np.array(-np.inf)\n",
        "        self.best_rolling_mean = np.array(-np.inf)\n",
        "        self.rolling_returns = deque(maxlen=config.n_eval_episodes)\n",
        "        self.current_step_limit = config.warmup_step_limit  # Used only for warm up inputs\n",
        "        self.current_desired_return = (config.warmup_desired_return, 0)\n",
        "\n",
        "    def warm_up(self) -> List[Tuple]:\n",
        "        results: List[Tuple] = []\n",
        "        episodes, _ = self.run_episodes(self.current_step_limit, self.current_desired_return, label='Warmup',\n",
        "                                        actions='random', n_episodes=self.config.n_warm_up_episodes)\n",
        "        self.total_episodes += self.config.n_warm_up_episodes\n",
        "        for episode in episodes:\n",
        "            self.replay.add(episode)\n",
        "            self.rolling_returns.append(episode.total_reward)\n",
        "        stats = get_stats(self.replay.returns)\n",
        "        self.msg(f\"\\nWarmup | Replay max: {stats['max']:7.2f} mean: {stats['mean']:7.2f} \"\n",
        "                 f\"min: {stats['min']:7.2f} size: {stats['size']:3}\")\n",
        "        results += [('replay.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min', 'size']]\n",
        "        return results\n",
        "\n",
        "    def train_step(self) -> List[Tuple]:\n",
        "        results: List[Tuple] = []\n",
        "        n_updates = self.config.n_updates_per_iter\n",
        "        self.msg(f'\\nIteration {(self.iters + 1):3} | Training for {n_updates} updates')\n",
        "        # Learn behavior function\n",
        "        torch.set_grad_enabled(True)\n",
        "        self.behavior_fn.to(self.device)\n",
        "        self.behavior_fn.train()\n",
        "        loss = None\n",
        "        tq = trange(n_updates, disable=self.config.verbose is not True)\n",
        "        losses = []\n",
        "        for u in tq:\n",
        "            self.optimizer.zero_grad()\n",
        "            s, r, h, a = sample_batch(self.replay, self.config.batch_size, self.nprnd)\n",
        "            loss = self.behavior_fn.loss(s, r, h, a)\n",
        "            losses.append(loss.item())\n",
        "            tq.set_postfix(loss=loss.item())\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        results += [('loss', loss.item(), self.total_steps)]\n",
        "        # Generate more data\n",
        "        last_few_episodes = self.replay.episodes[-self.config.last_few:]\n",
        "        last_few_durations = [episodes.steps for episodes in last_few_episodes]\n",
        "        last_few_returns = [episodes.total_reward for episodes in last_few_episodes]\n",
        "        # self.current_step_limit = np.int(np.mean(last_few_durations))\n",
        "        self.current_step_limit = int(np.mean(last_few_durations))\n",
        "        self.current_desired_return = (np.mean(last_few_returns), np.std(last_few_returns))\n",
        "        episodes, eval_results = self.run_episodes(self.current_step_limit, self.current_desired_return,\n",
        "                                                   label='Train', actions=self.config.actions,\n",
        "                                                   n_episodes=self.config.n_episodes_per_iter)\n",
        "        self.total_episodes += self.config.n_episodes_per_iter\n",
        "        returns = []\n",
        "        for episode in episodes:\n",
        "            self.replay.add(episode)\n",
        "            episode_return = episode.total_reward\n",
        "            returns.append(episode_return)\n",
        "            self.rolling_returns.append(episode_return)\n",
        "        del episodes\n",
        "        # Logging\n",
        "        self.iters += 1\n",
        "        results += eval_results\n",
        "        rolling_mean = np.mean(self.rolling_returns)\n",
        "        results += [('rollouts.rolling_mean', rolling_mean, self.total_steps)]\n",
        "        self.best_rolling_mean = rolling_mean if rolling_mean > self.best_rolling_mean else self.best_rolling_mean\n",
        "        stats = get_stats(self.replay.returns)\n",
        "        self.msg(f\"Iteration {self.iters:3} | \"\n",
        "                 f\"Rollouts max: {np.max(returns):7.2f} mean: {np.mean(returns):7.2f} min: {np.min(returns):7.2f} | \"\n",
        "                 f\"Replay max: {stats['max']:7.2f} mean: {stats['mean']:7.2f} \"\n",
        "                 f\"min: {stats['min']: 7.2f} size: {stats['size']:3} | \"\n",
        "                 f\"steps so far: {self.total_steps:7} episodes so far: {self.total_episodes:6} | \"\n",
        "                 f\"Rolling Mean ({len(self.rolling_returns)}): {rolling_mean:7.2f}\")\n",
        "        results += [('iteration', self.iters, self.total_steps)]\n",
        "        results += [('current_step_limit', self.current_step_limit, self.total_steps)]\n",
        "        results += [('current_desired_return.mean', np.mean(last_few_returns), self.total_steps)]\n",
        "        results += [('current_desired_return.std', np.std(last_few_returns), self.total_steps)]\n",
        "        results += [('replay.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min', 'size']]\n",
        "        stats = get_stats(returns)\n",
        "        results += [('rollouts.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min']]\n",
        "        results += [('total_steps', self.total_steps, self.total_steps)]\n",
        "        return results\n",
        "\n",
        "    def _eval(self) -> List[Tuple]:\n",
        "        results: List[Tuple] = [('episodes', self.total_episodes, self.total_steps)]\n",
        "        if self.config.eval_goal == 'max':\n",
        "            desired_test_return = self.config.env_max_return\n",
        "        elif self.config.eval_goal == 'current':\n",
        "            desired_test_return = self.current_desired_return[0]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        actions = 'on_policy'  # can also evaluation with \"greedy\" actions here\n",
        "        self.msg(f'\\nTesting on {self.config.n_eval_episodes} episodes with {actions} actions')\n",
        "        episodes, _ = self.run_episodes(self.current_step_limit, desired_test_return, label='Test',\n",
        "                                        actions=actions, n_episodes=self.config.n_eval_episodes)\n",
        "        stats = get_stats([episode.total_reward for episode in episodes])\n",
        "        results += [(f'eval.{actions}.{k}', stats[k], self.total_steps)\n",
        "                    for k in ['max', 'median', 'mean', 'std', 'min']]\n",
        "        print(f\"Eval | {actions} | max: {stats['max']:7.2f} | median: {stats['median']:7.2f} | \"\n",
        "              f\"mean: {stats['mean']:7.2f} | std: {stats['std']: 7.2f} | min: {stats['min']:7.2f} | \"\n",
        "              f\"steps so far: {self.total_steps:7} | episodes so far: {self.total_episodes:6}\")\n",
        "        if actions == 'on_policy':\n",
        "            self.best_onpolicy_mean = max(stats['mean'], self.best_onpolicy_mean)\n",
        "        else:\n",
        "            self.best_greedy_mean = max(stats['mean'], self.best_greedy_mean)\n",
        "        del episodes, stats\n",
        "        return results\n",
        "\n",
        "\n",
        "    def run_episodes(self, step_limit: int, desired_return: Union[float, Tuple], label: str,\n",
        "                     actions: str, n_episodes: int = 1, render: bool = False) -> Tuple[List[Episode], List[Tuple]]:\n",
        "        assert label in ['Warmup', 'Train', 'Test']\n",
        "        assert actions in ['random', 'on_policy', 'greedy'] or actions.startswith('epsg')\n",
        "        behavior_fn, config, device, nprnd = self.behavior_fn, self.config, self.device, self.nprnd\n",
        "        torch.set_grad_enabled(False)\n",
        "        behavior_fn.eval()\n",
        "        behavior_fn.to(device)\n",
        "        episodes: List[Episode] = []\n",
        "        eval_results: List[Tuple] = []\n",
        "        env = self.test_env if label == 'Test' else self.train_env\n",
        "        for i in range(n_episodes):\n",
        "            episode_reward = 0 #for sparse\n",
        "            if isinstance(desired_return, tuple):\n",
        "                desired_return_final = desired_return[0] + desired_return[1] * nprnd.random_sample()\n",
        "            else:\n",
        "                desired_return_final = desired_return\n",
        "            if config.env_name == 'TakeCover-v0' or config.env_name == 'CartPoleContinuous-v0':\n",
        "                # desired_return_final = np.int(desired_return_final)\n",
        "                desired_return_final = int(desired_return_final)\n",
        "                step_limit = desired_return_final\n",
        "            # Prepare env\n",
        "            state = env.reset()\n",
        "            if render: env.render()\n",
        "            # Generate episode\n",
        "            episode = Episode(state, desired_return_final, step_limit)\n",
        "            done = False\n",
        "            while episode.steps < config.env_step_limit and not done:\n",
        "                state = np.asarray(state)\n",
        "                if actions == 'random':\n",
        "                    action = behavior_fn.action_dist.random_sample()\n",
        "                elif actions == 'on_policy':\n",
        "                    desired_return_remaining = np.array([[desired_return_final - episode.total_reward]])\n",
        "                    steps_remaining = np.array([[step_limit - episode.steps]])\n",
        "                    action_scores = behavior_fn(state[None],\n",
        "                                                desired_return_remaining,\n",
        "                                                steps_remaining,\n",
        "                                                device=device)\n",
        "                    action = behavior_fn.action_dist.sample(action_scores)\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "                clipped_action = behavior_fn.action_dist.clip(action)\n",
        "                state, reward, done, _ = env.step(clipped_action)\n",
        "\n",
        "                # for sparse\n",
        "                episode_reward += reward\n",
        "                if not done:\n",
        "                    reward = 0\n",
        "                else:\n",
        "                    reward = episode_reward\n",
        "\n",
        "                if render: env.render()\n",
        "                if label == 'Test':\n",
        "                    episode.add(0, 0, reward)  # reduce memory usage\n",
        "                else:\n",
        "                    episode.add(state, clipped_action, reward)\n",
        "                    self.total_steps += 1\n",
        "                    if label == 'Train' and self.total_steps % self.config.eval_freq == 0:\n",
        "                        eval_results += self._eval()\n",
        "\n",
        "            self.msg(f'{label} | {actions} | Episode {i:3} | '\n",
        "                     f'Goals: ({desired_return_final:7.2f}, {step_limit:4}) | '\n",
        "                     f'Return: {episode.total_reward:7.2f} Steps: {episode.steps:4} | '\n",
        "                     f'Return gap: {episode.return_gap:7.2f} Horizon gap: {episode.horizon_gap:5} ')\n",
        "            wandb.log({\"reward\": episode.total_reward})\n",
        "            episodes.append(episode)\n",
        "        return episodes, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZ2fdF9rMww"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "If4nafnjnFOf",
        "outputId": "60701213-2220-48ba-802f-23d231e6f07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warm-up complete. Starting training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval | on_policy | max:  264.42 | median:  -30.30 | mean:  -24.74 | std:   80.76 | min: -291.46 | steps so far:   50000 | episodes so far:    430\n",
            "Eval | on_policy | max:  199.56 | median:  -51.27 | mean:  -51.78 | std:   77.50 | min: -390.82 | steps so far:  100000 | episodes so far:    630\n",
            "Eval | on_policy | max:  251.04 | median:  -22.55 | mean:  -11.30 | std:   94.57 | min: -209.05 | steps so far:  150000 | episodes so far:    830\n",
            "Eval | on_policy | max:  240.09 | median:   -6.91 | mean:    2.34 | std:  118.41 | min: -394.51 | steps so far:  200000 | episodes so far:    990\n",
            "Eval | on_policy | max:  256.93 | median:    0.70 | mean:    8.06 | std:   97.11 | min: -329.71 | steps so far:  250000 | episodes so far:   1130\n",
            "Eval | on_policy | max:  260.32 | median:  -42.37 | mean:  -22.81 | std:  137.09 | min: -386.46 | steps so far:  300000 | episodes so far:   1270\n",
            "Eval | on_policy | max:  251.80 | median:    9.31 | mean:   34.31 | std:  116.73 | min: -274.23 | steps so far:  350000 | episodes so far:   1430\n",
            "Eval | on_policy | max:  259.35 | median:  -21.84 | mean:  -24.28 | std:  140.43 | min: -411.82 | steps so far:  400000 | episodes so far:   1570\n",
            "Eval | on_policy | max:  259.43 | median:   -3.59 | mean:   17.32 | std:  110.15 | min: -245.71 | steps so far:  450000 | episodes so far:   1710\n",
            "Eval | on_policy | max:  263.55 | median:   -9.05 | mean:    1.64 | std:  108.18 | min: -252.35 | steps so far:  500000 | episodes so far:   1850\n",
            "Eval | on_policy | max:  278.30 | median:   27.53 | mean:   52.65 | std:  117.70 | min: -241.54 | steps so far:  550000 | episodes so far:   1990\n",
            "Eval | on_policy | max:  242.86 | median:   -9.81 | mean:   17.90 | std:  109.92 | min: -176.25 | steps so far:  600000 | episodes so far:   2110\n",
            "Eval | on_policy | max:  262.50 | median:   24.60 | mean:   49.05 | std:  133.81 | min: -250.16 | steps so far:  650000 | episodes so far:   2250\n",
            "Eval | on_policy | max:  251.10 | median:   33.97 | mean:   51.25 | std:  122.07 | min: -231.26 | steps so far:  700000 | episodes so far:   2390\n",
            "Eval | on_policy | max:  239.89 | median:   39.35 | mean:   30.21 | std:  146.94 | min: -270.11 | steps so far:  750000 | episodes so far:   2510\n",
            "Eval | on_policy | max:  244.61 | median:    9.70 | mean:   26.14 | std:  126.82 | min: -261.85 | steps so far:  800000 | episodes so far:   2630\n",
            "Eval | on_policy | max:  268.46 | median:  124.46 | mean:   91.69 | std:  117.46 | min: -219.30 | steps so far:  850000 | episodes so far:   2750\n",
            "Eval | on_policy | max:  269.68 | median:    8.75 | mean:   27.80 | std:  138.38 | min: -274.09 | steps so far:  900000 | episodes so far:   2850\n",
            "Eval | on_policy | max:  292.22 | median:   34.26 | mean:   44.65 | std:  142.00 | min: -251.27 | steps so far:  950000 | episodes so far:   2970\n",
            "Eval | on_policy | max:  270.84 | median:   27.84 | mean:   57.06 | std:  126.82 | min: -263.91 | steps so far: 1000000 | episodes so far:   3090\n",
            "Eval | on_policy | max:  267.87 | median:   85.13 | mean:   58.40 | std:  127.56 | min: -253.58 | steps so far: 1050000 | episodes so far:   3190\n",
            "Eval | on_policy | max:  256.31 | median:   59.37 | mean:   71.68 | std:  122.67 | min: -298.00 | steps so far: 1100000 | episodes so far:   3310\n",
            "Eval | on_policy | max:  283.35 | median:   34.75 | mean:   59.63 | std:  127.52 | min: -253.39 | steps so far: 1150000 | episodes so far:   3410\n",
            "Eval | on_policy | max:  263.37 | median:   37.53 | mean:   54.80 | std:  118.23 | min: -205.44 | steps so far: 1200000 | episodes so far:   3510\n",
            "Eval | on_policy | max:  276.24 | median:   84.61 | mean:   76.91 | std:  121.37 | min: -275.28 | steps so far: 1250000 | episodes so far:   3630\n",
            "Eval | on_policy | max:  272.28 | median:   64.77 | mean:   42.11 | std:  141.59 | min: -255.21 | steps so far: 1300000 | episodes so far:   3730\n",
            "Eval | on_policy | max:  286.67 | median:  110.06 | mean:   74.14 | std:  139.53 | min: -237.37 | steps so far: 1350000 | episodes so far:   3830\n",
            "Eval | on_policy | max:  272.82 | median:   65.92 | mean:   55.07 | std:  130.76 | min: -258.06 | steps so far: 1400000 | episodes so far:   3950\n",
            "Eval | on_policy | max:  266.75 | median:  128.40 | mean:   97.33 | std:  113.14 | min: -180.53 | steps so far: 1450000 | episodes so far:   4050\n",
            "Eval | on_policy | max:  278.78 | median:  137.63 | mean:  100.41 | std:  130.02 | min: -197.55 | steps so far: 1500000 | episodes so far:   4150\n",
            "Eval | on_policy | max:  270.21 | median:  133.60 | mean:   92.45 | std:  134.23 | min: -258.88 | steps so far: 1550000 | episodes so far:   4250\n",
            "Eval | on_policy | max:  261.81 | median:  122.80 | mean:   79.95 | std:  141.50 | min: -253.87 | steps so far: 1600000 | episodes so far:   4350\n",
            "Eval | on_policy | max:  283.83 | median:   95.14 | mean:   67.81 | std:  149.61 | min: -287.23 | steps so far: 1650000 | episodes so far:   4430\n",
            "Eval | on_policy | max:  272.40 | median:   73.51 | mean:   49.61 | std:  139.35 | min: -241.18 | steps so far: 1700000 | episodes so far:   4530\n",
            "Eval | on_policy | max:  265.22 | median:   28.96 | mean:   51.05 | std:  141.06 | min: -294.68 | steps so far: 1750000 | episodes so far:   4630\n",
            "Eval | on_policy | max:  276.01 | median:  143.95 | mean:   99.09 | std:  124.49 | min: -212.66 | steps so far: 1800000 | episodes so far:   4730\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # environment\n",
        "    env_name = 'LunarLander-v2'\n",
        "    n_state = 8\n",
        "    n_action = 4\n",
        "    train_dir = '.'\n",
        "    env_max_return = 300\n",
        "    env_step_limit = 1000\n",
        "    clip_limits = None\n",
        "    warmup_desired_return = 0\n",
        "    warmup_step_limit = 100\n",
        "    # agent\n",
        "    net_option = 'bilinear'\n",
        "    net_arch = (64, 128, 128)\n",
        "    action_type = 'discrete'\n",
        "    activation = 'relu'\n",
        "    return_scale = 0.015\n",
        "    horizon_scale = 0.03\n",
        "    # training & testing\n",
        "    replay = 'highest'\n",
        "    replay_size = 600\n",
        "    n_warm_up_episodes = 50\n",
        "    n_episodes_per_iter = 20\n",
        "    last_few = 100\n",
        "    learning_rate = 0.0008709635899560805\n",
        "    batch_size = 768\n",
        "    n_updates_per_iter = 150\n",
        "    max_training_steps = 10_000_000\n",
        "    eval_freq = 50_000\n",
        "    eval_goal = 'current'\n",
        "    train_alg = 'udrl'\n",
        "    train_seeds = (1_000_000, 10_000_000)\n",
        "    eval_seeds = (1, 500_000)\n",
        "    n_eval_episodes = 100\n",
        "    actions = 'on_policy'\n",
        "    save_model = True\n",
        "    verbose = False\n",
        "    use_gpu = True #False\n",
        "    seed = 9\n",
        "\n",
        "ud = UpsideDownAgent(Config)\n",
        "ud.warm_up()\n",
        "print(f\"Warm-up complete. Starting training.\")\n",
        "eval_means, eval_medians = [], []\n",
        "while ud.total_steps < Config.max_training_steps:\n",
        "    results = ud.train_step()\n",
        "    for r in results:\n",
        "        if r[0] == 'eval.on_policy.mean':\n",
        "            eval_means.append(r[1])\n",
        "        if r[0] == 'eval.on_policy.median':\n",
        "            eval_medians.append(r[1])\n",
        "    ud.msg(f'Iteration {ud.iters} complete\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "UPyzwfxWpGjs",
        "outputId": "d5d27632-a2fc-4273-cccc-5efebb3cd785"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ecd9932d2146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for _ in tqdm(range(10)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eval.on_policy.mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b9df0dcca049>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnprnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-5890e01ff404>\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(replay, batch_size, nprnd)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprnd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnprnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrailing_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprnd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: high <= 0"
          ]
        }
      ],
      "source": [
        "# # import tqdm\n",
        "# from tqdm.notebook import tqdm\n",
        "\n",
        "# for _ in tqdm(range(10)):\n",
        "for x in range(10):\n",
        "    results = ud.train_step()\n",
        "    for r in results:\n",
        "        if r[0] == 'eval.on_policy.mean':\n",
        "            eval_means.append(r[1])\n",
        "        if r[0] == 'eval.on_policy.median':\n",
        "            eval_medians.append(r[1])\n",
        "    ud.msg(f'Iteration {ud.iters} complete\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzDOZCmtoOaH"
      },
      "outputs": [],
      "source": [
        "name = \"model.pth\"\n",
        "# torch.save(ud.state_dict(), name)\n",
        "torch.save(ud.behavior_fn.state_dict(), name)\n",
        "# torch.save(ud.behavior_fn.state_dict, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTzdCcsiw3SH",
        "outputId": "42e52995-5f87-408a-8b56-b26ad71fc678"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ud = UpsideDownAgent(Config)\n",
        "# ud.load_state_dict(torch.load(name))\n",
        "ud.behavior_fn.load_state_dict(torch.load(name))\n",
        "# ud.eval()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sCCOEthAqwrP"
      ],
      "name": "from_UDRL-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
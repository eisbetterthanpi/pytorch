{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_curious",
      "provenance": [],
      "collapsed_sections": [
        "REtcq-hwDPHa",
        "rOMrdwSYOWSC",
        "YB0Cxrw1StrP",
        "7vmEJYJSEDZR"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/reinforce_curious.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "REtcq-hwDPHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/blog/deep-rl-pg\n",
        "# https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb\n",
        "# Reinforce (aka Monte Carlo Policy Gradient) Policy-Gradient Method\n",
        "!apt install python-opengl ffmpeg xvfb\n",
        "# !pip3 install pyvirtualdisplay\n",
        "# Virtual display\n",
        "# from pyvirtualdisplay import Display\n",
        "# virtual_display = Display(visible=0, size=(500, 500))\n",
        "# virtual_display.start()\n",
        "\n",
        "!pip install gym\n",
        "%pip install gym[atari,accept-rom-license]\n",
        "# !pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "# !pip install git+https://github.com/qlan3/gym-games.git # Extra gym environments made with PyGame\n",
        "\n",
        "# !pip install pyyaml==6.0 # avoid key error metadata\n",
        "# !pip install pyglet # Virtual Screen\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "# import gym_pygame\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install colabgymrender\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "\n"
      ],
      "metadata": {
        "id": "kgxMH5wMXME8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gym wrappers"
      ],
      "metadata": {
        "id": "Q6rIxoaDeT4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/2_gym_wrappers_saving_loading.ipynb\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: reward = self.total_rewards\n",
        "        else: reward = 0\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioSparse, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            # return observation, score, True, info # lost one life, end env\n",
        "            done = True\n",
        "        # else:\n",
        "            # self.total_score = 0\n",
        "        return observation, score, done, info\n",
        "    def reset(self):\n",
        "        self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioEarlyStop, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            # return observation, reward, True, info # early stop\n",
        "            done = True\n",
        "        # else:\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "L4wnbtD_eRxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### policy"
      ],
      "metadata": {
        "id": "rOMrdwSYOWSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py\n",
        "# https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(s_size, h_size), nn.ReLU(),\n",
        "            nn.Linear(h_size, a_size),\n",
        "            # nn.Linear(h_size, h_size*2), nn.ReLU(),\n",
        "            # nn.Linear(h_size*2, a_size),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        # state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        # state = torch.tensor(state.copy(), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        probs = self.model(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample() # can't use action = np.argmax(m) use  m.sample(), sample an action with prob dist P(.|s)\n",
        "        return action.item(), m.log_prob(action)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ho_UHf49N9i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ICM"
      ],
      "metadata": {
        "id": "jUQYXIKr66lQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mcTd9Al4n58"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Conv_Encoder(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv_Encoder, self).__init__()\n",
        "        self.conv_encoder = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "            nn.Conv2d(in_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((64,64)),\n",
        "            nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "            nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "            # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        # print(\"conv forward shape\",x.shape)\n",
        "        # x=x.squeeze()\n",
        "        # x = torch.transpose(x, 1,2)\n",
        "        # x = torch.transpose(x, 0,1)\n",
        "        x = torch.transpose(x, -2,-1)\n",
        "        x = torch.transpose(x, -3,-2)\n",
        "        # print(\"conv forward\",x.shape)\n",
        "        x = self.conv_encoder(x)\n",
        "        x=x.flatten(start_dim=1)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [batch, 256]\n",
        "\n",
        "class InverseModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims):\n",
        "        super(InverseModel, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_dims*2, n_actions)\n",
        "        \n",
        "    def forward(self, features):\n",
        "        features = features.view(1, -1) # (1, hidden_dims)\n",
        "        action = self.fc(features) # (1, n_actions)\n",
        "        return action\n",
        "\n",
        "class ForwardModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims):\n",
        "        super(ForwardModel, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_dims+n_actions, hidden_dims)\n",
        "        self.eye = torch.eye(n_actions, device=device)\n",
        "        \n",
        "    def forward(self, action, features):\n",
        "        # print(\"ForwardModel\",action.shape, features.shape)\n",
        "        # print(\"ForwardModel2\",self.eye[action], features.shape)\n",
        "        x = torch.cat([self.eye[action], features], dim=-1) # (1, n_actions+hidden_dims)\n",
        "        features = self.fc(x) # (1, hidden_dims)\n",
        "        return features\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, space_dims, hidden_dims):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        space_dims=256\n",
        "        self.fc = nn.Linear(space_dims, hidden_dims)\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv_encoder(x)\n",
        "        y = torch.tanh(self.fc(x))\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ICM\n",
        "feature_extractor = FeatureExtractor(env.observation_space.shape[0], 32).to(device)\n",
        "forward_model = ForwardModel(env.action_space.n, 32).to(device)\n",
        "inverse_model = InverseModel(env.action_space.n, 32).to(device)\n",
        "icm_optim = torch.optim.Adam(inverse_model.parameters(), lr=0.001)\n",
        "\n",
        "beta = 0.2\n",
        "# lamda = 0.1\n",
        "eta = 100.0 # scale factor for intrinsic reward\n",
        "# gamma = 0.99\n",
        "# def icm(st, st1, inverse_model, forward_model):\n",
        "def icm(st, st1, action, inverse_model, forward_model):\n",
        "    # ICM\n",
        "    obs_cat = torch.cat([st, st1], dim=0)\n",
        "    features = feature_extractor(obs_cat) # (2, hidden_dims) [2, 32]\n",
        "    inverse_action_prob = inverse_model(features) # (n_actions)\n",
        "    # action=action.reshape((1))\n",
        "    # print(\"icm act\",action.shape)\n",
        "    # action =torch.tensor([action]).view(1,1)\n",
        "    action =torch.tensor(action).view(1).to(device)\n",
        "    # action =torch.tensor(action)\n",
        "    # print(\"icm act\",action.shape)\n",
        "    # action =action.view(1,1)\n",
        "    # est_next_features = forward_model(action.squeeze(0), features[0:1]) #[1] [1, 32]\n",
        "    est_next_features = forward_model(action, features[0:1]) #[1] [1, 32]\n",
        "    # Loss - ICM\n",
        "    # print(\"1\",est_next_features.squeeze(0).shape, features[1].shape)\n",
        "    forward_loss = nn.MSELoss()(est_next_features.squeeze(0), features[1])\n",
        "    # print(\"2\",inverse_action_prob.shape, action.view(-1).shape)\n",
        "    # print(\"2\",inverse_action_prob.device, action.view(-1).device)\n",
        "    inverse_loss = nn.CrossEntropyLoss()(inverse_action_prob.squeeze(1), action.view(-1))\n",
        "    icm_loss = (1-beta)*inverse_loss + beta*forward_loss\n",
        "    intrinsic_reward = eta*forward_loss.detach()\n",
        "    return icm_loss, intrinsic_reward\n",
        "\n",
        "\n",
        "\n",
        "class phist_for_policy(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(phist_for_policy, self).__init__()\n",
        "        self.conv_encoder = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "            nn.Conv2d(in_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((64,64)),\n",
        "            nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "            nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "            # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        # print(\"conv forward shape\",x.shape)\n",
        "        # x=x.squeeze()\n",
        "        # x = torch.transpose(x, 1,2)\n",
        "        # x = torch.transpose(x, 0,1)\n",
        "        x = torch.transpose(x, -2,-1)\n",
        "        x = torch.transpose(x, -3,-2)\n",
        "        # print(\"conv forward\",x.shape)\n",
        "        x = self.conv_encoder(x)\n",
        "        x=x.flatten(start_dim=1)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [batch, 256]\n",
        "\n",
        "# # print(state_space, action_space, h_size) 240 12 64\n",
        "# # policy = Policy(state_space, action_space, h_size).to(device)\n",
        "# policy = Policy(256, action_space, h_size).to(device)\n",
        "\n",
        "# convencode = phist_for_policy(3).to(device)\n",
        "# state = env.reset()\n",
        "# state = torch.tensor(state.copy(), dtype=torch.float).unsqueeze(0).to(device)\n",
        "# # print(state.shape) #(240, 256, 3)\n",
        "# state = convencode(state)\n",
        "# print(state.shape)\n",
        "# action, log_prob = policy(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### reinforce"
      ],
      "metadata": {
        "id": "YB0Cxrw1StrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# update policy after every episode\n",
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state.copy(), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        st=state\n",
        "        # Line 4 of pseudocode run 1 full episode using current policy\n",
        "        for t in range(max_t):\n",
        "            # state = conv_encoder(state)\n",
        "            # print(state.shape) #(240, 256, 3)\n",
        "            state = convencode(state)\n",
        "            # print(state.shape)\n",
        "\n",
        "            action, log_prob = policy(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            state = torch.tensor(state.copy(), dtype=torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "            st1=state\n",
        "            # icm_loss, intrinsic_reward = icm(st, st1, inverse_model, forward_model)\n",
        "            icm_loss, intrinsic_reward = icm(st, st1, action, inverse_model, forward_model)\n",
        "            st=st1\n",
        "            reward = intrinsic_reward\n",
        "\n",
        "            # print(type(reward))\n",
        "            # print(reward.cpu)\n",
        "            rewards.append(reward.cpu())\n",
        "            if done:\n",
        "                break\n",
        "        # print(type(sum(rewards)))\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        # Line 6 of pseudocode: calculate the return\n",
        "        discounts = [gamma**i for i in range(len(rewards)+1)] # [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
        "        ## We calculate the return by sum(gamma[t] * reward[t]) \n",
        "        # R = sum([a*b for a,b in zip(discounts, rewards)])\n",
        "        R = sum([a*b for a,b in zip(discounts, rewards)]) - np.mean(scores_deque) # baseline subtraction from discord\n",
        "        # R = sum([a*b for a,b in zip(discounts, rewards)]) - torch.mean(scores_deque) # baseline subtraction from discord\n",
        "        # R = sum([gamma**a*b for a,b in enumerate(rewards)])\n",
        "        \n",
        "        # Line 7:\n",
        "        policy_loss = []\n",
        "        for log_prob in saved_log_probs:\n",
        "            policy_loss.append(-log_prob * R)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        # max πθ(a3|s;θ) = min 1−πθ(a3|s;θ)\n",
        "\n",
        "        icm_optim.zero_grad()\n",
        "        icm_loss.backward()\n",
        "        icm_optim.step()\n",
        "\n",
        "       \n",
        "        optimizer.zero_grad() # Line 8: loss.backward()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        if i_episode % 100 == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "NCNvyElRStWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "bkPvgr9vU6Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env_id = \"CartPole-v1\"\n",
        "# env_id = \"Pixelcopter-PLE-v0\"\n",
        "# env_id = \"Pong-PLE-v0\"\n",
        "\n",
        "# env = gym.make(env_id)\n",
        "\n",
        "\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "# env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n",
        "\n",
        "\n",
        "\n",
        "# eval_env = gym.make(env_id)\n",
        "eval_env = env\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "h_size=64 # cp 16 pc 64 p 64\n",
        "n_training_episodes=10000 # cp 1000 pc 50000 p 20000\n",
        "n_evaluation_episodes=10\n",
        "max_t=10000 # cp 1000 pc 10000 p 5000 max episode length\n",
        "gamma=0.99 # cp 1.0 pc/p 0.99\n",
        "lr=1e-4  # cp 1e-2 pc 1e-4 p 1e-2\n",
        "# env_id=env_id\n",
        "state_space=s_size\n",
        "action_space=a_size\n",
        "\n",
        "# policy = Policy(state_space, action_space, h_size).to(device)\n",
        "policy = Policy(256, action_space, h_size).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "POOOk15_K6KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "RIWhQyJjfpEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scores = reinforce(policy, optimizer, n_training_episodes, max_t, gamma)\n"
      ],
      "metadata": {
        "id": "utRe1NgtVBYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### eval"
      ],
      "metadata": {
        "id": "Qajj2kXqhB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    episode_rewards = []\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        done = False\n",
        "        total_rewards_ep = 0\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy(state)\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "            if done:\n",
        "                break\n",
        "            state = new_state\n",
        "            episode_rewards.append(total_rewards_ep)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "    return mean_reward, std_reward\n",
        "\n",
        "evaluate_agent(eval_env, max_t, n_evaluation_episodes, policy)\n"
      ],
      "metadata": {
        "id": "3FamHmxyhBEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### save"
      ],
      "metadata": {
        "id": "JovTCH5efDem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "name='Curious_reinforce_cp.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "model=policy\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# actor=model\n"
      ],
      "metadata": {
        "id": "4B2-yRuxfC90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### video"
      ],
      "metadata": {
        "id": "7vmEJYJSEDZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "# from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# \"MontezumaRevengeDeterministic-v4\"\n",
        "# env = gym.make(env_id)\n",
        "\n",
        "# env = SparseEnv(env)\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "# env = MarioSparse(env)\n",
        "# env = MarioEarlyStop(env)\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "# model.eval()\n",
        "x=0\n",
        "\n",
        "while True:\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    x+=1\n",
        "    action, _ = policy(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done: break\n",
        "env.play()\n",
        "print(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "P7PVJARxDlQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
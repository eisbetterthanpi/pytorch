{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/from_UDRL_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv4NQbMZSgeT"
      },
      "source": [
        "#### Setup some basic dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S2vicGMBmduX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "791fb632-5d46-44af-c400-67e5fdcc3c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 855 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=b362f7caa6367ebe0b09a831acf9c78814b340d7640a7a74c924e3ce4b37e88b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m No API key specified.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.18"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220615_003634-16kdnq5x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/bobdole/from_UDRL/runs/16kdnq5x\" target=\"_blank\">wise-darkness-2</a></strong> to <a href=\"https://wandb.ai/bobdole/from_UDRL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/bobdole/from_UDRL/runs/16kdnq5x?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4e48d6a090>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# !pip uninstall -y torch gym\n",
        "# !pip install torch==1.4.0+cpu gym[box2d]==0.15.4 tqdm sortedcontainers -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!pip install gym[box2d] tqdm sortedcontainers\n",
        "# https://arxiv.org/pdf/1912.02877.pdf\n",
        "# https://colab.research.google.com/drive/1ynS9g7YzFpNSwhva2_RDKYLjyGckCA8H\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # \n",
        "wandb.init(project=\"from_UDRL\", entity=\"bobdole\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCCOEthAqwrP"
      },
      "source": [
        "#### Replay Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "id": "Y2K3Ul0Op_jJ"
      },
      "outputs": [],
      "source": [
        "# Replay Utilities\n",
        "import copy, os, pickle\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from sortedcontainers import SortedListWithKey\n",
        "\n",
        "class Episode:\n",
        "    \"\"\"For any episode, this container has len(actions) == len(rewards) == len(states) - 1\n",
        "    This is because we initialize using the starting state.\n",
        "    The add() method adds the action just taken, the obtained reward, and the **next** state.\n",
        "    This makes accessing the episode data simple:\n",
        "    states[0] is the first state\n",
        "    actions[0] is the action taken in that state\n",
        "    rewards[0] reward obtained by taking the action, and so on\n",
        "    The last state added is never actually processed by the agent.\"\"\"\n",
        "    def __init__(self, init_state, desired_return, desired_horizon):\n",
        "        self.states = [init_state]\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.desired_return = desired_return\n",
        "        self.desired_horizon = desired_horizon\n",
        "\n",
        "    def add(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    @property\n",
        "    def total_reward(self):\n",
        "        return sum(self.rewards)\n",
        "\n",
        "    @property\n",
        "    def steps(self):\n",
        "        return len(self.actions)\n",
        "\n",
        "    @property\n",
        "    def return_gap(self):\n",
        "        return self.desired_return - self.total_reward\n",
        "\n",
        "    @property\n",
        "    def horizon_gap(self):\n",
        "        return self.desired_horizon - self.steps\n",
        "\n",
        "\n",
        "def get_reward(episode: Episode):\n",
        "    return episode.total_reward\n",
        "\n",
        "def make_replay(config):\n",
        "    if config.replay == 'highest':\n",
        "        replay = HighestReplay(max_size=config.replay_size)\n",
        "    elif config.replay == 'recent':\n",
        "        replay = RecentReplay(max_size=config.replay_size)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return replay\n",
        "\n",
        "\n",
        "class Replay(ABC):\n",
        "    def __init__(self):\n",
        "        self.episodes = []\n",
        "        self.known_returns = []\n",
        "        self.known_horizons = []\n",
        "\n",
        "    @abstractmethod\n",
        "    def add(self, episode):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def best_episode(self):\n",
        "        return max(self.episodes, key=get_reward)\n",
        "\n",
        "    def get_closest_horizon(self, desired_return):\n",
        "        idx = np.abs(np.asarray(self.known_returns) - desired_return).argmin()\n",
        "        return self.known_horizons[idx]\n",
        "\n",
        "    @property\n",
        "    def returns(self):\n",
        "        return [episode.total_reward for episode in self.episodes]\n",
        "\n",
        "\n",
        "class HighestReplay(Replay):\n",
        "    def __init__(self, max_size: int):\n",
        "        super().__init__()\n",
        "        self.episodes = SortedListWithKey(key=get_reward)\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def add(self, episode: Episode):\n",
        "        self.episodes.add(episode)\n",
        "        self.known_returns.append(episode.total_reward)\n",
        "        self.known_horizons.append(episode.steps)\n",
        "        if len(self.episodes) > self.max_size:\n",
        "            self.episodes.pop(0)\n",
        "\n",
        "\n",
        "def trailing_segments(episode: Episode, nprnd: np.random.RandomState):\n",
        "    steps = episode.steps\n",
        "    i = nprnd.randint(0, steps)\n",
        "    j = steps\n",
        "    return episode.states[i], sum(episode.rewards[i:j]), (j - i), episode.actions[i]\n",
        "\n",
        "def sample_batch(replay: Replay, batch_size: int, nprnd: np.random.RandomState):\n",
        "    idxs = nprnd.randint(0, len(replay.episodes), batch_size)\n",
        "    episodes = [replay.episodes[idx] for idx in idxs]\n",
        "    segments = [trailing_segments(episode, nprnd) for episode in episodes]\n",
        "    states, desired_rewards, horizons, actions = [], [], [], []\n",
        "    for state, desired_reward, horizon, action in segments:\n",
        "        states.append(state)\n",
        "        desired_rewards.append(desired_reward)\n",
        "        horizons.append(horizon)\n",
        "        actions.append(action)\n",
        "    states = np.array(states, dtype=np.float32)\n",
        "    desired_rewards = np.array(desired_rewards, dtype=np.float32)[:, None]\n",
        "    horizons = np.array(horizons, dtype=np.float32)[:, None]\n",
        "    actions = np.array(actions, dtype=np.float32)\n",
        "    return states, desired_rewards, horizons, actions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc1B-S01q-RI"
      },
      "source": [
        "### Behavior Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zSQ2Nd4Ppprs"
      },
      "outputs": [],
      "source": [
        "# Behavior Function\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import orthogonal_\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Categorical:\n",
        "    def __init__(self, dim: int):\n",
        "        self.dim = dim\n",
        "        self.loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    @staticmethod\n",
        "    def distribution(probs):\n",
        "        return torch.distributions.Categorical(probs)\n",
        "\n",
        "    def sample(self, scores):\n",
        "        probs = F.softmax(scores, dim=1)\n",
        "        dist = self.distribution(probs)\n",
        "        sample = dist.sample().item()                     # item() forces single env\n",
        "        return sample\n",
        "\n",
        "    def mode(self, scores):\n",
        "        probs = F.softmax(scores, dim=1)\n",
        "        mode = probs.to('cpu').data.numpy()[0].argmax()   # [0] forces single env\n",
        "        return mode\n",
        "\n",
        "    def random_sample(self):\n",
        "        return torch.randint(0, self.dim, (1,)).item()    # (1,) & item() force single env\n",
        "\n",
        "    def clip(self, action):\n",
        "        return action\n",
        "\n",
        "\n",
        "class ScaledIntent:\n",
        "    def __init__(self, return_scale: float, horizon_scale: float, max_return: float):\n",
        "        self.return_scale = return_scale\n",
        "        self.horizon_scale = horizon_scale\n",
        "        self.max_return = max_return\n",
        "\n",
        "    def __call__(self, intent):\n",
        "        _intent = np.zeros_like(intent)\n",
        "        returns = np.minimum(intent[:, 0], self.max_return)\n",
        "        horizons = np.maximum(intent[:, 1], 1)\n",
        "        _intent[:, 0] = returns * self.return_scale\n",
        "        _intent[:, 1] = horizons * self.horizon_scale\n",
        "        intent = _intent.astype(np.float32)\n",
        "        return intent\n",
        "\n",
        "\n",
        "def make_behavior_fn(config, nprnd: np.random.RandomState, device: torch.device):\n",
        "    intent_transform = ScaledIntent(config.return_scale, config.horizon_scale, config.env_max_return)\n",
        "    behavior_fn = BehaviorFn(ProductNetwork(config),\n",
        "                             intent_transform=intent_transform,\n",
        "                             config=config,\n",
        "                             nprnd=nprnd,\n",
        "                             device=device)\n",
        "    return behavior_fn\n",
        "\n",
        "\n",
        "class BehaviorFn(nn.Module):\n",
        "    def __init__(self, net: nn.Module, intent_transform: ScaledIntent, config, nprnd: np.random.RandomState, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.intent_transform = intent_transform\n",
        "        self.nprnd = nprnd\n",
        "        self.device = device\n",
        "        self.state_dtype = np.float32\n",
        "        if config.action_type == 'discrete':\n",
        "            self.action_dist = Categorical(config.n_action)\n",
        "            self.action_dtype = np.int64\n",
        "        else:\n",
        "            raise NotImplementedError(config.action_type)\n",
        "\n",
        "    def forward(self, state, desired_reward, horizon, device=None):\n",
        "        if device is None: device = self.device\n",
        "        state = np.asarray(state)\n",
        "        intent = np.concatenate([desired_reward, horizon], axis=1)\n",
        "        transformed_intent = self.intent_transform(intent)\n",
        "        state = self.make_variable(state, dtype=self.state_dtype, device=device)\n",
        "        intent = self.make_variable(transformed_intent, dtype=self.state_dtype, device=device)\n",
        "        net_output = self.net(state, intent)\n",
        "        if hasattr(self, 'logstd'):\n",
        "            net_output = torch.cat((net_output, self.logstd.expand_as(net_output)), dim=-1)\n",
        "        return net_output\n",
        "\n",
        "    def loss(self, states, desired_rewards, horizons, actions):\n",
        "        outputs = self(states, desired_rewards, horizons)\n",
        "        targets = self.make_variable(actions, dtype=self.action_dtype)\n",
        "        loss = self.action_dist.loss(outputs, targets)\n",
        "        return loss\n",
        "\n",
        "    def make_variable(self, x, dtype, device=None):\n",
        "        if device is None: device = self.device\n",
        "        return torch.from_numpy(np.asarray(x, dtype=dtype)).to(device)\n",
        "\n",
        "\n",
        "class ProductNetwork(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        activation, n_state, n_output, net_arch = config.activation, config.n_state, config.n_action, config.net_arch\n",
        "        self.n_state = n_state\n",
        "        self.net_option = config.net_option\n",
        "        n_proj = net_arch[0]\n",
        "        if activation == 'relu':\n",
        "            activation = nn.ReLU\n",
        "            gain = np.sqrt(2)\n",
        "        elif activation == 'tanh':\n",
        "            activation = nn.Tanh\n",
        "            gain = 1.0\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.layer1 = FastWeightLayer(n_proj, n_state, 2, activation, option=self.net_option)\n",
        "        hids = []\n",
        "        n_last = n_proj\n",
        "        for n_current in net_arch[1:]:\n",
        "            hids += [nn.Linear(n_last, n_current), activation()]\n",
        "            n_last = n_current\n",
        "        self.hids = nn.Sequential(*hids)\n",
        "        self.op = nn.Linear(n_last, n_output)\n",
        "        self.init_params(gain)\n",
        "\n",
        "    def forward(self, state, intent):\n",
        "        out = self.layer1(state, intent)\n",
        "        out = self.hids(out) if len(self.hids) > 0 else out\n",
        "        out = self.op(out)\n",
        "        return out\n",
        "\n",
        "    def init_params(self, gain):\n",
        "        def init(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                orthogonal_(m.weight.data, gain=gain)\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "        def init_hyper(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                orthogonal_(m.weight.data, gain=1.0)\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "        self.layer1.apply(init_hyper)\n",
        "        self.hids.apply(init)\n",
        "\n",
        "\n",
        "class FastWeightLayer(nn.Module):\n",
        "    def __init__(self, size, x_size, c_size, activation, option):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.x_size = x_size\n",
        "        self.c_size = c_size\n",
        "        self.option = option\n",
        "        self.activation = activation\n",
        "        if option == 'bilinear':\n",
        "            self.Wlinear = nn.Linear(c_size, self.size * self.x_size)\n",
        "            self.blinear = nn.Linear(c_size, self.size)\n",
        "        elif option == 'gated':\n",
        "            self.xlinear = nn.Linear(x_size, size)\n",
        "            self.clinear = nn.Linear(c_size, size)\n",
        "        else:\n",
        "            raise NotImplementedError(option)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        if self.option == 'bilinear':\n",
        "            batch_size = x.shape[0]\n",
        "            W, b = self.Wlinear(c), self.blinear(c)\n",
        "            W = torch.reshape(W, (batch_size, self.x_size, self.size))\n",
        "            x = torch.reshape(x, (batch_size, 1, self.x_size))  # add a dimension for matmul, then remove it\n",
        "            output = self.activation()(torch.matmul(x, W).reshape((batch_size, self.size)) + b)\n",
        "        elif self.option == 'gated':\n",
        "            x_proj = self.activation()(self.xlinear(x))\n",
        "            c_proj = torch.sigmoid(self.clinear(c))\n",
        "            output = x_proj * c_proj\n",
        "        else:\n",
        "            raise NotImplementedError(self.option)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MynJcnRkrG2l"
      },
      "source": [
        "### Agent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uDbs9BoGnbrX"
      },
      "outputs": [],
      "source": [
        "# Agent Implementation\n",
        "from collections import deque\n",
        "from typing import List, Tuple, Union\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from gym.core import Wrapper\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "class SeedEnv(Wrapper):\n",
        "    \"\"\" Every reset() set a new seed from a given seed range \"\"\"\n",
        "    def __init__(self, env, seed_range):\n",
        "        super().__init__(env)\n",
        "        self.seed_range = seed_range\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.seed(np.random.randint(*self.seed_range))\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "\n",
        "def get_stats(scalar_list: list) -> dict:\n",
        "    if len(scalar_list) == 0:\n",
        "        stats = {key: np.nan for key in ('max', 'mean', 'median', 'min', 'std')}\n",
        "        stats['size'] = 0\n",
        "    else:\n",
        "        stats = {'max': np.max(scalar_list), 'mean': np.mean(scalar_list), 'median': np.median(scalar_list),\n",
        "                 'min': np.min(scalar_list), 'std': np.std(scalar_list, ddof=1), 'size': len(scalar_list)}\n",
        "    return stats\n",
        "\n",
        "\n",
        "class UpsideDownAgent:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.msg = print if config.verbose else lambda *a, **k: None\n",
        "        self.device = torch.device('cuda:0' if config.use_gpu and torch.cuda.is_available() else 'cpu')\n",
        "        self.msg('Using device', self.device)\n",
        "        seed = config.seed\n",
        "        self.nprnd = np.random.RandomState(seed)\n",
        "        np.random.seed(Config.seed)\n",
        "        torch.manual_seed(seed)\n",
        "        self.train_env = SeedEnv(gym.make(config.env_name), seed_range=config.train_seeds)\n",
        "        self.test_env  = SeedEnv(gym.make(config.env_name), seed_range=config.eval_seeds)\n",
        "        self.replay: Replay = make_replay(config)\n",
        "        self.behavior_fn: BehaviorFn = make_behavior_fn(config, self.nprnd, self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.behavior_fn.parameters(), lr=config.learning_rate)\n",
        "        self.iters = 0\n",
        "        self.total_episodes = 0\n",
        "        self.total_steps = 0\n",
        "        self.best_onpolicy_mean = np.array(-np.inf)\n",
        "        self.best_greedy_mean = np.array(-np.inf)\n",
        "        self.best_rolling_mean = np.array(-np.inf)\n",
        "        self.rolling_returns = deque(maxlen=config.n_eval_episodes)\n",
        "        self.current_step_limit = config.warmup_step_limit  # Used only for warm up inputs\n",
        "        self.current_desired_return = (config.warmup_desired_return, 0)\n",
        "\n",
        "    def warm_up(self) -> List[Tuple]:\n",
        "        results: List[Tuple] = []\n",
        "        episodes, _ = self.run_episodes(self.current_step_limit, self.current_desired_return, label='Warmup',\n",
        "                                        actions='random', n_episodes=self.config.n_warm_up_episodes)\n",
        "        self.total_episodes += self.config.n_warm_up_episodes\n",
        "        for episode in episodes:\n",
        "            self.replay.add(episode)\n",
        "            self.rolling_returns.append(episode.total_reward)\n",
        "        stats = get_stats(self.replay.returns)\n",
        "        self.msg(f\"\\nWarmup | Replay max: {stats['max']:7.2f} mean: {stats['mean']:7.2f} \"\n",
        "                 f\"min: {stats['min']:7.2f} size: {stats['size']:3}\")\n",
        "        results += [('replay.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min', 'size']]\n",
        "        return results\n",
        "\n",
        "    def train_step(self) -> List[Tuple]:\n",
        "        results: List[Tuple] = []\n",
        "        n_updates = self.config.n_updates_per_iter\n",
        "        self.msg(f'\\nIteration {(self.iters + 1):3} | Training for {n_updates} updates')\n",
        "        # Learn behavior function\n",
        "        torch.set_grad_enabled(True)\n",
        "        self.behavior_fn.to(self.device)\n",
        "        self.behavior_fn.train()\n",
        "        loss = None\n",
        "        tq = trange(n_updates, disable=self.config.verbose is not True)\n",
        "        losses = []\n",
        "        for u in tq:\n",
        "            self.optimizer.zero_grad()\n",
        "            s, r, h, a = sample_batch(self.replay, self.config.batch_size, self.nprnd)\n",
        "            loss = self.behavior_fn.loss(s, r, h, a)\n",
        "            losses.append(loss.item())\n",
        "            tq.set_postfix(loss=loss.item())\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        results += [('loss', loss.item(), self.total_steps)]\n",
        "        # Generate more data\n",
        "        last_few_episodes = self.replay.episodes[-self.config.last_few:]\n",
        "        last_few_durations = [episodes.steps for episodes in last_few_episodes]\n",
        "        last_few_returns = [episodes.total_reward for episodes in last_few_episodes]\n",
        "        # self.current_step_limit = np.int(np.mean(last_few_durations))\n",
        "        self.current_step_limit = int(np.mean(last_few_durations))\n",
        "        self.current_desired_return = (np.mean(last_few_returns), np.std(last_few_returns))\n",
        "        episodes, eval_results = self.run_episodes(self.current_step_limit, self.current_desired_return,\n",
        "                                                   label='Train', actions=self.config.actions,\n",
        "                                                   n_episodes=self.config.n_episodes_per_iter)\n",
        "        self.total_episodes += self.config.n_episodes_per_iter\n",
        "        returns = []\n",
        "        for episode in episodes:\n",
        "            self.replay.add(episode)\n",
        "            episode_return = episode.total_reward\n",
        "            returns.append(episode_return)\n",
        "            self.rolling_returns.append(episode_return)\n",
        "        del episodes\n",
        "        # Logging\n",
        "        self.iters += 1\n",
        "        results += eval_results\n",
        "        rolling_mean = np.mean(self.rolling_returns)\n",
        "        results += [('rollouts.rolling_mean', rolling_mean, self.total_steps)]\n",
        "        self.best_rolling_mean = rolling_mean if rolling_mean > self.best_rolling_mean else self.best_rolling_mean\n",
        "        stats = get_stats(self.replay.returns)\n",
        "        self.msg(f\"Iteration {self.iters:3} | \"\n",
        "                 f\"Rollouts max: {np.max(returns):7.2f} mean: {np.mean(returns):7.2f} min: {np.min(returns):7.2f} | \"\n",
        "                 f\"Replay max: {stats['max']:7.2f} mean: {stats['mean']:7.2f} \"\n",
        "                 f\"min: {stats['min']: 7.2f} size: {stats['size']:3} | \"\n",
        "                 f\"steps so far: {self.total_steps:7} episodes so far: {self.total_episodes:6} | \"\n",
        "                 f\"Rolling Mean ({len(self.rolling_returns)}): {rolling_mean:7.2f}\")\n",
        "        results += [('iteration', self.iters, self.total_steps)]\n",
        "        results += [('current_step_limit', self.current_step_limit, self.total_steps)]\n",
        "        results += [('current_desired_return.mean', np.mean(last_few_returns), self.total_steps)]\n",
        "        results += [('current_desired_return.std', np.std(last_few_returns), self.total_steps)]\n",
        "        results += [('replay.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min', 'size']]\n",
        "        stats = get_stats(returns)\n",
        "        results += [('rollouts.' + k, stats[k], self.total_steps) for k in ['max', 'mean', 'min']]\n",
        "        results += [('total_steps', self.total_steps, self.total_steps)]\n",
        "        return results\n",
        "\n",
        "    def _eval(self) -> List[Tuple]:\n",
        "        results: List[Tuple] = [('episodes', self.total_episodes, self.total_steps)]\n",
        "        if self.config.eval_goal == 'max':\n",
        "            desired_test_return = self.config.env_max_return\n",
        "        elif self.config.eval_goal == 'current':\n",
        "            desired_test_return = self.current_desired_return[0]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        actions = 'on_policy'  # can also evaluation with \"greedy\" actions here\n",
        "        self.msg(f'\\nTesting on {self.config.n_eval_episodes} episodes with {actions} actions')\n",
        "        episodes, _ = self.run_episodes(self.current_step_limit, desired_test_return, label='Test',\n",
        "                                        actions=actions, n_episodes=self.config.n_eval_episodes)\n",
        "        stats = get_stats([episode.total_reward for episode in episodes])\n",
        "        results += [(f'eval.{actions}.{k}', stats[k], self.total_steps)\n",
        "                    for k in ['max', 'median', 'mean', 'std', 'min']]\n",
        "        print(f\"Eval | {actions} | max: {stats['max']:7.2f} | median: {stats['median']:7.2f} | \"\n",
        "              f\"mean: {stats['mean']:7.2f} | std: {stats['std']: 7.2f} | min: {stats['min']:7.2f} | \"\n",
        "              f\"steps so far: {self.total_steps:7} | episodes so far: {self.total_episodes:6}\")\n",
        "        if actions == 'on_policy':\n",
        "            self.best_onpolicy_mean = max(stats['mean'], self.best_onpolicy_mean)\n",
        "        else:\n",
        "            self.best_greedy_mean = max(stats['mean'], self.best_greedy_mean)\n",
        "        del episodes, stats\n",
        "        return results\n",
        "\n",
        "\n",
        "    def run_episodes(self, step_limit: int, desired_return: Union[float, Tuple], label: str,\n",
        "                     actions: str, n_episodes: int = 1, render: bool = False) -> Tuple[List[Episode], List[Tuple]]:\n",
        "        assert label in ['Warmup', 'Train', 'Test']\n",
        "        assert actions in ['random', 'on_policy', 'greedy'] or actions.startswith('epsg')\n",
        "        behavior_fn, config, device, nprnd = self.behavior_fn, self.config, self.device, self.nprnd\n",
        "        torch.set_grad_enabled(False)\n",
        "        behavior_fn.eval()\n",
        "        behavior_fn.to(device)\n",
        "        episodes: List[Episode] = []\n",
        "        eval_results: List[Tuple] = []\n",
        "        env = self.test_env if label == 'Test' else self.train_env\n",
        "        for i in range(n_episodes):\n",
        "            episode_reward = 0 #for sparse\n",
        "            if isinstance(desired_return, tuple):\n",
        "                desired_return_final = desired_return[0] + desired_return[1] * nprnd.random_sample()\n",
        "            else:\n",
        "                desired_return_final = desired_return\n",
        "            if config.env_name == 'TakeCover-v0' or config.env_name == 'CartPoleContinuous-v0':\n",
        "                # desired_return_final = np.int(desired_return_final)\n",
        "                desired_return_final = int(desired_return_final)\n",
        "                step_limit = desired_return_final\n",
        "            # Prepare env\n",
        "            state = env.reset()\n",
        "            if render: env.render()\n",
        "            # Generate episode\n",
        "            episode = Episode(state, desired_return_final, step_limit)\n",
        "            done = False\n",
        "            while episode.steps < config.env_step_limit and not done:\n",
        "                state = np.asarray(state)\n",
        "                if actions == 'random':\n",
        "                    action = behavior_fn.action_dist.random_sample()\n",
        "                elif actions == 'on_policy':\n",
        "                    desired_return_remaining = np.array([[desired_return_final - episode.total_reward]])\n",
        "                    steps_remaining = np.array([[step_limit - episode.steps]])\n",
        "                    action_scores = behavior_fn(state[None],\n",
        "                                                desired_return_remaining,\n",
        "                                                steps_remaining,\n",
        "                                                device=device)\n",
        "                    action = behavior_fn.action_dist.sample(action_scores)\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "                clipped_action = behavior_fn.action_dist.clip(action)\n",
        "                state, reward, done, _ = env.step(clipped_action)\n",
        "\n",
        "                # for sparse\n",
        "                episode_reward += reward\n",
        "                if not done:\n",
        "                    reward = 0\n",
        "                else:\n",
        "                    reward = episode_reward\n",
        "\n",
        "                if render: env.render()\n",
        "                if label == 'Test':\n",
        "                    episode.add(0, 0, reward)  # reduce memory usage\n",
        "                else:\n",
        "                    episode.add(state, clipped_action, reward)\n",
        "                    self.total_steps += 1\n",
        "                    if label == 'Train' and self.total_steps % self.config.eval_freq == 0:\n",
        "                        eval_results += self._eval()\n",
        "\n",
        "            self.msg(f'{label} | {actions} | Episode {i:3} | '\n",
        "                     f'Goals: ({desired_return_final:7.2f}, {step_limit:4}) | '\n",
        "                     f'Return: {episode.total_reward:7.2f} Steps: {episode.steps:4} | '\n",
        "                     f'Return gap: {episode.return_gap:7.2f} Horizon gap: {episode.horizon_gap:5} ')\n",
        "            wandb.log({\"reward\": episode.total_reward})\n",
        "            episodes.append(episode)\n",
        "        return episodes, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZ2fdF9rMww"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "If4nafnjnFOf",
        "outputId": "62e00dd0-fd4e-40ef-f975-9424cceccd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warm-up complete. Starting training.\n",
            "Eval | on_policy | max:  -20.91 | median:  -78.43 | mean:  -77.81 | std:   20.03 | min: -121.80 | steps so far:   50000 | episodes so far:    550\n",
            "Eval | on_policy | max:   10.46 | median:  -72.35 | mean:  -76.28 | std:   32.97 | min: -167.03 | steps so far:  100000 | episodes so far:   1130\n",
            "Eval | on_policy | max:   10.50 | median:  -65.49 | mean:  -71.94 | std:   47.36 | min: -248.17 | steps so far:  150000 | episodes so far:   1690\n",
            "Eval | on_policy | max:   18.22 | median:  -54.33 | mean:  -65.70 | std:   53.98 | min: -350.48 | steps so far:  200000 | episodes so far:   2270\n",
            "Eval | on_policy | max:   42.72 | median:  -72.16 | mean:  -73.98 | std:   60.37 | min: -192.87 | steps so far:  250000 | episodes so far:   2830\n",
            "Eval | on_policy | max:   52.68 | median:  -88.07 | mean:  -82.85 | std:   75.54 | min: -205.24 | steps so far:  300000 | episodes so far:   3370\n",
            "Eval | on_policy | max:  242.17 | median:  -85.78 | mean:  -99.51 | std:   94.48 | min: -289.22 | steps so far:  350000 | episodes so far:   3790\n",
            "Eval | on_policy | max:  213.08 | median:  -88.12 | mean: -110.30 | std:  127.72 | min: -483.13 | steps so far:  400000 | episodes so far:   4130\n",
            "Eval | on_policy | max:  241.15 | median: -131.06 | mean: -124.56 | std:  141.04 | min: -429.36 | steps so far:  450000 | episodes so far:   4370\n",
            "Eval | on_policy | max:  268.20 | median:  -35.13 | mean:  -61.95 | std:  178.36 | min: -456.54 | steps so far:  500000 | episodes so far:   4550\n",
            "Eval | on_policy | max:  267.73 | median:  -50.77 | mean:  -58.62 | std:  162.75 | min: -375.83 | steps so far:  550000 | episodes so far:   4770\n",
            "Eval | on_policy | max:  299.82 | median:  -15.68 | mean:  -27.18 | std:  162.28 | min: -553.44 | steps so far:  600000 | episodes so far:   4970\n",
            "Eval | on_policy | max:  273.71 | median:  -72.41 | mean:  -39.21 | std:  123.97 | min: -284.00 | steps so far:  650000 | episodes so far:   5170\n",
            "Eval | on_policy | max:  275.62 | median:  -41.22 | mean:  -25.01 | std:  150.63 | min: -298.96 | steps so far:  700000 | episodes so far:   5370\n",
            "Eval | on_policy | max:  278.81 | median:  -44.47 | mean:  -39.38 | std:  151.18 | min: -450.99 | steps so far:  750000 | episodes so far:   5570\n",
            "Eval | on_policy | max:  255.46 | median:  -53.16 | mean:  -53.45 | std:  152.05 | min: -403.59 | steps so far:  800000 | episodes so far:   5750\n",
            "Eval | on_policy | max:  303.35 | median:  -37.03 | mean:  -46.45 | std:  169.01 | min: -429.21 | steps so far:  850000 | episodes so far:   5930\n",
            "Eval | on_policy | max:  304.64 | median:  -17.90 | mean:  -29.33 | std:  179.64 | min: -543.18 | steps so far:  900000 | episodes so far:   6090\n",
            "Eval | on_policy | max:  278.83 | median:  -38.74 | mean:  -31.13 | std:  175.79 | min: -526.82 | steps so far:  950000 | episodes so far:   6230\n",
            "Eval | on_policy | max:  312.63 | median:  -12.36 | mean:   20.34 | std:  183.09 | min: -398.98 | steps so far: 1000000 | episodes so far:   6390\n",
            "Eval | on_policy | max:  286.12 | median:   -6.10 | mean:   28.40 | std:  168.63 | min: -718.67 | steps so far: 1050000 | episodes so far:   6550\n",
            "Eval | on_policy | max:  275.33 | median:  -37.51 | mean:    2.95 | std:  154.29 | min: -331.21 | steps so far: 1100000 | episodes so far:   6690\n",
            "Eval | on_policy | max:  277.34 | median:  -39.27 | mean:  -20.15 | std:  167.68 | min: -382.15 | steps so far: 1150000 | episodes so far:   6830\n",
            "Eval | on_policy | max:  296.33 | median:  -28.01 | mean:   12.72 | std:  157.45 | min: -262.75 | steps so far: 1200000 | episodes so far:   6970\n",
            "Eval | on_policy | max:  298.89 | median:  -18.63 | mean:   24.73 | std:  157.15 | min: -499.86 | steps so far: 1250000 | episodes so far:   7130\n",
            "Eval | on_policy | max:  295.45 | median:   -2.77 | mean:   39.03 | std:  172.86 | min: -442.71 | steps so far: 1300000 | episodes so far:   7250\n",
            "Eval | on_policy | max:  293.03 | median:    8.90 | mean:   36.10 | std:  155.53 | min: -299.10 | steps so far: 1350000 | episodes so far:   7370\n",
            "Eval | on_policy | max:  282.69 | median:   16.65 | mean:   58.34 | std:  148.18 | min: -295.10 | steps so far: 1400000 | episodes so far:   7510\n",
            "Eval | on_policy | max:  303.74 | median:   10.78 | mean:   35.48 | std:  146.36 | min: -461.69 | steps so far: 1450000 | episodes so far:   7630\n",
            "Eval | on_policy | max:  307.55 | median:   11.24 | mean:   48.37 | std:  159.36 | min: -314.18 | steps so far: 1500000 | episodes so far:   7770\n",
            "Eval | on_policy | max:  291.81 | median:   12.04 | mean:   47.73 | std:  167.89 | min: -353.18 | steps so far: 1550000 | episodes so far:   7890\n",
            "Eval | on_policy | max:  287.70 | median:    6.47 | mean:   48.21 | std:  166.89 | min: -384.78 | steps so far: 1600000 | episodes so far:   8010\n",
            "Eval | on_policy | max:  292.77 | median:   21.08 | mean:   45.63 | std:  165.86 | min: -302.36 | steps so far: 1650000 | episodes so far:   8130\n",
            "Eval | on_policy | max:  290.12 | median:   53.71 | mean:   88.59 | std:  150.51 | min: -266.95 | steps so far: 1700000 | episodes so far:   8230\n",
            "Eval | on_policy | max:  307.75 | median:   27.40 | mean:   43.11 | std:  176.60 | min: -347.40 | steps so far: 1750000 | episodes so far:   8350\n",
            "Eval | on_policy | max:  286.99 | median:   15.51 | mean:   56.00 | std:  152.31 | min: -356.50 | steps so far: 1800000 | episodes so far:   8470\n",
            "Eval | on_policy | max:  315.43 | median:   72.38 | mean:   89.91 | std:  150.37 | min: -231.45 | steps so far: 1850000 | episodes so far:   8590\n",
            "Eval | on_policy | max:  296.20 | median:   17.13 | mean:   59.62 | std:  152.78 | min: -416.66 | steps so far: 1900000 | episodes so far:   8710\n",
            "Eval | on_policy | max:  310.51 | median:   42.62 | mean:   80.17 | std:  163.04 | min: -310.93 | steps so far: 1950000 | episodes so far:   8830\n",
            "Eval | on_policy | max:  304.44 | median:  155.62 | mean:  105.08 | std:  153.33 | min: -373.62 | steps so far: 2000000 | episodes so far:   8950\n",
            "Eval | on_policy | max:  302.87 | median:  186.95 | mean:  112.65 | std:  161.33 | min: -470.79 | steps so far: 2050000 | episodes so far:   9070\n",
            "Eval | on_policy | max:  295.22 | median:   42.09 | mean:   76.40 | std:  163.01 | min: -422.45 | steps so far: 2100000 | episodes so far:   9190\n",
            "Eval | on_policy | max:  313.28 | median:   49.94 | mean:   96.91 | std:  138.83 | min: -205.31 | steps so far: 2150000 | episodes so far:   9310\n",
            "Eval | on_policy | max:  285.26 | median:   83.87 | mean:   98.49 | std:  149.83 | min: -320.86 | steps so far: 2200000 | episodes so far:   9430\n",
            "Eval | on_policy | max:  283.59 | median:   73.77 | mean:   96.15 | std:  147.59 | min: -200.14 | steps so far: 2250000 | episodes so far:   9530\n",
            "Eval | on_policy | max:  310.18 | median:   20.82 | mean:   64.37 | std:  158.21 | min: -243.47 | steps so far: 2300000 | episodes so far:   9630\n",
            "Eval | on_policy | max:  273.39 | median:   61.33 | mean:   69.00 | std:  174.92 | min: -346.90 | steps so far: 2350000 | episodes so far:   9770\n",
            "Eval | on_policy | max:  302.45 | median:   46.92 | mean:   67.00 | std:  167.86 | min: -364.95 | steps so far: 2400000 | episodes so far:   9870\n",
            "Eval | on_policy | max:  298.79 | median:  192.47 | mean:  122.40 | std:  151.25 | min: -379.32 | steps so far: 2450000 | episodes so far:   9990\n",
            "Eval | on_policy | max:  303.70 | median:  191.90 | mean:  126.90 | std:  146.68 | min: -249.29 | steps so far: 2500000 | episodes so far:  10110\n",
            "Eval | on_policy | max:  304.88 | median:  200.20 | mean:  117.16 | std:  157.34 | min: -290.25 | steps so far: 2550000 | episodes so far:  10210\n",
            "Eval | on_policy | max:  298.12 | median:  190.34 | mean:  104.59 | std:  170.09 | min: -327.20 | steps so far: 2600000 | episodes so far:  10350\n",
            "Eval | on_policy | max:  287.36 | median:  122.67 | mean:   82.50 | std:  161.15 | min: -323.89 | steps so far: 2650000 | episodes so far:  10450\n",
            "Eval | on_policy | max:  310.17 | median:   38.21 | mean:   88.26 | std:  157.19 | min: -381.75 | steps so far: 2700000 | episodes so far:  10570\n",
            "Eval | on_policy | max:  290.75 | median:  119.27 | mean:   94.19 | std:  162.82 | min: -269.11 | steps so far: 2750000 | episodes so far:  10670\n",
            "Eval | on_policy | max:  301.17 | median:  183.32 | mean:  108.48 | std:  161.28 | min: -525.19 | steps so far: 2800000 | episodes so far:  10790\n",
            "Eval | on_policy | max:  289.05 | median:   52.43 | mean:   83.14 | std:  158.48 | min: -395.18 | steps so far: 2850000 | episodes so far:  10890\n",
            "Eval | on_policy | max:  314.89 | median:  173.96 | mean:  111.46 | std:  159.91 | min: -270.85 | steps so far: 2900000 | episodes so far:  11010\n",
            "Eval | on_policy | max:  304.24 | median:  208.61 | mean:  128.64 | std:  154.00 | min: -272.36 | steps so far: 2950000 | episodes so far:  11110\n",
            "Eval | on_policy | max:  312.73 | median:  177.32 | mean:  116.33 | std:  160.67 | min: -257.26 | steps so far: 3000000 | episodes so far:  11230\n",
            "Eval | on_policy | max:  305.50 | median:  222.71 | mean:  131.56 | std:  161.34 | min: -304.21 | steps so far: 3050000 | episodes so far:  11350\n",
            "Eval | on_policy | max:  286.50 | median:  119.78 | mean:   89.99 | std:  169.90 | min: -260.86 | steps so far: 3100000 | episodes so far:  11470\n",
            "Eval | on_policy | max:  310.02 | median:  157.94 | mean:  125.20 | std:  148.49 | min: -223.98 | steps so far: 3150000 | episodes so far:  11590\n",
            "Eval | on_policy | max:  303.16 | median:  213.89 | mean:  133.08 | std:  145.13 | min: -308.71 | steps so far: 3200000 | episodes so far:  11710\n",
            "Eval | on_policy | max:  293.27 | median:  230.87 | mean:  134.06 | std:  153.01 | min: -411.11 | steps so far: 3250000 | episodes so far:  11810\n",
            "Eval | on_policy | max:  317.83 | median:  230.59 | mean:  166.93 | std:  121.04 | min: -173.62 | steps so far: 3300000 | episodes so far:  11930\n",
            "Eval | on_policy | max:  297.03 | median:  217.29 | mean:  129.77 | std:  148.29 | min: -212.98 | steps so far: 3350000 | episodes so far:  12030\n",
            "Eval | on_policy | max:  298.24 | median:  222.60 | mean:  128.59 | std:  156.48 | min: -254.79 | steps so far: 3400000 | episodes so far:  12150\n",
            "Eval | on_policy | max:  314.46 | median:  222.53 | mean:  157.20 | std:  145.36 | min: -212.80 | steps so far: 3450000 | episodes so far:  12250\n",
            "Eval | on_policy | max:  309.79 | median:   52.17 | mean:  105.78 | std:  146.45 | min: -407.87 | steps so far: 3500000 | episodes so far:  12370\n",
            "Eval | on_policy | max:  292.08 | median:  222.93 | mean:  133.51 | std:  159.44 | min: -262.62 | steps so far: 3550000 | episodes so far:  12470\n",
            "Eval | on_policy | max:  302.59 | median:  203.25 | mean:  125.78 | std:  158.50 | min: -208.89 | steps so far: 3600000 | episodes so far:  12590\n",
            "Eval | on_policy | max:  295.93 | median:  236.31 | mean:  133.52 | std:  175.11 | min: -496.89 | steps so far: 3650000 | episodes so far:  12710\n",
            "Eval | on_policy | max:  292.78 | median:  201.99 | mean:  112.70 | std:  172.71 | min: -369.50 | steps so far: 3700000 | episodes so far:  12810\n",
            "Eval | on_policy | max:  301.42 | median:  210.57 | mean:  125.84 | std:  160.57 | min: -320.38 | steps so far: 3750000 | episodes so far:  12930\n",
            "Eval | on_policy | max:  324.08 | median:  226.65 | mean:  135.31 | std:  162.06 | min: -356.69 | steps so far: 3800000 | episodes so far:  13030\n",
            "Eval | on_policy | max:  297.51 | median:  234.44 | mean:  144.64 | std:  167.74 | min: -463.82 | steps so far: 3850000 | episodes so far:  13130\n",
            "Eval | on_policy | max:  299.06 | median:  229.60 | mean:  136.87 | std:  153.42 | min: -317.78 | steps so far: 3900000 | episodes so far:  13230\n",
            "Eval | on_policy | max:  296.39 | median:  221.48 | mean:  139.68 | std:  147.99 | min: -255.01 | steps so far: 3950000 | episodes so far:  13330\n",
            "Eval | on_policy | max:  304.08 | median:  228.41 | mean:  148.85 | std:  150.21 | min: -331.95 | steps so far: 4000000 | episodes so far:  13450\n",
            "Eval | on_policy | max:  300.54 | median:  236.56 | mean:  143.25 | std:  167.42 | min: -358.03 | steps so far: 4050000 | episodes so far:  13550\n",
            "Eval | on_policy | max:  305.85 | median:  229.36 | mean:  122.27 | std:  180.18 | min: -517.06 | steps so far: 4100000 | episodes so far:  13650\n",
            "Eval | on_policy | max:  302.37 | median:  245.58 | mean:  155.17 | std:  173.28 | min: -631.22 | steps so far: 4150000 | episodes so far:  13750\n",
            "Eval | on_policy | max:  303.10 | median:  215.39 | mean:  116.02 | std:  176.96 | min: -404.34 | steps so far: 4200000 | episodes so far:  13870\n",
            "Eval | on_policy | max:  301.02 | median:  241.81 | mean:  135.10 | std:  200.75 | min: -533.39 | steps so far: 4250000 | episodes so far:  13970\n",
            "Eval | on_policy | max:  301.40 | median:  176.13 | mean:   94.46 | std:  198.96 | min: -596.47 | steps so far: 4300000 | episodes so far:  14070\n",
            "Eval | on_policy | max:  302.80 | median:  233.90 | mean:  134.05 | std:  186.98 | min: -539.02 | steps so far: 4350000 | episodes so far:  14170\n",
            "Eval | on_policy | max:  308.36 | median:  238.06 | mean:  162.88 | std:  145.83 | min: -305.44 | steps so far: 4400000 | episodes so far:  14290\n",
            "Eval | on_policy | max:  290.97 | median:  233.99 | mean:  150.72 | std:  150.34 | min: -402.46 | steps so far: 4450000 | episodes so far:  14390\n",
            "Eval | on_policy | max:  289.40 | median:  243.78 | mean:  158.11 | std:  167.21 | min: -398.66 | steps so far: 4500000 | episodes so far:  14490\n",
            "Eval | on_policy | max:  308.25 | median:  238.24 | mean:  141.91 | std:  171.28 | min: -474.29 | steps so far: 4550000 | episodes so far:  14610\n",
            "Eval | on_policy | max:  290.20 | median:  219.22 | mean:  130.48 | std:  184.28 | min: -590.49 | steps so far: 4600000 | episodes so far:  14710\n",
            "Eval | on_policy | max:  312.79 | median:  216.27 | mean:  130.71 | std:  155.28 | min: -318.92 | steps so far: 4650000 | episodes so far:  14810\n",
            "Eval | on_policy | max:  302.86 | median:  242.92 | mean:  160.13 | std:  150.04 | min: -241.01 | steps so far: 4700000 | episodes so far:  14930\n",
            "Eval | on_policy | max:  306.04 | median:  247.08 | mean:  167.95 | std:  142.25 | min: -419.71 | steps so far: 4750000 | episodes so far:  15030\n",
            "Eval | on_policy | max:  302.35 | median:  243.97 | mean:  172.82 | std:  168.20 | min: -632.02 | steps so far: 4800000 | episodes so far:  15130\n",
            "Eval | on_policy | max:  300.63 | median:  244.61 | mean:  165.94 | std:  158.93 | min: -587.66 | steps so far: 4850000 | episodes so far:  15230\n",
            "Eval | on_policy | max:  291.56 | median:  236.25 | mean:  143.46 | std:  172.65 | min: -492.06 | steps so far: 4900000 | episodes so far:  15330\n",
            "Eval | on_policy | max:  313.16 | median:  250.61 | mean:  158.24 | std:  152.93 | min: -290.12 | steps so far: 4950000 | episodes so far:  15430\n",
            "Eval | on_policy | max:  294.04 | median:  245.54 | mean:  150.81 | std:  182.00 | min: -496.64 | steps so far: 5000000 | episodes so far:  15530\n",
            "Eval | on_policy | max:  304.44 | median:  245.35 | mean:  159.71 | std:  189.12 | min: -640.64 | steps so far: 5050000 | episodes so far:  15630\n",
            "Eval | on_policy | max:  300.93 | median:  243.20 | mean:  169.26 | std:  137.28 | min: -213.90 | steps so far: 5100000 | episodes so far:  15730\n",
            "Eval | on_policy | max:  309.65 | median:  243.49 | mean:  166.49 | std:  148.36 | min: -325.42 | steps so far: 5150000 | episodes so far:  15830\n",
            "Eval | on_policy | max:  312.00 | median:  244.13 | mean:  173.07 | std:  148.44 | min: -329.82 | steps so far: 5200000 | episodes so far:  15910\n",
            "Eval | on_policy | max:  296.76 | median:  240.80 | mean:  154.83 | std:  169.10 | min: -621.43 | steps so far: 5250000 | episodes so far:  16010\n",
            "Eval | on_policy | max:  299.77 | median:  231.32 | mean:  128.47 | std:  198.81 | min: -506.19 | steps so far: 5300000 | episodes so far:  16110\n",
            "Eval | on_policy | max:  300.41 | median:  229.99 | mean:  143.40 | std:  172.83 | min: -483.25 | steps so far: 5350000 | episodes so far:  16230\n",
            "Eval | on_policy | max:  320.40 | median:  247.91 | mean:  153.96 | std:  171.05 | min: -509.34 | steps so far: 5400000 | episodes so far:  16330\n",
            "Eval | on_policy | max:  300.89 | median:  250.85 | mean:  185.36 | std:  147.60 | min: -389.38 | steps so far: 5450000 | episodes so far:  16430\n",
            "Eval | on_policy | max:  297.68 | median:  238.80 | mean:  156.84 | std:  142.01 | min: -205.01 | steps so far: 5500000 | episodes so far:  16530\n",
            "Eval | on_policy | max:  302.81 | median:  245.19 | mean:  168.70 | std:  161.25 | min: -584.54 | steps so far: 5550000 | episodes so far:  16630\n",
            "Eval | on_policy | max:  306.38 | median:  243.29 | mean:  140.71 | std:  194.50 | min: -516.37 | steps so far: 5600000 | episodes so far:  16730\n",
            "Eval | on_policy | max:  302.01 | median:  241.80 | mean:  147.61 | std:  182.67 | min: -533.55 | steps so far: 5650000 | episodes so far:  16830\n",
            "Eval | on_policy | max:  303.87 | median:  238.34 | mean:  160.89 | std:  152.68 | min: -304.45 | steps so far: 5700000 | episodes so far:  16950\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-89200be848af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0meval_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_medians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_training_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eval.on_policy.mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aa34210bfd9b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         episodes, eval_results = self.run_episodes(self.current_step_limit, self.current_desired_return,\n\u001b[1;32m    101\u001b[0m                                                    \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                                                    n_episodes=self.config.n_episodes_per_iter)\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_episodes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_episodes_per_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aa34210bfd9b>\u001b[0m in \u001b[0;36mrun_episodes\u001b[0;34m(self, step_limit, desired_return, label, actions, n_episodes, render)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mclipped_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbehavior_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m# for sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aa34210bfd9b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                            True)\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # environment\n",
        "    env_name = 'LunarLander-v2'\n",
        "    n_state = 8\n",
        "    n_action = 4\n",
        "    train_dir = '.'\n",
        "    env_max_return = 300\n",
        "    env_step_limit = 1000\n",
        "    clip_limits = None\n",
        "    warmup_desired_return = 0\n",
        "    warmup_step_limit = 100\n",
        "    # agent\n",
        "    net_option = 'bilinear'\n",
        "    net_arch = (64, 128, 128)\n",
        "    action_type = 'discrete'\n",
        "    activation = 'relu'\n",
        "    return_scale = 0.015\n",
        "    horizon_scale = 0.03\n",
        "    # training & testing\n",
        "    replay = 'highest'\n",
        "    replay_size = 600\n",
        "    n_warm_up_episodes = 50\n",
        "    n_episodes_per_iter = 20\n",
        "    last_few = 100\n",
        "    learning_rate = 0.0008709635899560805\n",
        "    batch_size = 768\n",
        "    n_updates_per_iter = 150\n",
        "    max_training_steps = 10_000_000\n",
        "    eval_freq = 50_000\n",
        "    eval_goal = 'current'\n",
        "    train_alg = 'udrl'\n",
        "    train_seeds = (1_000_000, 10_000_000)\n",
        "    eval_seeds = (1, 500_000)\n",
        "    n_eval_episodes = 100\n",
        "    actions = 'on_policy'\n",
        "    save_model = True\n",
        "    verbose = False\n",
        "    use_gpu = True #False\n",
        "    seed = 9\n",
        "\n",
        "ud = UpsideDownAgent(Config)\n",
        "ud.warm_up()\n",
        "print(f\"Warm-up complete. Starting training.\")\n",
        "eval_means, eval_medians = [], []\n",
        "while ud.total_steps < Config.max_training_steps:\n",
        "    results = ud.train_step()\n",
        "    for r in results:\n",
        "        if r[0] == 'eval.on_policy.mean':\n",
        "            eval_means.append(r[1])\n",
        "        if r[0] == 'eval.on_policy.median':\n",
        "            eval_medians.append(r[1])\n",
        "    ud.msg(f'Iteration {ud.iters} complete\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPyzwfxWpGjs"
      },
      "outputs": [],
      "source": [
        "for x in range(10):\n",
        "    results = ud.train_step()\n",
        "    for r in results:\n",
        "        if r[0] == 'eval.on_policy.mean':\n",
        "            eval_means.append(r[1])\n",
        "        if r[0] == 'eval.on_policy.median':\n",
        "            eval_medians.append(r[1])\n",
        "    ud.msg(f'Iteration {ud.iters} complete\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RzDOZCmtoOaH"
      },
      "outputs": [],
      "source": [
        "name = \"from_UDRL.pth\"\n",
        "# torch.save(ud.state_dict(), name)\n",
        "torch.save(ud.behavior_fn.state_dict(), name)\n",
        "# torch.save(ud.behavior_fn.state_dict, name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jdOXqHxNuLZi",
        "outputId": "9287c9e3-8c0a-4ca6-94ba-861dca70d196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 124\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTzdCcsiw3SH"
      },
      "outputs": [],
      "source": [
        "ud = UpsideDownAgent(Config)\n",
        "# ud.load_state_dict(torch.load(name))\n",
        "ud.behavior_fn.load_state_dict(torch.load(name))\n",
        "# ud.eval()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sCCOEthAqwrP"
      ],
      "name": "from_UDRL-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

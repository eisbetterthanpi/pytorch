{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/curiousity_perceiverio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQjuJIepAvA"
      },
      "source": [
        "#### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C1GD7lk8H13h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4403cee-76db-48f1-b92e-97c641365eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting gym\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 31.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793153 sha256=809ed5d0ee9599aecaed845183b16a47c2b366175d455a07f32e2dae34ab4306\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0e/54/63d9f3d16ddf0fec1622e90d28140df5e6016bcf8ea920037d\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.24.1 gym-notices-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=279078d036d1b734f435e1cd4ae9b05e68817e6f8edb34a47374d7ef5c3503ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 33.0 MB/s \n",
            "\u001b[?25hCollecting nes-py\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (4.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (0.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (3.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp37-cp37m-linux_x86_64.whl size=438000 sha256=e0981d098e4cd9390807c66f67634e96966957aedb7b0eb78a7dba06f052b567\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/96/0e/22a8c7dbdf412d8e988286f223b223baf0f4ad90c9e699c56d\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.64.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3132 sha256=c480a1d19aac7320bebccefea231bd423e9ab605024cb9e2f49838a46270fc58\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/0a/2a/86955ea711b461ab7918236fed2568733f75ed677d0524b56c\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting perceiver-pytorch\n",
            "  Downloading perceiver_pytorch-0.8.3-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from perceiver-pytorch) (1.11.0+cu113)\n",
            "Collecting einops>=0.3\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->perceiver-pytorch) (4.1.1)\n",
            "Installing collected packages: einops, perceiver-pytorch\n",
            "Successfully installed einops-0.4.1 perceiver-pytorch-0.8.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# # https://github.com/kimhc6028/pytorch-noreward-rl\n",
        "# https://stackoverflow.com/questions/67808779/running-gym-atari-in-google-colab\n",
        "%pip install -U gym\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "import gym\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "!pip install colabgymrender\n",
        "!pip install perceiver-pytorch\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "log=False\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.login() # \n",
        "# wandb.init(project=\"curiousity_simple\", entity=\"bobdole\")\n",
        "# log=True\n",
        "\n",
        "!pip install einops\n",
        "from math import pi, log\n",
        "from functools import wraps\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# functions"
      ],
      "metadata": {
        "id": "uYNDFUK_cp1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            self.total_rewards = 0\n",
        "            return observation, 0, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        # print(\"MarioSparse\",life,score)\n",
        "        # if done: return observation, self.total_rewards, done, info\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            return observation, score, True, info # lost one life, end env\n",
        "        else:\n",
        "            # self.total_score = 0\n",
        "            return observation, score, False, info\n",
        "    def reset(self):\n",
        "        # self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 30:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            return observation, reward, True, info # early stop\n",
        "        else:\n",
        "            return observation, reward, False, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "0HtmCw7DAuOR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gym wrappers"
      ],
      "metadata": {
        "id": "Q6rIxoaDeT4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            self.total_rewards = 0\n",
        "            return observation, 0, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        if done: return observation, reward, done, info\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            return observation, score, True, info # lost one life, end env\n",
        "        else:\n",
        "            # self.total_score = 0\n",
        "            return observation, score, False, info\n",
        "    def reset(self):\n",
        "        # self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        if done: return observation, reward, done, info\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            return observation, reward, True, info # early stop\n",
        "        else:\n",
        "            return observation, reward, False, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "L4wnbtD_eRxa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### helpers"
      ],
      "metadata": {
        "id": "ncK_m9R1Pxsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OvAaHBGgK2qY"
      },
      "outputs": [],
      "source": [
        "# helpers\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cache_fn(f):\n",
        "    cache = None\n",
        "    # @wraps(f)\n",
        "    def cached_fn(*args, _cache = True, **kwargs):\n",
        "        if not _cache:\n",
        "            return f(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = f(*args, **kwargs)\n",
        "        return cache\n",
        "    return cached_fn\n",
        "\n",
        "# helper classes\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        if exists(self.norm_context):\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context = normed_context)\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None):\n",
        "        h = self.heads\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIO"
      ],
      "metadata": {
        "id": "DsVq7W_oPzzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PerceiverIO class save\n",
        "class PerceiverIO(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        dim,\n",
        "        queries_dim,\n",
        "        logits_dim = None,\n",
        "        num_latents = 512,\n",
        "        latent_dim = 512,\n",
        "        cross_heads = 1,\n",
        "        latent_heads = 8,\n",
        "        cross_dim_head = 64,\n",
        "        latent_dim_head = 64,\n",
        "        weight_tie_layers = False,\n",
        "        decoder_ff = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([get_latent_attn(**cache_args), get_latent_ff(**cache_args)]))\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
        "\n",
        "    def forward(self, data, mask = None, queries = None):\n",
        "        b, *_, device = *data.shape, data.device\n",
        "        x = repeat(self.latents, 'n d -> b n d', b = b)\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        # cross attention only happens once for Perceiver IO\n",
        "        x = cross_attn(x, context = data, mask = mask) + x\n",
        "        x = cross_ff(x) + x\n",
        "        # layers\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "        if not exists(queries):\n",
        "            return x\n",
        "        # make sure queries contains batch dimension\n",
        "        if queries.ndim == 2:\n",
        "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
        "        # cross attend from decoder queries to latents\n",
        "        latents = self.decoder_cross_attn(queries, context = x)\n",
        "        if exists(self.decoder_ff):\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "        return self.to_logits(latents)\n",
        "\n",
        "def preprocess(X):\n",
        "    if X.dim()==1:\n",
        "        X=X.unsqueeze(dim=0)\n",
        "    X=X.flatten(start_dim=1, end_dim=-1) #(start_dim=1)\n",
        "    X=X.unsqueeze(dim=1)\n",
        "    return X\n",
        "\n",
        "def postprocess(logits):\n",
        "    if logits.dim()==3:\n",
        "        logits=logits.squeeze(dim=1)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "6_r7WRKMLcc2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIOrnn"
      ],
      "metadata": {
        "id": "hqxWCz07-Yv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PerceiverIOrnn(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        dim,\n",
        "        queries_dim,\n",
        "        logits_dim = None,\n",
        "        num_latents = 512,\n",
        "        latent_dim = 512,\n",
        "        cross_heads = 1,\n",
        "        latent_heads = 8,\n",
        "        cross_dim_head = 64,\n",
        "        latent_dim_head = 64,\n",
        "        weight_tie_layers = False,\n",
        "        decoder_ff = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "        self.latents = torch.zeros(num_latents, latent_dim)\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([get_latent_attn(**cache_args), get_latent_ff(**cache_args)]))\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
        "\n",
        "    def forward(self, data, mask = None, queries = None, x = None):\n",
        "        b, *_, device = *data.shape, data.device\n",
        "        if x == None: x = repeat(self.latents, 'n d -> b n d', b = b).to(device)\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        # cross attention only happens once for Perceiver IO\n",
        "        x = cross_attn(x, context = data, mask = mask) + x\n",
        "        x = cross_ff(x) + x\n",
        "        # layers\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "        if not exists(queries):\n",
        "            return x\n",
        "        # make sure queries contains batch dimension\n",
        "        if queries.ndim == 2:\n",
        "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
        "        # cross attend from decoder queries to latents\n",
        "        latents = self.decoder_cross_attn(queries, context = x)\n",
        "        if exists(self.decoder_ff):\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "        # return self.to_logits(latents)\n",
        "        return x, self.to_logits(latents)\n"
      ],
      "metadata": {
        "id": "i8Lh8uAVLixh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### cnn"
      ],
      "metadata": {
        "id": "ZWHdTK3-efrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Conv(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.Conv2d(in_channels, 1, 5, stride=2, padding=3),\n",
        "            nn.Conv2d(in_channels, 1, 5, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.MaxPool2d(3, stride=2, padding=0),\n",
        "            # nn.MaxPool2d(5, stride=2, padding=1),\n",
        "        )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        x = self.conv(x)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [4, 1, 56, 56]\n",
        "\n",
        "class Conv_Encoder(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv_Encoder, self).__init__()\n",
        "        self.conv_encoder = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "            nn.Conv2d(in_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((64,64)),\n",
        "            nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "            nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "            # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        x = self.conv_encoder(x)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [4, 1, 56, 56]\n"
      ],
      "metadata": {
        "id": "a7L9VGgY6NNY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### perceiverio + fourier"
      ],
      "metadata": {
        "id": "lErlrfaRpQmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from einops import rearrange, repeat\n",
        "import numpy as np\n",
        "num_freq_bands = 6 # num_bands = 4\n",
        "max_freq = 10\n",
        "# freq_base = 2,\n",
        "\n",
        "\n",
        "input_axis = 2 # 2 for images, 3 for video\n",
        "input_channels = 3\n",
        "# fourier_encode_data True\n",
        "fourier_channels = (input_axis * ((num_freq_bands * 2) + 1)) # 26\n",
        "input_dim = fourier_channels + input_channels # 29\n",
        "# print(\"input_dim\",input_dim)\n",
        "\n",
        "\n",
        "def fourier_encode(x, max_freq = 10, num_bands = 6):\n",
        "    x = x.unsqueeze(-1)\n",
        "    device, dtype, orig_x = x.device, x.dtype, x\n",
        "    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)\n",
        "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
        "    x = x * scales * np.pi\n",
        "    x = torch.cat([x.sin(), x.cos()], dim = -1)\n",
        "    x = torch.cat((x, orig_x), dim = -1)\n",
        "    return x\n",
        "\n",
        "def fourier(data): # https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py\n",
        "    b, *axis, _, device, dtype = *data.shape, data.device, data.dtype\n",
        "    axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))\n",
        "    pos = torch.stack(torch.meshgrid(*axis_pos, indexing = 'ij'), dim = -1) # [32, 32, 2]\n",
        "    # print(\"fourier pos\",pos.shape)\n",
        "    enc_pos = fourier_encode(pos, max_freq, num_freq_bands) # [32, 32, 2, 13]\n",
        "    # print(\"fourier fourier_encode\",enc_pos.shape)\n",
        "    enc_pos = rearrange(enc_pos, '... n d -> ... (n d)') # [32, 32, 26]\n",
        "    # print(\"fourier enc_pos rearrange\",enc_pos.shape)\n",
        "    enc_pos = repeat(enc_pos, '... -> b ...', b = b) # [4, 32, 32, 26]\n",
        "    # print(\"fourier enc_pos\",enc_pos.shape)\n",
        "    data = torch.cat((data, enc_pos), dim = -1) # [4, 32, 32, 29]\n",
        "    # print(\"fourier cat\",data.shape)\n",
        "    data = rearrange(data, 'b ... d -> b (...) d') # [4, 1024, 29]\n",
        "    # print(\"fourier rearrange\",data.shape)\n",
        "    return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHwWgPmYH1JH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 224 -> CNN -> 32\n",
        "# images = torch.randn(4, 3, 32, 32, device=device) # batch, rgb, dim_x, dim_y\n",
        "# seq = np.transpose(images, (0, 2, 3, 1)) # [4, 32, 32, 3] batch, dim_x, dim_y, rgb\n",
        "\n",
        "images = torch.randn(240, 256, 3, device=device) # dim_x, dim_y, rgb\n",
        "seq = fourier(images) # [4, 1024, 29] [240, 256, 16]\n",
        "print(seq.shape)\n",
        "# batch,_,h,w= seq.shape\n",
        "# batch,axis,input_dim=seq.shape # [240, 256, 16]\n",
        "h,w,input_dim=seq.shape # [240, 256, 16]\n",
        "\n",
        "model = PerceiverIO(\n",
        "    dim = h*w*input_dim,         # 32*32 dimension of sequence to be encoded\n",
        "    queries_dim = 10,            # dimension of decoder queries\n",
        "    logits_dim = None,           # dimension of final logits\n",
        "    depth = 6,                   # depth of net\n",
        "    num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 128,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        ").to(device)\n",
        "\n",
        "seq = seq.flatten()\n",
        "seq = preprocess(seq) # [4, 1, 29696]\n",
        "# print(seq.shape)\n",
        "queries = torch.randn(1, 10, device=device)\n",
        "logits = model(seq, queries = queries)\n",
        "pred = postprocess(logits)\n",
        "# print(pred.shape)\n",
        "# print(pred)\n",
        "pred_probab = nn.Softmax(dim=1)(pred)\n",
        "outputs = pred_probab\n",
        "\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "id": "g9egXpRiwdkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2dcbed0-3959-43b0-aca3-7cd8378a7e42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([240, 256, 16])\n",
            "tensor([5], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models"
      ],
      "metadata": {
        "id": "Sk-h3eIjSYVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model simplier"
      ],
      "metadata": {
        "id": "TDFTp2wVc-5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jGpJGeDM8HvU"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/model.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, in_shape, action_space):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.in_dim = in_shape # mario (240, 256)\n",
        "        self.conv = nn.Sequential( # embed pi\n",
        "            nn.Conv2d(in_shape[0], 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(), # added for RuntimeError: Input batch size 2 doesn't match hidden0 batch size 1\n",
        "            )\n",
        "        self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        num_outputs = action_space.n\n",
        "        self.critic_linear = nn.Linear(256, 1) # -> value\n",
        "        self.actor_linear = nn.Linear(256, num_outputs) # -> action\n",
        "\n",
        "        self.inv_lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        self.fwd_lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        self.inv_linear = nn.Sequential( # inv learning, predict at\n",
        "            nn.Linear(in_shape[1] + in_shape[1], 256), nn.ReLU(),\n",
        "            nn.Linear(256, num_outputs), nn.Softmax()\n",
        "            ) # cat(phi(st), phi(st+1)) -> athat\n",
        "        self.fwd_linear = nn.Sequential( # predict phi st+1\n",
        "            nn.Linear(in_shape[1] + num_outputs, 256), nn.ReLU(),\n",
        "            nn.Linear(256, in_shape[1])\n",
        "            ) # cat(phi(st), at) -> phihat(st+1)\n",
        "\n",
        "    def forward(self, inputs, icm):\n",
        "        if icm == False: #A3C\n",
        "            st, (a3c_hx, a3c_cx) = inputs # [1, 210, 160, 3], ([1, 256], [1, 256])\n",
        "            vec_st = self.conv(st).view(-1, self.in_dim[1])\n",
        "            a3c_hx1, a3c_cx1 = self.lstm(vec_st, (a3c_hx, a3c_cx))\n",
        "            critic = self.critic_linear(a3c_hx1)\n",
        "            actor = self.actor_linear(a3c_hx1)\n",
        "            # print(\"forward A3C \",critic.shape, actor.shape, a3c_hx.shape, a3c_cx.shape)\n",
        "            return critic, actor, (a3c_hx1, a3c_cx1) # [1, 1], [1, 18], ([1, 256], [1, 256])\n",
        "\n",
        "        else: #icm\n",
        "            (inv_hx, inv_cx), (fwd_hx, fwd_cx), st1, at = inputs\n",
        "            vec_st1 = self.conv(st1).view(-1, self.in_dim[1])\n",
        "            inv_hx1, inv_cx1 = self.inv_lstm(vec_st1, (icm_hx, icm_cx)) # inv model\n",
        "            fwd_hx1, fwd_cx1 = self.fwd_lstm(vec_st1, (icm_hx, icm_cx)) # world model\n",
        "\n",
        "            inv_vec = torch.cat((icm_hx, vec_st1), 1) # predict at\n",
        "            fwd_vec = torch.cat((icm_hx, at), 1) # predict vec_st1\n",
        "            inverse = self.inv_linear(inv_vec)\n",
        "            forward = self.fwd_linear(fwd_vec)\n",
        "            # print(\"forward icm \",vec_st1.shape, inverse.shape, forward.shape)\n",
        "            return vec_st1, inverse, forward, (inv_hx1, inv_cx1), (fwd_hx1, fwd_cx1) # [1, 320], [1, 18], [1, 320], ()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### curiousity perceiverio"
      ],
      "metadata": {
        "id": "KxNSVT9rurZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, in_shape, action_space):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.in_dim = in_shape # mario (240, 256, 3)\n",
        "        self.conv = Conv(in_shape[2]).to(device) # embed pi\n",
        "        # self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        # print(in_shape[0]*in_shape[1]/4)\n",
        "        self.encoder = PerceiverIO( # conv -> PerceiverIO ;encodes state for everyone\n",
        "            dim = int(in_shape[0]*in_shape[1]/4),# dimension of sequence to be encoded\n",
        "            queries_dim = 256,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 2,                   # depth of net\n",
        "            num_latents = 16,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 16,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # st(240*256) -zeros> phist(256)\n",
        "        self.encoder_query = torch.zeros(1, 256, device=device)\n",
        "        num_outputs = action_space.n\n",
        "        self.lstm = PerceiverIOrnn( # latent + phist1(256) -zeros> latent1 + vec_st(256)\n",
        "            dim = 256,                  # dimension of sequence to be encoded\n",
        "            queries_dim = 256,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 2,                   # depth of net\n",
        "            num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 32,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # st-> phist ; 240*256 -zeros> 256\n",
        "        self.lstm_query = torch.zeros(1, 256, device=device)\n",
        "        self.actor_linear = nn.Linear(256, num_outputs) # vec_st -> action\n",
        "        self.critic_linear = nn.Linear(256, 1) # vec_st -> value\n",
        "\n",
        "        self.inv_lstm = PerceiverIOrnn( # inverse model, predict taken action\n",
        "            dim = 256,                   # dimension of sequence to be encoded\n",
        "            queries_dim = num_outputs,   # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 64,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 64,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # inv_latent + phi(st+1) -zeros> at +inv_latent1\n",
        "        self.inv_query = torch.zeros(1, num_outputs, device=device)\n",
        "        self.fwd_lstm = PerceiverIOrnn( # world model\n",
        "            dim = 256 + num_outputs,     # dimension of sequence to be encoded\n",
        "            queries_dim = 256,           # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 128,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 32,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 32,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # fwd_latent + phi(st) cat at -zeros> phi(st1) +fwd_latent1\n",
        "        self.fwd_query = torch.zeros(1, 256, device=device)\n",
        "\n",
        "    def encode(self, st):\n",
        "        st = torch.transpose(st, 1,2)\n",
        "        st = torch.transpose(st, 0,1) # [3, 240, 256] rgb, dim_x, dim_y\n",
        "        # vec_st = self.conv(st).view(-1, self.in_dim[1]) # [15, 256]\n",
        "        cst = self.conv(st).flatten() # [120*128]\n",
        "        cst = cst.view(1,1,-1) # [1, 1, 120*128]\n",
        "        phist = self.encoder(cst, queries = self.encoder_query) # \n",
        "        return phist # 256\n",
        "\n",
        "    def forward(self, inputs, icm):\n",
        "        if icm == False: #A3C\n",
        "            st, latent = inputs # [240, 256, 3]\n",
        "            phist = self.encode(st)\n",
        "            latent1, vec_st = self.lstm(phist, queries = self.lstm_query, x=latent) # \n",
        "            critic = self.critic_linear(vec_st)\n",
        "            actor = self.actor_linear(vec_st)\n",
        "            return critic[0], actor[0], latent1 # [1, 1], [1, 18], \n",
        "        else: #icm\n",
        "            inv_latent, fwd_latent, st1, at = inputs\n",
        "            phist = self.encode(st1)\n",
        "            inv_latent1, inverse = self.inv_lstm(phist, queries = self.inv_query, x=inv_latent) # inv model; inv_latent + phi(st+1) -> at +inv_latent1\n",
        "            fwd_latent1, forward = self.fwd_lstm(torch.cat((phist, at.unsqueeze(0)), -1), queries = self.fwd_query, x=fwd_latent) # world model; fwd_latent + at cat phi(st) -> phi(st1) +fwd_latent1\n",
        "            inverse = nn.Softmax()(inverse[0])\n",
        "            forward = nn.Softmax()(forward[0])\n",
        "            # print(\"forward icm \",phist.shape, inverse.shape, forward.shape)\n",
        "            # print(\"forward icm \",inverse, forward)\n",
        "            return phist[0], inverse, forward, inv_latent1, fwd_latent1 # [1, 320], [1, 18], [1, 320], ()\n"
      ],
      "metadata": {
        "id": "9xCONztqwRdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### alternate light"
      ],
      "metadata": {
        "id": "mLirlem_Se4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, in_shape, action_space):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.in_dim = in_shape # mario (240, 256, 3)\n",
        "        self.conv = Conv(in_shape[2]).to(device) # embed pi\n",
        "        phist_size= 512\n",
        "        # self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        # print(in_shape[0]*in_shape[1]/4)\n",
        "        self.encoder = PerceiverIO( # conv -> PerceiverIO ;encodes state for everyone\n",
        "            dim = int(in_shape[0]*in_shape[1]/4),# dimension of sequence to be encoded\n",
        "            queries_dim = 256,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 2,                   # depth of net\n",
        "            num_latents = 16,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 16,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # st(240*256) -zeros> phist(256)\n",
        "        self.encoder_query = torch.zeros(1, 256, device=device)\n",
        "        \n",
        "        self.conv_encoder = Conv_Encoder(in_shape[2]).to(device)\n",
        "        num_outputs = action_space.n\n",
        "        self.lstm = PerceiverIOrnn( # latent + phist1(256) -zeros> latent1 + vec_st(256)\n",
        "            dim = 256,                  # dimension of sequence to be encoded\n",
        "            queries_dim = 256,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 2,                   # depth of net\n",
        "            num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 32,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # st-> phist ; 240*256 -zeros> 256\n",
        "        self.lstm_query = torch.zeros(1, 256, device=device)\n",
        "        self.lstmcell = nn.LSTMCell(256, phist_size)\n",
        "\n",
        "        self.actor_linear = nn.Linear(phist_size, num_outputs) # vec_st -> action\n",
        "        self.critic_linear = nn.Linear(phist_size, 1) # vec_st -> value\n",
        "\n",
        "        self.inv_lstm = PerceiverIOrnn( # inverse model, predict taken action\n",
        "            dim = 256,                   # dimension of sequence to be encoded\n",
        "            queries_dim = num_outputs,   # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 64,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 64,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # inv_latent + phi(st+1) -zeros> at +inv_latent1\n",
        "        self.inv_query = torch.zeros(1,1, num_outputs, device=device)\n",
        "        self.fwd_lstm = PerceiverIOrnn( # world model\n",
        "            dim = 256 + num_outputs,     # dimension of sequence to be encoded\n",
        "            queries_dim = 256,           # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 128,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 32,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 32,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # fwd_latent + phi(st) cat at -zeros> phi(st1) +fwd_latent1\n",
        "        self.fwd_query = torch.zeros(1, 256, device=device)\n",
        "\n",
        "    def encode(self, st):\n",
        "        st = torch.transpose(st, 1,2)\n",
        "        st = torch.transpose(st, 0,1) # [3, 240, 256] rgb, dim_x, dim_y\n",
        "        # vec_st = self.conv(st).view(-1, self.in_dim[1]) # [15, 256]\n",
        "        cst = self.conv(st).flatten() # [120*128]\n",
        "        cst = cst.view(1,1,-1) # [1, 1, 120*128]\n",
        "        phist = self.encoder(cst, queries = self.encoder_query) # \n",
        "        return phist # 256\n",
        "\n",
        "    def conv_encode(self, st):\n",
        "        st = torch.transpose(st, 1,2)\n",
        "        st = torch.transpose(st, 0,1) # [3, 240, 256] rgb, dim_x, dim_y\n",
        "        phist = self.conv_encoder(st).flatten() # [256]\n",
        "        # phist = phist.view(1,1,-1)\n",
        "        phist = phist.view(1,-1)\n",
        "        return phist # 256\n",
        "\n",
        "    def forward(self, inputs, icm):\n",
        "        if icm == False: #A3C\n",
        "            # st, latent = inputs # [240, 256, 3]\n",
        "            # phist = self.encode(st) # using perceiverio to encode\n",
        "            # latent1, vec_st = self.lstm(phist, queries = self.lstm_query, x=latent)\n",
        "            # vec_st, a3c_cx1 = self.lstmcell(phist, latent)\n",
        "\n",
        "            st, (a3c_hx, a3c_cx) = inputs # [240, 256, 3]\n",
        "            phist = self.conv_encode(st) # using cnn to encode\n",
        "            # print(phist.shape, (a3c_hx.shape, a3c_cx.shape)) # [1, 1, 256] ([1, 512], [1, 512])\n",
        "            vec_st, a3c_cx1 = self.lstmcell(phist, (a3c_hx, a3c_cx))\n",
        "\n",
        "            critic = self.critic_linear(vec_st)\n",
        "            actor = self.actor_linear(vec_st)\n",
        "            # print(critic.shape,actor.shape)\n",
        "            # return critic[0], actor[0], latent1 # for perceiverio encode [1, 1], [1, 18], \n",
        "            return critic, actor, (vec_st, a3c_cx1) # for cnn encode [1, 1], [1, 18], \n",
        "        else: #icm\n",
        "            inv_latent, fwd_latent, st1, at = inputs\n",
        "            # phist = self.encode(st1) # perceiverio\n",
        "            phist = self.conv_encode(st1).unsqueeze(0) # cnn [1, 1, 256]\n",
        "            # print(phist.shape, self.inv_query.shape) #) torch.Size([1, 12]\n",
        "            inv_latent1, inverse = self.inv_lstm(phist, queries = self.inv_query, x=inv_latent) # inv model; inv_latent + phi(st+1) -> at +inv_latent1\n",
        "            fwd_latent1, forward = self.fwd_lstm(torch.cat((phist, at.unsqueeze(0)), -1), queries = self.fwd_query, x=fwd_latent) # world model; fwd_latent + at cat phi(st) -> phi(st1) +fwd_latent1\n",
        "            inverse = nn.Softmax()(inverse[0])\n",
        "            forward = nn.Softmax()(forward[0])\n",
        "            # print(\"forward icm \",phist.shape, inverse.shape, forward.shape)\n",
        "            # print(\"forward icm \",inverse, forward)\n",
        "            return phist[0], inverse, forward, inv_latent1, fwd_latent1 # [1, 320], [1, 18], [1, 320], ()\n"
      ],
      "metadata": {
        "id": "fRov_Uc2Sjf4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "h7kyJMKO9uPJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpPda4YEv13"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Erf14x8KEv2A"
      },
      "outputs": [],
      "source": [
        "# train.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/train.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def train(env, args, model, optimizer=None):\n",
        "    # torch.manual_seed(seed)\n",
        "    # model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "    if optimizer is None:\n",
        "        optimizer = torch.optim.Adam(shared_model.parameters(), lr)\n",
        "    model.train()\n",
        "    for x in range(num_episodes):\n",
        "        # model.load_state_dict(shared_model.state_dict()) # Sync with the shared model\n",
        "        latent = None\n",
        "        vec_st = torch.zeros(1, 256).to(device)\n",
        "        a3c_hx = torch.zeros(1, 512).to(device)\n",
        "        a3c_cx = torch.zeros(1, 512).to(device)\n",
        "        inv_latent = None\n",
        "        fwd_latent = None\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropies = []\n",
        "        inverses = []\n",
        "        forwards = []\n",
        "        actions = []\n",
        "        vec_st1s = []\n",
        "        episode_length = 0\n",
        "\n",
        "        state = env.reset()\n",
        "        # state=state[:,:,0]\n",
        "        state = torch.from_numpy(state.copy()).type(torch.float).to(device) # i added, change from int to float\n",
        "        st1 = state.float()\n",
        "        # print(\"#####www####\",state.dtype,hx.dtype)\n",
        "        while True:\n",
        "            episode_length += 1\n",
        "            # value, logit, latent = model((state, latent), icm = False)\n",
        "            value, logit, (a3c_hx, a3c_cx) = model((state, (a3c_hx, a3c_cx)), icm = False)\n",
        "            # print(value.shape,logit.shape)\n",
        "\n",
        "            prob = F.softmax(logit, dim=1)\n",
        "            log_prob = F.log_softmax(logit, dim=1)\n",
        "            entropy = -(log_prob * prob).sum(1)\n",
        "            entropies.append(entropy.cpu())\n",
        "            action = prob.multinomial(1).data\n",
        "            log_prob = log_prob.gather(1, action)\n",
        "            oh_action = torch.zeros(1, env.action_space.n)\n",
        "            oh_action[0][action.item()] = 1.0\n",
        "            at = oh_action\n",
        "            actions.append(oh_action)\n",
        "            state, reward, done, _ = env.step(action.item())\n",
        "            state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "            # state=state[:,:,0]\n",
        "            # print(\"reward\",reward)\n",
        "            done = done or episode_length >= max_episode_length\n",
        "            # reward = max(min(reward, 1), -1) #why clip rewards?\n",
        "            st = st1\n",
        "            st1 = state.float()\n",
        "            vec_st1, inverse, forward, inv_latent, fwd_latent = model((inv_latent, fwd_latent, st1, at.to(device)), icm = True)            \n",
        "            reward_intrinsic = eta * ((vec_st1 - forward).pow(2)).sum(1) / 2.\n",
        "            #reward_intrinsic = eta * ((vec_st1 - forward).pow(2)).sum(1).sqrt() / 2.\n",
        "            # print(\"reward_intrinsic\", reward_intrinsic)\n",
        "            reward_intrinsic = reward_intrinsic.item()\n",
        "            # print(\"ep \",x,\", rwd ext: \", reward, \" ,rwd int: \", reward_intrinsic.item())\n",
        "            reward += reward_intrinsic\n",
        "            values.append(value.cpu())\n",
        "            log_probs.append(log_prob.cpu())\n",
        "            rewards.append(reward)\n",
        "            vec_st1s.append(vec_st1.cpu())\n",
        "            inverses.append(inverse.cpu())\n",
        "            forwards.append(forward.cpu())\n",
        "            if done:\n",
        "                print(episode_length)\n",
        "                episode_length = 0\n",
        "                break\n",
        "        R = torch.zeros(1, 1)\n",
        "        if not done:\n",
        "            # value, _, _ = model((state, latent), icm = False)\n",
        "            value, _, _ = model((state, (a3c_hx, a3c_cx)), icm = False)\n",
        "            R = value.data\n",
        "        values.append(R)\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "        inverse_loss = 0\n",
        "        forward_loss = 0\n",
        "        gae = torch.zeros(1, 1)\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = gamma * R + rewards[i]\n",
        "            advantage = R - values[i]\n",
        "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
        "            # Generalized Advantage Estimataion\n",
        "            # delta_t = rewards[i] + gamma * values[i + 1].data - values[i].data\n",
        "            delta_t = torch.tensor(rewards[i]) + gamma * values[i + 1].data - values[i].data\n",
        "            gae = gae * gamma * tau + delta_t\n",
        "            policy_loss = policy_loss - log_probs[i] * gae - 0.01 * entropies[i]\n",
        "            cross_entropy = - (actions[i] * torch.log(inverses[i] + 1e-15)).sum(1)\n",
        "            inverse_loss = inverse_loss + cross_entropy\n",
        "            forward_err = forwards[i] - vec_st1s[i]\n",
        "            forward_loss = forward_loss + 0.5 * (forward_err.pow(2)).sum(1)\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"invvvvv\",inverse_loss , forward_loss)\n",
        "        # ((1-beta) * inverse_loss + beta * forward_loss).backward(retain_variables=True)\n",
        "        inv_loss = (1-beta) * inverse_loss + beta * forward_loss\n",
        "        pol_loss = lmbda * (policy_loss + 0.5 * value_loss)\n",
        "        (inv_loss + pol_loss).backward()\n",
        "        # (inv_loss + 0*pol_loss).backward()\n",
        "        # (((1-beta) * inverse_loss + beta * forward_loss) + lmbda * (policy_loss + 0.5 * value_loss)).backward()\n",
        "        print(''.join([str(torch.argmax(a).item()) for a in actions]))\n",
        "        print(\"inv_loss: \", inv_loss.item(), \" ,pol_loss: \", pol_loss.item())\n",
        "        # if log:\n",
        "        #     wandb.log({\"inv_loss\": inv_loss.item(), \"pol_loss\": pol_loss.item()})\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 40)\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHpcDteZdLS"
      },
      "source": [
        "#### test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ja6KZf13p--B"
      },
      "outputs": [],
      "source": [
        "# test.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/test.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def test(env, args, model):\n",
        "    # torch.manual_seed(seed)\n",
        "    # model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "    # model.load_state_dict(shared_model.state_dict())\n",
        "    model.eval()\n",
        "    state = env.reset()\n",
        "    state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "    reward_sum = 0\n",
        "    start_time = time.time()\n",
        "    actions = []\n",
        "    episode_length = 0\n",
        "    result = []\n",
        "    latent = None\n",
        "    a3c_hx = torch.zeros(1, 512).to(device)\n",
        "    a3c_cx = torch.zeros(1, 512).to(device)\n",
        "    while True:\n",
        "        episode_length += 1\n",
        "        # value, logit, latent = model((state, latent), icm = False)\n",
        "        value, logit, (a3c_hx, a3c_cx) = model((state, (a3c_hx, a3c_cx)), icm = False)\n",
        "        prob = F.softmax(logit, dim=1) #from train\n",
        "        action = prob.multinomial(1).data\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "        state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "\n",
        "        done = done or episode_length >= max_episode_length\n",
        "        # print(\"rwd ext: \", reward)\n",
        "        reward_sum += reward\n",
        "        actions.append(action[0])\n",
        "        if done:\n",
        "            end_time = time.time()\n",
        "            print(\"Time {}, episode reward {}, episode length {}\".format(\n",
        "                time.strftime(\"%Hh %Mm %Ss\", time.gmtime(end_time - start_time)), reward_sum, episode_length))\n",
        "            result.append((reward_sum, end_time - start_time))\n",
        "            torch.save(model.state_dict(), 'model.pth')\n",
        "            # print(''.join([str(a.item()) for a in actions]))\n",
        "            print([a.item() for a in actions])\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj3tv7XHZmD9"
      },
      "source": [
        "#### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjPLqIBgH9xJ",
        "outputId": "f906d53f-aac2-4e23-fa04-11274a1112b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3) Discrete(12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/main.py\n",
        "# import os, sys, cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "\n",
        "lr=0.001\n",
        "gamma=0.99\n",
        "tau=1.00\n",
        "seed=1\n",
        "num_processes=4\n",
        "num_steps=20\n",
        "max_episode_length=500 # 10000\n",
        "# env_name='PongDeterministic-v4'\n",
        "# env_name='LunarLander-v2'\n",
        "# env_name='MontezumaRevengeDeterministic-v4'\n",
        "# env_name='MontezumaRevengeDeterministic-ram-v4'\n",
        "\n",
        "no_shared=False\n",
        "eta=0.01\n",
        "beta=0.2\n",
        "lmbda=0.1\n",
        "outdir=\"output\"\n",
        "record='store_true'\n",
        "num_episodes=10#100\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "# env = gym.make(env_name)\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n",
        "# query_environment(\"MountainCar-v0\")\n",
        "\n",
        "print(env.observation_space.shape, env.action_space) # (210, 160, 3) Discrete(18); mario complex (240, 256, 3) Discrete(12)\n",
        "\n",
        "shared_model = ActorCritic(env.observation_space.shape, env.action_space).to(device)\n",
        "# shared_model.share_memory()\n",
        "if no_shared:\n",
        "    optimizer = None\n",
        "else:\n",
        "    optimizer = torch.optim.Adam(shared_model.parameters(), lr=lr)\n",
        "    # optimizer.share_memory()\n",
        "args=None\n",
        "# train(0, args, shared_model, optimizer)\n",
        "\n",
        "# processes = []\n",
        "# import torch.multiprocessing as mp\n",
        "# p = mp.Process(target=test, args=(num_processes, args, shared_model))\n",
        "# p.start()\n",
        "# processes.append(p)\n",
        "# for rank in range(0, num_processes):\n",
        "#     p = mp.Process(target=train, args=(rank, args, shared_model, optimizer))\n",
        "#     p.start()\n",
        "#     processes.append(p)\n",
        "# for p in processes:\n",
        "#     p.join()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFrRKvOwhYM_"
      },
      "source": [
        "#### run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-KuzIfVWPBIg"
      },
      "outputs": [],
      "source": [
        "max_episode_length=2000 # 10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dHLw7ewldCLm",
        "outputId": "128e8706-a35c-4302-fe66-689c077a37c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "08627007421868968721004477101159649\n",
            "inv_loss:  65.11924743652344  ,pol_loss:  -0.2707885205745697\n",
            "MarioEarlyStop: early stop  91\n",
            "306\n",
            "2101108252195111911157469102254349554028566101010348304883847104436058111071084375108115119259681023381004011810546571117536441124391045825106151110440716750780695109113849249288645450265994743109828108841122648611118510102210132111010038253810761010809440105621528011247716353671032010109782810761172710121724211405114516101115210995101191958408776059106310495\n",
            "inv_loss:  617.2274169921875  ,pol_loss:  6.465696811676025\n",
            "MarioEarlyStop: early stop  42\n",
            "46\n",
            "2811091530370605592118111581011511855610327105995374610\n",
            "inv_loss:  93.12956237792969  ,pol_loss:  -0.4165794551372528\n",
            "MarioEarlyStop: early stop  52\n",
            "109\n",
            "73432765110551118434866310590792321115184524390671192102456115331011932165950104414746108971155021010995328770251821061814\n",
            "inv_loss:  217.3004150390625  ,pol_loss:  -0.912269115447998\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "856027118450101326098087151721083118\n",
            "inv_loss:  63.39533615112305  ,pol_loss:  -0.21948157250881195\n",
            "MarioEarlyStop: early stop  41\n",
            "38\n",
            "40211400786711831796583111110114849663992107\n",
            "inv_loss:  77.39045715332031  ,pol_loss:  -0.2838243544101715\n",
            "MarioEarlyStop: early stop  44\n",
            "48\n",
            "73123294011105610491630701158941071034011351092740774115\n",
            "inv_loss:  96.47679138183594  ,pol_loss:  -0.5506592392921448\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "672911410397101111129911066210106708516\n",
            "inv_loss:  63.92876434326172  ,pol_loss:  -0.3190711438655853\n",
            "MarioEarlyStop: early stop  41\n",
            "46\n",
            "72112748548210484972956048711989303686546111635797\n",
            "inv_loss:  93.27137756347656  ,pol_loss:  -0.3566431403160095\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "1171071163101077411105114362157694587711\n",
            "inv_loss:  62.6143798828125  ,pol_loss:  -0.2845204472541809\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "6864811101968011108218827211357610212\n",
            "inv_loss:  65.7021484375  ,pol_loss:  -0.20066745579242706\n",
            "MarioEarlyStop: early stop  43\n",
            "50\n",
            "375104414610961527876458110009485782847293254251109016\n",
            "inv_loss:  102.7449722290039  ,pol_loss:  -0.24166595935821533\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "65103175225117610112045322103610670501\n",
            "inv_loss:  63.48465347290039  ,pol_loss:  -0.15313664078712463\n",
            "MarioEarlyStop: early stop  44\n",
            "61\n",
            "931512084092602711611041125511103611411789483655116111394466380126102109\n",
            "inv_loss:  121.58507537841797  ,pol_loss:  -0.04903049394488335\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "18971117271052858107903462611422310112\n",
            "inv_loss:  63.38145446777344  ,pol_loss:  -0.0794723853468895\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "101150910889108113712111061149841133513\n",
            "inv_loss:  65.43565368652344  ,pol_loss:  -0.012250901199877262\n",
            "MarioEarlyStop: early stop  163\n",
            "271\n",
            "811985410106841043559341031532633608161571113397101923194095261451481198114410181064727492924810155406411121111053787809010638091210325831034121541010437115500701062119285121193011119103111060650911031601022906610432781061045405246082251095111190435661008853733611076109725548510905863398461066869871098115811116230\n",
            "inv_loss:  554.3003540039062  ,pol_loss:  6.4379096031188965\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "17106067101540691172676026853032456\n",
            "inv_loss:  64.72098541259766  ,pol_loss:  -0.04100028797984123\n",
            "MarioEarlyStop: early stop  42\n",
            "55\n",
            "107245465260951011118514114310684636649107961291111752886921074711\n",
            "inv_loss:  111.85843658447266  ,pol_loss:  0.11012052744626999\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "875811103476347013722011466610161563\n",
            "inv_loss:  63.605857849121094  ,pol_loss:  -0.021176470443606377\n",
            "MarioEarlyStop: early stop  60\n",
            "94\n",
            "129861263101811101011823124910607101084356941171319281172410880144234622826711261181016111041011381070545301101\n",
            "inv_loss:  185.60226440429688  ,pol_loss:  0.37463176250457764\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "8725173621169751257251601101694044\n",
            "inv_loss:  63.416080474853516  ,pol_loss:  -0.09643994271755219\n",
            "MarioEarlyStop: early stop  45\n",
            "104\n",
            "1961131111791410299109011906114245471945842581183368383392177410108581349110810711821118991022161095596493336773538610557\n",
            "inv_loss:  212.41787719726562  ,pol_loss:  -0.06082922965288162\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "6311099711311378772116761112111104810024\n",
            "inv_loss:  64.50956726074219  ,pol_loss:  -0.12166174501180649\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "6718211411119604640511111115796779110107\n",
            "inv_loss:  65.3621597290039  ,pol_loss:  -0.15295030176639557\n",
            "MarioEarlyStop: early stop  41\n",
            "41\n",
            "111324739211117081088751164809638108911287486937\n",
            "inv_loss:  84.0276870727539  ,pol_loss:  -0.1994551569223404\n",
            "MarioEarlyStop: early stop  46\n",
            "72\n",
            "0411404834103250966801086119511322761135583111064471170910784281010471136051031055964\n",
            "inv_loss:  144.39439392089844  ,pol_loss:  -0.259524941444397\n",
            "MarioEarlyStop: early stop  343\n",
            "354\n",
            "1141013310151110859110949819710110451022687434883991144243109433551103212111677831129710611077144272347012161111541034161051081110471117118331169356771104639444345204411866266501032666171679175010289855104284542817691011110726881365811111011681114109167104750861710511214861004425169408101182151331125498566610109438101105103241111191165870393926104580140800411121105683100161109103593671171391011971101175211110184211\n",
            "inv_loss:  712.9419555664062  ,pol_loss:  101868224.0\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "10811101669118410909101773170943961083\n",
            "inv_loss:  65.3705825805664  ,pol_loss:  -0.40542563796043396\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "6628106444801086054517354709776619\n",
            "inv_loss:  62.65483093261719  ,pol_loss:  -0.6483965516090393\n",
            "MarioEarlyStop: early stop  133\n",
            "186\n",
            "387134438503251075123158101071310726101111173401102711148266578947105465744670101111110249109091109041154631268937190801010410997551010555281010136910071031110741851041403150941994789256898674311833662033101443650117\n",
            "inv_loss:  379.6510925292969  ,pol_loss:  -3.418691396713257\n",
            "MarioEarlyStop: early stop  75\n",
            "279\n",
            "834804865311591116934105842112521131141088114811301090101002821159581044263210310044341104598138501321010710611026793101081330420118728214511133221106387111409981543110100103981984473107746180507110457041051105379462409324597100123916944599154944116011776101194210383408112847010896711489946118114538651151891163811134\n",
            "inv_loss:  569.093505859375  ,pol_loss:  -10.509177207946777\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "6206007544251060675510039611109310311\n",
            "inv_loss:  63.933353424072266  ,pol_loss:  -1.335252046585083\n",
            "MarioEarlyStop: early stop  61\n",
            "104\n",
            "948410115351134302031170299066721072117110190113911702510144371510241110315401184871111751073118963218948333571108051071036\n",
            "inv_loss:  214.16259765625  ,pol_loss:  -0.5955575704574585\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "843210881099711140107721148284033411101\n",
            "inv_loss:  66.90994262695312  ,pol_loss:  -0.12785518169403076\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioSparse: died\n",
            "MarioEarlyStop: early stop  245\n",
            "356\n",
            "35104374622770710109381384363811311103821173232424910811339737076703452520340101056642292510510459766424952711134504441113111040494469549110101132801101058816781641091978191163151534963240011417609747863522171466473698053510148381119381108426745944059741076661115116710118110512089611022692592745076310236106611031171078611141541124811268211031359411092539317117449110018803831011351502181019245101102841933\n",
            "inv_loss:  726.496337890625  ,pol_loss:  -2.976048707962036\n",
            "MarioEarlyStop: early stop  54\n",
            "107\n",
            "471072914101129293406259341106910110854210088821110547431021100435263111101648209422101037710953586822911116988710519409111108\n",
            "inv_loss:  216.91062927246094  ,pol_loss:  -7.060186862945557\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "699831794541013221197334768104620310\n",
            "inv_loss:  64.89395141601562  ,pol_loss:  -3.0111911296844482\n",
            "MarioEarlyStop: early stop  41\n",
            "45\n",
            "5555290113411784866079604101003107573420101108801011011\n",
            "inv_loss:  94.10397338867188  ,pol_loss:  -5.119350910186768\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "7111102964961197395367105118018371192\n",
            "inv_loss:  68.13758087158203  ,pol_loss:  -3.48667573928833\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "988111084454278111131191111100547849939\n",
            "inv_loss:  66.16232299804688  ,pol_loss:  -2.6442127227783203\n",
            "MarioEarlyStop: early stop  43\n",
            "50\n",
            "349538114119137307886259561155051798581013920672900080\n",
            "inv_loss:  99.5759048461914  ,pol_loss:  -2.6203129291534424\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "1098532101197187611987191160121910821\n",
            "inv_loss:  66.65731048583984  ,pol_loss:  -0.4913419783115387\n",
            "MarioEarlyStop: early stop  68\n",
            "176\n",
            "31150071171010469156106910471109153511101146348481021172791111071013106463945103425131088148110291145103572101110101181735581410111149108115304771658485162048605288406311110188680710561059011164931081100101066199\n",
            "inv_loss:  368.86822509765625  ,pol_loss:  8.92935562133789\n",
            "MarioEarlyStop: early stop  76\n",
            "160\n",
            "24110211574106105112421051034210680052405374544100611761039770119116101047211305318110115511176555929261160441191138108103045410117461027101142210110061128109925247824976632410336987236811518\n",
            "inv_loss:  333.8074951171875  ,pol_loss:  7.622432231903076\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "8844422641191110100118854000141111099\n",
            "inv_loss:  65.04017639160156  ,pol_loss:  -0.4211302697658539\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "10710996321063381165353736611430103805\n",
            "inv_loss:  65.53472137451172  ,pol_loss:  -0.8858912587165833\n",
            "MarioEarlyStop: early stop  130\n",
            "170\n",
            "8742391733421012551076104894458156911511311111185412261321127626233724819477105819114933110954282111183821238611357651110849241119101076211851110611449001110450997610109513909112113118116010599105491015\n",
            "inv_loss:  345.5689392089844  ,pol_loss:  -1.3527368307113647\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "11084627671111876329574045410113119112\n",
            "inv_loss:  65.08009338378906  ,pol_loss:  -0.8605969548225403\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "95789698057851011521110411106610165288\n",
            "inv_loss:  64.86454010009766  ,pol_loss:  -1.003673791885376\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "848225116430988221160330114910203394\n",
            "inv_loss:  64.20374298095703  ,pol_loss:  -0.9961259961128235\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "6881499568829113410413610511103481670\n",
            "inv_loss:  63.95368194580078  ,pol_loss:  -1.0059452056884766\n",
            "MarioEarlyStop: early stop  45\n",
            "70\n",
            "5201168502116044732721032123380398909710837499811108162010900701110026104918283\n",
            "inv_loss:  140.95899963378906  ,pol_loss:  -1.851829171180725\n",
            "MarioEarlyStop: early stop  41\n",
            "44\n",
            "754306294311410986603910396111169811381125791688110\n",
            "inv_loss:  88.0695571899414  ,pol_loss:  -0.6011383533477783\n",
            "MarioEarlyStop: early stop  52\n",
            "132\n",
            "1017599931711362111108823949811311115710444138386311301088254641177809221011796217112298211046063100983032744510711115711689611217410551030911834515061152109\n",
            "inv_loss:  264.9564208984375  ,pol_loss:  -0.24308685958385468\n",
            "MarioEarlyStop: early stop  47\n",
            "75\n",
            "1120350125101011105610910106011134104389118841045494420112572111187615511111758250761127061110\n",
            "inv_loss:  149.95986938476562  ,pol_loss:  -1.7033995389938354\n",
            "MarioEarlyStop: early stop  102\n",
            "251\n",
            "534718021056425011781045618653015916883535145966011710689910013510107108101009820401030111651111510811221141482119104541044824855325210831157610211311114363541915106692809667741015211014953112885982254603459635245631010370830110942810415741131119746887289671111019728910301023139663436662\n",
            "inv_loss:  505.3312072753906  ,pol_loss:  -5.777329444885254\n",
            "MarioEarlyStop: early stop  56\n",
            "148\n",
            "3910000328612841010104181096395101315162982394950511110478562104468510721195105113518916132883109711114731711119492703349111034726277483991885948711110330103880980010856\n",
            "inv_loss:  296.3700866699219  ,pol_loss:  -4.725355625152588\n",
            "MarioEarlyStop: early stop  45\n",
            "74\n",
            "811022711097115872701515486412345827101148110118954674556660004272722671749711837\n",
            "inv_loss:  147.39248657226562  ,pol_loss:  -2.1033060550689697\n",
            "MarioEarlyStop: early stop  48\n",
            "70\n",
            "11151011011106119181210104821477116402100111150169850114101092210194359393512111107394\n",
            "inv_loss:  141.4403839111328  ,pol_loss:  -1.9560555219650269\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "110629100107811114103697567647961115611\n",
            "inv_loss:  63.78855895996094  ,pol_loss:  -1.2058823108673096\n",
            "MarioEarlyStop: early stop  41\n",
            "39\n",
            "0442911101188797112298182210710071051869994097\n",
            "inv_loss:  77.51019287109375  ,pol_loss:  -1.3429065942764282\n",
            "MarioEarlyStop: early stop  41\n",
            "45\n",
            "1151116209913107449651118107294095921070671128040192\n",
            "inv_loss:  89.9411392211914  ,pol_loss:  -0.6172929406166077\n",
            "MarioEarlyStop: early stop  42\n",
            "68\n",
            "6017102681036685111111039931051111141106066573699105032103022801621011110210117512\n",
            "inv_loss:  137.6895294189453  ,pol_loss:  -0.4560191333293915\n",
            "MarioEarlyStop: early stop  65\n",
            "89\n",
            "04111021043163154785103399410144029264111010421141626011210137894961111811110891110131066165046808472593\n",
            "inv_loss:  179.50987243652344  ,pol_loss:  -1.9406778812408447\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "189019831104415808359110115022792105\n",
            "inv_loss:  64.79222106933594  ,pol_loss:  -0.8460957407951355\n",
            "MarioEarlyStop: early stop  53\n",
            "90\n",
            "43268274111085404253105618317911541111888108811192576151246953411683845080911511957710222731111100241085\n",
            "inv_loss:  180.21038818359375  ,pol_loss:  -0.606081485748291\n",
            "MarioEarlyStop: early stop  80\n",
            "169\n",
            "71010116825101711113208111810519436105702072263201040192111084508243210113901121104108960341052110107530754759111180141804673111510643061115436035571112166844910488763725841171108120211125172128596\n",
            "inv_loss:  336.9078674316406  ,pol_loss:  -1.6085504293441772\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "1010656911341143161511310987591060103296\n",
            "inv_loss:  64.28593444824219  ,pol_loss:  -0.732852578163147\n",
            "MarioEarlyStop: early stop  59\n",
            "95\n",
            "422311463180840144910010522020701141143031080539572040010109081010551158610109628821111110106081088117163310575\n",
            "inv_loss:  190.33892822265625  ,pol_loss:  -1.0352152585983276\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "1180011011611635280101836102982117896112\n",
            "inv_loss:  65.16128540039062  ,pol_loss:  0.6243999600410461\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "047456647211929563996019542261988\n",
            "inv_loss:  64.15823364257812  ,pol_loss:  0.813607394695282\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "8551788235031694901299111191749221\n",
            "inv_loss:  63.5584831237793  ,pol_loss:  0.051589664071798325\n",
            "MarioEarlyStop: early stop  45\n",
            "88\n",
            "710235107450221101035792084171552101627106111115555015418574529981015366989345179743106831177583149\n",
            "inv_loss:  175.87506103515625  ,pol_loss:  -0.7782843112945557\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "7691002539475014010611966951110102505\n",
            "inv_loss:  64.67341613769531  ,pol_loss:  -0.09733649343252182\n",
            "MarioEarlyStop: early stop  49\n",
            "103\n",
            "611301101710240380626711268711011648304446450307387710005413107911410647234196959551072854311362710673465841121091088106\n",
            "inv_loss:  208.38290405273438  ,pol_loss:  -0.06772096455097198\n",
            "MarioEarlyStop: early stop  57\n",
            "163\n",
            "11109108223554931146723753503240111199929107185111211109147101072634319821264815871010908256728318736851310991417114427081031010114952101132022026311160680849221113680407109108311114106576125\n",
            "inv_loss:  325.9515380859375  ,pol_loss:  -1.47532057762146\n",
            "MarioEarlyStop: early stop  51\n",
            "98\n",
            "9528115491134461085101111823216665551064357253849940231111103023110254352161008491259511611193218651095020711259107\n",
            "inv_loss:  195.74624633789062  ,pol_loss:  -0.0458947978913784\n",
            "MarioEarlyStop: early stop  53\n",
            "122\n",
            "119710111342416739965943702636484282617011111054910101261041082637059833320510111138011104254470110605210546107757100188687618157867391111411398\n",
            "inv_loss:  247.5253448486328  ,pol_loss:  5.112149238586426\n",
            "MarioEarlyStop: early stop  42\n",
            "65\n",
            "7401199103947851128119981121104192522808978115813905156261056858983553\n",
            "inv_loss:  129.33447265625  ,pol_loss:  2.715480327606201\n",
            "MarioEarlyStop: early stop  42\n",
            "67\n",
            "101124108147665326627931105733366909236391094948693668351070010731137103810\n",
            "inv_loss:  135.88291931152344  ,pol_loss:  0.9536598324775696\n",
            "MarioEarlyStop: early stop  86\n",
            "226\n",
            "5810211033110987106208011222731011010834117860119211010710926290310241011491110103239531162011610167511937104553281170654104210830365942271219191221186811291010477073105200371110071011874151420211423597588110234118004495691211454877644820235281077940910626660581011\n",
            "inv_loss:  454.9714050292969  ,pol_loss:  -0.5279244780540466\n",
            "MarioEarlyStop: early stop  53\n",
            "122\n",
            "20136410590107553746011111290631181210681139762423937001811052108010505378619677731114251161182512210118471128994160103737610296675335104993\n",
            "inv_loss:  248.0816192626953  ,pol_loss:  -0.8972244262695312\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "111010109010889611109115249167031181110415\n",
            "inv_loss:  65.0357437133789  ,pol_loss:  -0.4993959069252014\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "8810119109516617116638813295413148119\n",
            "inv_loss:  65.2276382446289  ,pol_loss:  0.06571371853351593\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "650575611111088429719267071117087118\n",
            "inv_loss:  64.18937683105469  ,pol_loss:  0.34298282861709595\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "196091107821111111410411106911035041162110\n",
            "inv_loss:  63.72683334350586  ,pol_loss:  0.09902045875787735\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "9869962871102214635968541101107116110\n",
            "inv_loss:  64.36268615722656  ,pol_loss:  -0.3436734974384308\n",
            "MarioEarlyStop: early stop  55\n",
            "164\n",
            "44111801325991250206800576111112010920560111111171064510261131164101010711116101021688116118619721110698531191724510117104414118108178651071148337011884932291127509588347559587975964681001617\n",
            "inv_loss:  328.7230224609375  ,pol_loss:  -0.12268149107694626\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "5669211401528382411102116101178796628\n",
            "inv_loss:  63.63248825073242  ,pol_loss:  -0.22680313885211945\n",
            "MarioEarlyStop: early stop  51\n",
            "115\n",
            "111126911651054842165210735816105111510192101032733727731878111122977531031111729101091148115828111163540810546417961011113107102111581129603210\n",
            "inv_loss:  232.21524047851562  ,pol_loss:  2.204408645629883\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "2909011492487769310125100810111189940\n",
            "inv_loss:  64.9314956665039  ,pol_loss:  0.4503071904182434\n",
            "MarioEarlyStop: early stop  315\n",
            "294\n",
            "2510131062316206534013248121522310711111210214443521150109477105011110101152101097892211324348251179393424113938118964918310946011411311947548101111110706911804810536121114847546667959101111211093668100116190531910497615558218034389011634271010993590711419432757671068710026851153847391111355011531114759992191078001810502741096087214022705110\n",
            "inv_loss:  599.463623046875  ,pol_loss:  999279232.0\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "9115692711510811110969103210951025289112\n",
            "inv_loss:  63.08919906616211  ,pol_loss:  -0.4531562030315399\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "1164110701010410862164802630107020181\n",
            "inv_loss:  64.77791595458984  ,pol_loss:  -0.26097264885902405\n",
            "MarioEarlyStop: early stop  45\n",
            "50\n",
            "3234101080001154741111113673501004901810113244757518113111197\n",
            "inv_loss:  103.14506530761719  ,pol_loss:  -0.07602111250162125\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "011010776748304665572673051163149114\n",
            "inv_loss:  65.72522735595703  ,pol_loss:  -0.6807565093040466\n",
            "MarioEarlyStop: early stop  41\n",
            "40\n",
            "114101719371099678356211157911585106862819594\n",
            "inv_loss:  79.24366760253906  ,pol_loss:  -1.6909115314483643\n",
            "MarioEarlyStop: early stop  42\n",
            "45\n",
            "104104310610855277162211568283229438579088690054108\n",
            "inv_loss:  91.69474029541016  ,pol_loss:  -1.9270877838134766\n",
            "MarioEarlyStop: early stop  42\n",
            "47\n",
            "311211404110904833116068898211779094033114071112158072\n",
            "inv_loss:  96.20458221435547  ,pol_loss:  -0.7708163857460022\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "911028951050628128064611511102459010117\n",
            "inv_loss:  63.402305603027344  ,pol_loss:  -0.5844287276268005\n",
            "MarioEarlyStop: early stop  40\n",
            "32\n",
            "1001163119006466491864634295803410106\n",
            "inv_loss:  63.53422164916992  ,pol_loss:  -1.5921884775161743\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-69189d8df01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d6e6d423ad3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, args, model, optimizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;31m# state=state[:,:,0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# print(\"reward\",reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train(env, args, shared_model)\n",
        "\n",
        "for x in range(20):\n",
        "    train(env, args, shared_model, optimizer)\n",
        "test(env, args, shared_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV2MvSK-aL_"
      },
      "source": [
        "#### save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TN-XW-LvZ8Xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cf6fc6-6eb1-4a2c-8ecc-07b00cf6d2d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "name='model_mario_perceiverio_fwdinv.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "model=shared_model\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# shared_model=model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6PNxVu-W6K"
      },
      "source": [
        "#### video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "W6fkhEcBB3tM",
        "outputId": "6837cf6a-ef63-4f8c-b532-c1208f2c3019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4145152/45929032 bytes (9.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8273920/45929032 bytes (18.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12222464/45929032 bytes (26.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16171008/45929032 bytes (35.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19922944/45929032 bytes (43.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23961600/45929032 bytes (52.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27918336/45929032 bytes (60.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31449088/45929032 bytes (68.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35479552/45929032 bytes (77.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39591936/45929032 bytes (86.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42401792/45929032 bytes (92.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n",
            "MarioEarlyStop: early stop  40\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls>\n",
              " <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAKERtZGF0AAACrQYF//+p\n",
              "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE0OCByMzMzIDkwYTYxZWMgLSBILjI2NC9NUEVH\n",
              "LTQgQVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5v\n",
              "cmcveDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5\n",
              "c2U9MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRf\n",
              "cmVmPTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRl\n",
              "YWR6b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MyBs\n",
              "b29rYWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVy\n",
              "bGFjZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9w\n",
              "eXJhbWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0w\n",
              "IHdlaWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVm\n",
              "cmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42\n",
              "MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAABsY\n",
              "ZYiEACP//sdv5lIWkEF5V2eiTJ559f0VfhQe/2K0PHi1DFu5CrqcTcNf4VWsQpJhScta1mO3EBf2\n",
              "Tc4exe7V+Bnri9vQ/lGFf62n48RozhYr2rdhg7jC5QlsmO2yqOL6tQE3g//3/jdQizC6rDvNaoW3\n",
              "+FCjlzNPkgUYE6INiiG2aNEJX9w1/5y5bWt5bOeqDwvdM1LRmnYhy6Ua1tGUGPzvDw2eZNBilZyW\n",
              "RTLrxN4gLF+S1QQcwSuho/chtSr8xVllvOFXSXlB2r66iDO13o6evggNDekSKWYiZOjW5fHCsdJ9\n",
              "PPFdwT/evEiiWOyGY0UyCSaATIzIE0mmG/HymqKiEG0tizof6+VBrJ764YaJodB7jmZW9K3NdhgN\n",
              "ZKSHIfiEBZyYExB9vPXPAFFxrgJ7B7EDxf0Pebeo8QKPVvU+nuw1sqtBzI5o9PtBD4dGm6J2iFwd\n",
              "7smP+3+aDhIx75/BMk7GZAh8enBgSMLZRc3YPRKE9Q+6WdmaSGFMyNDTvMDvsFw0N0PvtdxOvy8j\n",
              "nTgmW/Az3uEbkQJdcByyI1sTAqgS66T4cWLdAGXQ+hkIieLgFIfymEcmTsC/Hvv0DYJ6JMvhRIcS\n",
              "wrw2XKi5rOBXeTnJsYuY1Pzm7ah0O6a7fbr5Xyxuwy2uMyKft1jmEKqH5bTC6YVTQCvnH5cCrvPD\n",
              "4Z7mLFLYKfOh/+plahJ63jQfTrtF6z1QTW7ZZEIATM8U9Yf0y9zx2/du8XFAE2p6l9lhlakpsfrB\n",
              "5SkLUuTnP+jguAvsgoQz2UrC0ChQw5Nda1g72TIGXME0FUFwa/M0ZgBoVfW88ToPpJSFIHbAniKm\n",
              "GSXnAsKsJ48x7g+BLVVDPJUAx3Ke1bY99Lrw2Tc/QxArq/ZpzkX8/ShtABLD05edKVHIf9zGjTQq\n",
              "X4P+oXGWsiH5vmggOBfgoWKkqKhkrXRvO6iUjbaqpHVBOSXJbBHOhCpIbhxNxP4s/UTX3jXLZhKz\n",
              "nC9qziBukz5sC3Vd/pQTelVN/kQUmQtiIO5Qt23nqgzQNE1JnVNqn98x8LF/cJOzgT+5HxpTCQaS\n",
              "PaJkU9BqLhftsOwCM9Cd9Pyc6vFeRMHIT4uU6dN6AxU8Btw9UuQLcALynxGnN05NozVfoe1A5Te5\n",
              "h0DKHmd4+iMP+Ol8unfUwdc/wIPRZxxcaUEIk4nTZv08/n7H/VbOTtaPZCBzfngHOxIIYKnCYZHu\n",
              "2F275MA7gVTXOBWA78wl/n3Hawoc7wxVj8MEsQIEgqnczjGIxFxurclODvPal6wUjfMAJzvMs4/m\n",
              "4jogahnp4+l/B/8aCXEEoR1/3n6tnZzig9emWEc85l/VktoQ8Fe96pmvDmO8flzfcIcP7IXyk/Ps\n",
              "fkuYHFsTVM7NCcovEiViT4pR+Hce5rjg6DNNX/KLGo6vZ1jd18GIR5Bq0G/kdbwkvygFo/+U4RKS\n",
              "wSKkHFv7iNh4aghzVU825YHaBKF6Seu/ZaxZJfuV7vycmQIAeXawWd+bbfiUe8viWEn+SxurLEsZ\n",
              "Rk4jArkQvDXmMEhtLeLgFIfymFepOCF/gPajYKlR4+kYdJ2mmydnIJ4MDPZNfkPu8muuQq/AYPas\n",
              "b8QMJx2YY8/USLwo5ZZ7+rhlD2IknCkoXNMKf50lOgSHC+O6utyzBq4iKKNamNRay4gAnlIYQVju\n",
              "PO2NfOLX0KiuILTJZwCVhdJo2LVLOQTSW4cYz67lR9E8F5uyR9zB+707qcj4XQ4IrB5g+olD27FN\n",
              "f961Pv6SanRLbBqXmc6Ur9XGEXX8an81mxZ5prkVvOVK1xobgREpWtnQ2P0kbmQn4utU1jJxTwxD\n",
              "sz3iSG+Im4WELr6Huyvxyvc0yWgB+qLg/uCKOkKSds18Z9pDyKMFkWyMgsMDG8gMhG9V9ZOZoi5s\n",
              "52VZRg+tKwhIG7OESDTR9INMLvfNeYZpJtSpfwq9kyAfqWKK9EqAoL3bSVd4AEPo5UfXArfU/966\n",
              "tYLXTBxw9sZRgfNrTRRztSW+XPv+m33TPffyjjc63YfptqZ8i7WgNTjL4Bn+77j4a9qLSIrZRU/T\n",
              "8HVBp7IauTFl+PYKsNbatXGIEViewdzvlO26T6QNLJW/t3ZJGAIeY/JfV0iWr1HpNcdKk2K+S6Ap\n",
              "4tgaIcyoHm2+KuQUbUQc1ylaEnb5DYFpBbj6t5xGjkJsEazCczdhachnOqpqfc+FC578Ke+NjAkf\n",
              "icPCo+zxQOCwb25P2nCz0lDwtogyx1lyVWI1lPLa9ZShoq0G7d+vB3j7Fb4dSz+v5zuJpkwMlAAA\n",
              "Mm1rSdX5R8dyuv43B0NcLbmWha1OgDmpfQvldwmkUJV430pgia89enFED3TtH77yw7QZEp73MAZd\n",
              "3mD32/J0OrIIvKYcyso7+TNtxgK7dP8EK19Q1wppktnMNpTLNrwJfCHmjnYKAtHjS8cV1tBEvpWl\n",
              "+Gt/eFtH4lW6Cev5kXeLY3SJ52LXow8yeRsSPtKPka1OkVs919aRcOY9aRnwNKPYi9mmghwZmx7a\n",
              "iS/8DGXkj4JNJDj9lk8KffddowSfc6Y0MuHPaoR4C2DxOF7+vvE2AOs1ztNk4HMFGM4pJeVK62zv\n",
              "WWZ+VCf/EOgAUQ+21kVIfbosbXQ7nZonY5iW3MNvKDhefso4fzugSwGDcJuwAZervW3mxq4+XRx9\n",
              "Grm1tihAWHzgBVSZhywLppgmf1bJcz5udzJzsnnVuz3eWDZmCOv464OQzgfTi04yrxSWyWWS52xs\n",
              "/JQRHyCpkul0b0ijKFQPJY+cSD9N0rwz0WZdUfZUt4TR4QJdHyYz+61vJc6li9yHwsPcpy1J7jIe\n",
              "/fu59zffN597lGtpTBz0jdZz27Esp0AiEjnyVOM4/NY3lHfP9QoY25mml8Ed04RgXjmlxEP2MmiI\n",
              "+RMc+KTIHhGTE5kQUFs9EJ7tcbGjs9qW7P/PGoKldzjMl7FPH38d7fF22lxzj8PausxMVZxhth/s\n",
              "9Sqf3TGsQNosa39nf3/GrkTz2nmtQLQLW54cTwHRpjIKFLejglTDP25ePTTF7BhxO5JS3Hz18+ZM\n",
              "IKNqBluMyezkagUCr9G/YLiLefnxPMto8B9uXT15Vnu/ysXYqaMk/t9yiDNv2eUu2YmYE++WX9IR\n",
              "6r1Z/6NUmsbG476CLeCZgpxc9YbB1LYXNUwExGvWPODjzLVVRaD0ukYtZbYAEUp/TGJ1Cdp0IMCG\n",
              "bSG4WTeKx7OrqaXCeZbO4a6RU99wz3f5Y1443rPqb9PaHtDmoArc2Nx7OrU6chDPyXA7CX3IaGwE\n",
              "wrrq8L+r0kWU/s52lhSMYydm1SYy8uAJQccHQeRcGE2hh2p1eJ/vRXlqWcxQ0x0n2T6JNIpZl+Dn\n",
              "u/y4FkcfAmC4N+ZP4RGqj3dPQ6mdWp04uI5yUKFZg1RJAFBCsjomx7hBsSAp9rrybU9y1n6U8nW1\n",
              "PnQjwPo0qxOwoRXWxpGc6eD2KsmgEHgM65uFEZMQm+ShuXeDU+lYblWW/yXYtcYe9Mo3dXf2FAei\n",
              "VYM68UEjkg9H7O4QW7RD+wfdzi069y41VnYK7fxgkWDe+Oy7xK2oePe89B6vpEVQ0UOZ5A80THxZ\n",
              "rTPlQU7XDcnTSSpjRlNSUrH/dBLbGU5cMuw8XiRcMIMzW3gKd+ieUmS6cM8MiJ7LI0RTpywn8qrJ\n",
              "dmk1gMqOKYWH4R8W0ut7aAt937dBUJV/kSd35eIcs81pc02IzTOVp1QaJeI6MqRm/3X0ho/es7G8\n",
              "DN0SIFpD3AWeVUTazBChxmEi+gICKIx/tuLgYo24sGDXYUXJ9O8b6/6A22ryXDHnOgdDyVFoMB9P\n",
              "hq+WMhThGtTkIRsRv1jUuuPHWumBajM+ws+pXwHb71GD1mJ53IiR9uvwvXOiL5DlTbWcLjyKCRel\n",
              "WKB6o9X3Q83G7ief+LpZbBEEJO63dcawudx+yfscuxx5EYWspjM9JysRsvlpi0pR5SUvTv4sEm5b\n",
              "Gb3dbMye6Ie/pIzyyXlIKUEotWrcNhnmelLoYNVBk1lbWD+qT/rzZThNf3hH79S30cLTz9XQdtTg\n",
              "u3zEcIvDCz4eksBD5aTMKHpf1sY7X9HrjIJ99KEy0+GeMv0W9KZ1XH9JkLnSft3+0c2awIzMD69V\n",
              "DjyLK7WqF6mn3/MUWYRJTU0sacDHKQg+f+fzoPlGcCJ54AvaO8b/p5P4zG5cMec+yIPRy3b7Z/Lx\n",
              "VPbk0bo/dCqhX2Y1DSAB9PyJoq+h4bcmE3/7t0zlYyjMuQALBy2MCrSBF9o6Rg0llJ8TL4PR1BHB\n",
              "F54KJsYWArKrAIe+zwm6JcPFhgZxWVFrYK7Tv/OSpSZYl2UO0xURT4hASSpACHhZpIg9oQah4Lnc\n",
              "9o5YhEdNf0jn1AXNW0kn0X3Z0TJ56MCHy0lhnT+vfFo2XN0HfcyCpj02XvKNuVXYi5vk3rmkGyTl\n",
              "4AsWZbwbyJfpVdvIv6eRWHowQvvjJS8u5eFjLikvMYLXM3D0qdY8ydAqzCMpoOMAbZzEv4guxzwI\n",
              "Oyiivz6Ilrb/xAGkZpWBtJnbAJX0DK4vAaMcesJ59fbmpbD9Wy3ZJX2PKdbo9VzMZgmiRmzWRZ9C\n",
              "L671v3a1tpOQyaqhmgS5akvPfXYtKTZtcRZYvF5lUoj5TAPlVuj0qyxD8jwBLt+lEIL4KoVHLm78\n",
              "YFydY8ydAqxr82cOMAbZzEwEZuxzwIOyiiw7xCloQEkqYL3rSUuCc6iHSwK8w5wR6vUEmm0keJQ1\n",
              "CXXcHWSvseVAI0H5W1VStUnAmjlLv8eXZB5rJRW0mcPnLZzEmD772fGpXIvkUnbrw4ZWFWXtQEXD\n",
              "KI9KssRV160XTfpRCC+D/wHy5u/HbezWPMnQKr9clnDjAG2cxMHbdSkyxLsoosMIC7iEBJKmDAdG\n",
              "2q+mdRDpYKp7d8Eer1BI7FJniUNQl3hqlkr7HlOubs8XZCCO1zbo1B8SaeE1z0Rkv+PN4yttxMrW\n",
              "dhtRG5H+n2c5yGa6vQKrL2oCai7DtaCB7/ru/F9a4hekrPOQju3alptya8+wnQKr+VlnDjAG2cxM\n",
              "SfzY54EHZRRYZU9S1t/4gCxPcZkXBOdRDpYN5rg8Eer1BI6CWYYIvVMFlhs/bNKUarNDwWsb0Zbo\n",
              "y0f2lcnr1gFkoFO64cFvQgC/Y3XQoUJdJnLAYMyHFyHoFVl7UBNpdh2tBA9/zQRBg4hgqEGojIhy\n",
              "5u/I63U8+wnQKr+91nDjAG2cxMNi9SkyxLsoosNgrbiEBJKmDDLtkXBOdRDpYR96iqEer1BI80yK\n",
              "wjvVMG885mr7HlQRtyrGqG2w0TYKlNhFoPtJqR5AICUArlf+mFIu4EB92tK1LRHo0AoZWFWXtQGW\n",
              "GUR6VZYjKtDtjdgxC9JWhYiHLm78lpUdY8ydAqv9kccOMAbZzExEvuxzwIOyiiw2is+IQEkqYMCd\n",
              "GRcE51EOlhXqqKoR6vUEj3XsrCO9UwR4Gz9s0pRrOejRgsyeXEmmGhAvJtc3Al5Wq6EHRXyHOKqu\n",
              "AVx+HOw86j9BMynxlWLc1/9JDLsO1oIHv+UpRhB9poEGo3Km8VxtaNez8W1OeLyRlWcCsxME8WsA\n",
              "HdGe6Imho6vCoYj9eAklTBhgpGq+mdRDpYYD8VcEer1BJW/96YIvVMGLepJX2PKecdG5jGnRtB+j\n",
              "HazIRlRZJnUnPngXyq8i4hKBge40E5CLj1FwFY1tPy/IUVoSI+Xcvh9ij0vnfT0bXJR0lPTDYLu3\n",
              "alriziseZOgXGTEM4cYA2zmJjFObHPAg7KKKOah3xCAklTBdxSlLgnOoh0u+J2KuCPV6gk576NME\n",
              "XqmCFT9FJ/grCjMGT/OnafBreh+5lpdwQqEE/ftXpgdkU2d8QfCEuQWrB/SbMNm8J0c3gRr9hIgD\n",
              "mWesNB9Qq6JPIXCIfu5VTd/+uJXUz9WYJcJ0C40iqHDbgG2cxMaDzYtZ9kY48cmVdSyiRyVMNag5\n",
              "sg+JzqFazh5is6+AMC1l5xKqaTpFmIfEU1YcgMtBQq7OJuimqH3SFa5tKG1PqqRDbJmCA/+xRXYA\n",
              "Uckyq/tUfxxPKj2jgRr9hIbXjr/ExTFTSv5nVSgY6+ml0Us9cSuphQMwS4ToFxxwTOG3ANs5iY0b\n",
              "6kv59kY48ZbdlSyiRyVMMLjFdlu97a+VnEq4R18AYFrQhjVU0nSLKz2Ipqw5Ab08iPmi6Sb3dCa6\n",
              "VFImmdlOskHycv8RaySqj3tOkW+6nv6UvIJXfYgaDHL+OT1ngIBd9fdU/A/+xq99+j2BWuCWQUmi\n",
              "oTFkOyaA8IBsKhOOoQqYVs08LmJfo8PGTx6W4FMvG386xHA2uH2O/szQ6yFSkLBy46nrjTHCcATK\n",
              "09C1KDj9kad5uXhw9htBMPvLczjrUQE3Km6ZxMArHDOqk2nh8om1ln7V90kAc8Ctb2VTV/sqwG9b\n",
              "OCUnbEJbRfs74BZwXhafu81govLd/bNzm5rWXNCC3mkLjjr6bYA9dIurAtgP3G2FzFuBTPUIEnf0\n",
              "tdJKnGp/991aWEW1UfTsDPRbXzqL9rqBSh5CATeuprDodOybC34Extwurd+7B7EFEmA8WI1fvLx1\n",
              "RhSAzT0CJrIM2IbQuIDI47o16kS7xSpIKiCODgmcgHYNHuZy0eUDzGbCPeRl1HQPQ/X3GJYt+R56\n",
              "dWJgMpM+5kia3NQOm1eqEAy/ZMcbcp7q+hDHt8MK/O9xkcRRMtpFBQOm8vk9a/1r8mdlxdTVu/vk\n",
              "iQZv2FxjDRVhdDLhCXxeNq0F2K+lKDyLN1dDy0dcvTDcTT1mH/xNUc/Ta34v6CqbtIjrRb+JhqSG\n",
              "QePp9t5JlNcmqrpcemmwWAsCeDfbb0IYSxeDGCpC6jNZE97Ok3hO/mKLg42OVizyDLzD0gGkvMgM\n",
              "komWyDdcHSkTUb6vfzmn9lBE02uptzOQZv2Fvsgj77gMdH7XOJMmOAJ9Sws2gpIVH2NmL8ojBqC9\n",
              "umXWbqhqrT2tyegqm6CfICaD7ubo9x020mzGKELShmwPZzwueJYtKezfbb0IYVp6hjBUhdRmsi40\n",
              "nSbwnfzFFxbOYoCNphz2+H133DrJfB1s/tdidl5v8+gtfj2/H9lBE02upVTX6VSbC31iRCkpAmWv\n",
              "+Y2mY1DrwQYgC6+Nrv6bIjMb2BF/vGJUzXkHxmiq8X9BVNwFyurWQDsGj3TkTj2JSUUlph5h6D8d\n",
              "L9fcRLHbuNVWB3oC+GMHtSJrc1AxfV6oQDL9kyJ2hT1FpMWXmHqlHGwuY4iiZa3iQkXoGT6D//km\n",
              "Kf2XF1NW7+sOp07JsLjASKSE4TBbUThN1iIGGFHKFe0KcfIgNDo4Nf8/Fn76jM6HFGnNyegqm2is\n",
              "ZTgh3qakhyDoQwE1Cx7LdCiEFoUupKdoxru41VYHeNcwYwe1ImtzT+WrOxZQiK4XjwqgCzr6EMe3\n",
              "wwKcfmEBklEy1bEdIvQMn0Hd+3cWnsuLqat39E2qlUmwuL/pyhxmHWCJ0dj7ylCyglyQ1JRRzcvA\n",
              "MtUN/inSC5XcjEMSJq8X9BVNqFkSnBDvU1JDHabiDdErM0jLoq6X6bTYLASJLZvtt6EMJdShjBUh\n",
              "dRmsibeHSbwnfzFFw7dfmVe5hz2+GpUxP7mOIomWv4jpF5v8+gnvb5in9lBE02upa1CdOybC3j3v\n",
              "1LovFGizEV2SKVntQVKjA1SOwCc9CmeA0XSrIAvOh7rfB+L+gqm0TeYJnIB2DR7pWYTDmNS2Ee8j\n",
              "Lpz+Ol+vuGSXN3GqrA7yS0DGD2pE1uaf8VZ2LKERXC8hteQUFfQhj2+GJTj8wgMkomWoYjpF6Bk+\n",
              "g/f27w09lxdTVu/a02QZv2FxPneWLH7Q7csjM5y2TG/zswptdqXr1FABZ9uRjpsq71t0u0Oo/F/Q\n",
              "VTZxyK0W/iYakhj77QFIFzM0jLoq6X6bTYLATJLZvtt6EMJj6BjBUhdRmsiP+HSbwnfzFFyIQoXg\n",
              "s8gy8w8+yYn9zHEUTLXMR0i83+fQTvt8tT+ygiabXUuShOnZNhbygfRhjAaTkTAkv820M94qSEIY\n",
              "E4PMOV+fdm0I7VVG/IOP1odPxf0FU2WbTBM5AOwaPdOzF8EUrKktMPMPOdHpfr7iElzdxqqwO8rt\n",
              "Axg9qRNbmoBFWdiyhEVwvIq6YAqjgIY9vheU4/MIDJKJlqGI6RegZPoP/9Lf91iO0c9XhjHMOnZN\n",
              "hcUKkwNR5pOPGObhp1BnvIpqUzihTzGiZ/m15bF02Vd8igfrRAfi/oKpsg6laLfxMNSQx+9oSsK5\n",
              "maRl0VdM9NpsFgJ0ls3229CGE19AxgqQuozWRJ/DpN4Tv5ii5MoUKfXuYc9vhzZMT+5jiKJlr2I6\n",
              "Reb/PoJ32+Wp/ZQRNNrqXJQnTsmwt5lK8lyw0nHyDtwNBkz3jrLsPpFzzGgT7AIfFTpsq73nQ/Wi\n",
              "M/F/QVTYabHVrIB2DR7p2YvgilZUlph5h5zo9L9fcQkubuNVWB3ldoGMHtSJrc1AIqzsWUIiuF5F\n",
              "XTAHUcBDHt8LbHH5hAZJRMtSxHSL0DJ9B//pb/usR2jnq8MY5h07JsLii2mWoJsHoTLLztQ5M94y\n",
              "wjR8IueYayQTOHjF02VeAUwfrRKfi/oKpsVP8pwQ71NSQ5BwPzvKztIy6KukJC02CwCyTtC/eonN\n",
              "u1JQxgqQuozWRB2DpN4Tv5ii3fIIquG0w57fDezx0yzHEUTLVMR0i83+fQd37dCWuygiabXUk3KO\n",
              "nZNhcUVuYlGjqTkGmVy0eTPeXfblNSaXs5RUwZzzGLpsq78rJEWiWtyegqmxkTmnBDvU1JDj9nIE\n",
              "lzQHst0J06mzTYLAMJMcL96ic26FlDGCpC6jNZEWYOk3hO/mKLbDSmIq9zDnt8Q7WHTLMcRRMtax\n",
              "HSLzf59Bx/t3hxamCJptdSG9k6dk2FxSoLA5Qsen+fwLGfEGe8i91mcXLraicxBDbVXYTiODny+7\n",
              "6z8X9BVNjouNOCHepqSHHyl0KLm4PZboTp5tmmwWAVybIX71E5tzDKGMFSF1GayI/4dJvCd/MUWy\n",
              "O1OCQ2mHPb4i20OmWY4iiZa5iOkXm/z6Da/boTo1METTa6kc+J07JsLim/kgjVVJyKroiHp3DB6/\n",
              "nWFeP9bUhWo3lvT9qqjfpDAfimtyegqmyUYynBDvU1JDj1S6FJzgHst0J082zTYLAKZOML96ic24\n",
              "plDGCpC6jNZEn8Ok3hO/mKLXGSnckbTDnt8S7gHTLMcRRMtexHSLzf59Brft2h6amCJptdSLco6d\n",
              "k2FxUByaRs6+5CmAPcbOBFAe1LxXq3vZyxEvrRvyuwnEcEK3W2phAAAAPkGaJGxCP/3hM9AEAxIj\n",
              "rl07NCoAaxt1Fqq3NEEq6Nw9Y4GqxNbEm8cUVDMNx4MJ4cP2VTNkQ1J5DFU3N1amAAAAD0GeQniL\n",
              "/xT6SAgPD7t5gQAAAAkBnmF0RP8ADlgAAAAMAZ5jakT/CsPWShMDAAAAwkGaaEmoQWiZTAj//IQA\n",
              "Ee36IynSAgI4ABuL6UJ9NbwRc/EMWaagcaEBG4UHa6OSULnpAdu9vve9S7x8uH3M+qgUAzsup/48\n",
              "tOYc3uTg3nk9cfvtvcOEdMJaWKK+62wEQaczbDLB22bjkRR9mo7m8+h2M/QKLumEKX/UfON5X14A\n",
              "yyLz6dk4naRr/+tWf+PFSugq1kIGfCaa/D9HVlUKtYxeW/e5pVehVTkKD/KocPeBZJ4J42Sk0r8J\n",
              "dAevH3XjKWIdAAAAWEGehkURLF8JoJaJyjtNaMAVskXdOTqpywMXHEhCbcZVxmArNCugqR4HKsq0\n",
              "6zmUTHtXSG7yIeVdubtQ/yNo6gvI2v2h6byg30dDlhsUGw4+VYAQSrro1SUAAABNAZ6ldET/Crei\n",
              "iwFDJLd7juACnpK28vD9bvH/0oOTU2tocP+vPTTEB0kPMebQ0at4Nb8kabPC5XOEGVOgrDbM9+SO\n",
              "60Cj7N+NeqoURAcAAABFAZ6nakT/CsPu4hX+4/3cAOKm6yx/rbO+Rk0q7NHzgOZtAdMlWina9JQY\n",
              "DibAlfeLcDOgOp/NdOOSfYCn+RmXomQzwbheAAAAvkGaq0moQWyZTAj//IQAGwGtOeeAFcfKu+f5\n",
              "s5wV2Rq1fUTHf12sXV1Wu8PoIu26un1aie0FO06RTGQ/w/UHTlHQj1byDLS4dmmsUzJyXJw1056d\n",
              "/gBm4gn0xQuVIKJV5jqj6cPaRt7zTj8CI11DSWP/fJJu4yU9CRm/W4tZPNzqWxF0P6vQRzbw7mqA\n",
              "hxzX7do1cVW0/uWQMCedC09NziHPCkv0wqeKaO2jVTIOe/e6G8DAiV4mna8u7j4bxTgAAABBQZ7J\n",
              "RRUsTwrHAwVxDimoNZJKhjwBqtuAca2rPZHHCpfUVonQ3BXGxzX73c68mAGcgHX3rjgphb1V/d8w\n",
              "8Hetj8EAAABAAZ7qakT/CmvwOQOoSotAFVE+P5d9zPuncfVg8cH/6Ejlj9NDr+I6nP+bMOq2l7fC\n",
              "AAswLRfzKHICccHpGqXpgAAAAIlBmuxJqEFsmUwIR//94YSZgApMQ2NuJnU//oqIMJloMkTF2eU5\n",
              "cMC2iEPi9VDmtT7iheM27uBmFYsj6wuYYV6qkOKIquwmsZm84rhOryMrhqVf/1jvK2Av3oupZDyd\n",
              "DDP9dkWOiUsIR6dQBi+U6nnMYRb6jgTeXk+FyOcM1yklhwfIijNuezU4qgAAAFdBmxBJ4QpSZTAh\n",
              "H/3hAAbjj/SyrykAEVbvyaE5kS0zW4ohJuNEYSA7HN+blwyB6VOtetU9vMEtjKfbj+WfSWqERY9o\n",
              "dgrtJbOnHdNFB+7K4ce04D5ef+UAAABQQZ8uRTRMXwmevLjEuqXNVWLvJpkAE5UzWpCsq4kztsUW\n",
              "5kq9+3Hbty0uAtN90GYGMRqq5sm+/yrGZ93j8qZ6LAsV3xQmlwtqh+a+r8PFa4EAAAANAZ9NdET/\n",
              "AGoRDey20wAAABIBn09qRP8AamdNIQEBtiph7zwAAAEpQZtUSahBaJlMCEf//eEGCT+ZaoMxAJF2\n",
              "cm/I/1juDI8X/uqnM6UnbVU2Zn69Zu3zDIa8AdvGEqoWyoLpi9sNNWE1sSg/ef+m/hn7Zff0dAGN\n",
              "jdnYEUQAy/SEyxiE2pSy7ZBt3Yasu/QDOAfm7O2peQkjrHq9TJLo9PqXCaTCPWIHW9+SJyENZ1y2\n",
              "/JvRRQnZPKmTl24ZYl9FiPDnvNlZBmv276MXUqYEnFkRq5gEL8Dr2yeotxFWdsbwnARiL/w5X1ni\n",
              "WUgLXdIoPx/6C1vlWdN2rxCp8yfVIVx5BZsI+1Blu/nqe4rWhR7vLYBVDGz3Oa/S+RrvoCAY2tZ9\n",
              "oq95iGux6emKoAI4NM1QOL9TNFFvPP/Kwljj1eOHOTil8GB12WOZ1oK1d0mAAAAASEGfckURLF8M\n",
              "qPdAwMyzdrjNpUxgEAVWpfeg+pdlMNyjw16xu9SQneOzcmyibt3WNCBb/QJzhAueNl4YsVvu2xXM\n",
              "x5brydgRRQAAACEBn5F0RP8KX6NtkOJfUAKpgdtPGAciGjg3kVlFnmFPc+AAAAA/AZ+TakT/DeQT\n",
              "jeM0nyu05QBW0z4mkWHT5eVyAedDN6tpa0atJlcRtkLX+7AHIQAluY3/gVNjQtXQnGSjzIgQAAAA\n",
              "tUGbmEmoQWyZTAj//IQAEdL4N1GoBQbGORD/C9o3BY3/tuJ6w1LAkVJms9+CfnXckcojwjUsR4FV\n",
              "nUK8yvfAiAPs+eHt9UoiaedKbGM6d+GmdUvsRVaxoFKhRULTYZY0j56kZ0epfc/nrcHAC3U6h3hh\n",
              "6Ol1fO9CzrscCtYMyjjHHYxhl8mDuTUMmWdeu8SQPmGuOSu0wIEL2pXEqltdf0s4W6rJ+hDVEqPQ\n",
              "BLcjQmjIJ26u7vEAAABmQZ+2RRUsXwmglc3I2nrDaE54A3wK4peADgYMvJaP5uf/e2OsNc1RASxm\n",
              "a7RyFbj2u6GfygMQkf6s6HQGmtX4d/n4TtlzkepGS/Vq4ngNTyaxF5eFYdapwu53Vk8b3mE9xHkF\n",
              "abVSAAAARwGf1XRE/whANleQP6ao7kZW4AcdOrFO4iujZt+fcO1u7rN4QC3WCb97eXVT/4RWGL+/\n",
              "zwApYkr0jpRxDZc9MUrwY2uLAgWBAAAAQAGf12pE/whHcteB2qlEG4Av6aBtYcvyZyFpuL3Yb37e\n",
              "mg7Hs+tmanJDAtypWen4/qQfP7ku6yXC//9wS/FhDRUAAABkQZvcSahBbJlMCN/6WLylgADQ4ooX\n",
              "yRCu7P04anleUJwT3veT90417rDGzVL61DQoPin4RLxRmV4vO3pTkRG6+urFOz/80vxemTgK0fti\n",
              "q3H840j0vhETgzldw0y6ZPle11jfCwAAAERBn/pFFSxfE4rpSc3ALmbj+5d95WsQ9OprAC45TZEw\n",
              "08s60KErMfD6OMdnpol9VbVCEq83np7fkLq4zFPCbVuyQyHuJQAAAD0Bnhl0RP8VbYtgiWxWPv9t\n",
              "FB7Aqu/QBDxMHD8OxLi3eEFS1c8UGMGRTvrT4yX6/U3fPGAsliTVs9kkIEF0AAAAEgGeG2pE/wps\n",
              "AfCkeFQxwcUxYQAAAGFBmgBJqEFsmUwIn/MgAFYWbrQBEsgG9tHiUXFeUBYqobC7jUoSCeA0r6bO\n",
              "KByKcKUjDF2nz9f8NeRWHZfdK/0aQFwCHiBZl5TOuPMpNIirjheC+/Vy5hBi6uybvQJ6E/2hAAAA\n",
              "HkGePkUVLF8JoJXOCdBhtDHIxIN7kY8IAg3BRabY1gAAACIBnl10RP8IQDZYYsBqjFJbCQBBwdSw\n",
              "mB2cVO4oXdiWr+CgAAAASAGeX2pE/whHctiwol/+gCNUISzuBG6ow5KYMvjxjDYhf1nQqum3Qv56\n",
              "m2qQh6sf3ZdkTh2Uf0BIRhkisfdWVWiQp5QpWyg3GwAABJVtb292AAAAbG12aGQAAAAAAAAAAAAA\n",
              "AAAAAAPoAAACJgABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAA\n",
              "AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAADv3RyYWsAAABcdGtoZAAAAAMAAAAA\n",
              "AAAAAAAAAAEAAAAAAAACJgAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAA\n",
              "AAAAAAAAAEAAAAABAAAAAPAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAAiYAAAIAAAEAAAAA\n",
              "AzdtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADwAAAAhAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlk\n",
              "ZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAALibWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAA\n",
              "JGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAConN0YmwAAACWc3RzZAAAAAAAAAAB\n",
              "AAAAhmF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAABAADwAEgAAABIAAAAAAAAAAEAAAAAAAAA\n",
              "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAwYXZjQwFkABX/4QAXZ2QAFazZQQH6EAAA\n",
              "AwAQAAAHgPFi2WABAAZo6+PLIsAAAAAYc3R0cwAAAAAAAAABAAAAIQAAAQAAAAAUc3RzcwAAAAAA\n",
              "AAABAAAAAQAAARBjdHRzAAAAAAAAACAAAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAA\n",
              "AAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAA\n",
              "AQAAAgAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAAB\n",
              "AAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEA\n",
              "AAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAHHN0\n",
              "c2MAAAAAAAAAAQAAAAEAAAAhAAAAAQAAAJhzdHN6AAAAAAAAAAAAAAAhAAAdzQAAAEIAAAATAAAA\n",
              "DQAAABAAAADGAAAAXAAAAFEAAABJAAAAwgAAAEUAAABEAAAAjQAAAFsAAABUAAAAEQAAABYAAAEt\n",
              "AAAATAAAACUAAABDAAAAuQAAAGoAAABLAAAARAAAAGgAAABIAAAAQQAAABYAAABlAAAAIgAAACYA\n",
              "AABMAAAAFHN0Y28AAAAAAAAAAQAAADAAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAA\n",
              "AABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU3\n",
              "LjcyLjEwMQ==\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# # env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "# device='cpu'\n",
        "# model = ActorCritic(env.observation_space.shape, env.action_space)#.to(device)\n",
        "# model.load_state_dict(shared_model.state_dict())\n",
        "# model.eval()\n",
        "# latent = None\n",
        "# torch.manual_seed(6)\n",
        "x=0\n",
        "\n",
        "acts=[11, 11, 8, 7, 5, 1, 6, 8, 4, 0, 3, 3, 3, 8, 4, 6, 2, 7, 3, 6, 1, 11, 8, 0, 6, 3, 8, 10, 5, 1, 5, 2, 8, 10, 1, 2, 1, 4, 4, 1, 4, 1, 5, 7, 8, 4, 6, 4, 1, 6, 10, 1, 11, 6, 6, 5, 0, 0, 1, 6, 9, 7, 4, 3, 1, 7, 8, 0, 11, 11, 2, 0, 6, 5, 1, 10, 6, 4, 5, 3, 8, 1, 4, 1, 5, 6, 5, 4, 6, 10, 11, 0, 4, 1, 2, 6, 6, 3, 1, 10, 8, 8, 5, 8, 0, 10, 6, 6, 8, 0, 2, 10, 10, 8, 3, 8, 6, 4, 0, 4, 10, 6, 1, 4, 8, 10, 4, 8, 3, 8, 5, 7, 6, 0, 0, 10, 4, 5, 6, 0, 1, 10, 8, 8, 0, 6, 0, 2, 6, 1, 0, 6, 5, 0, 2, 0, 8, 7, 8, 2, 1, 7, 5, 10, 8, 4, 8, 5, 1, 6, 2, 4, 8, 4, 6, 10, 6, 5, 6, 5, 1, 0, 0, 0, 8, 1, 5, 1, 0, 8, 4, 2, 4, 8, 2, 6, 8, 1, 6, 5, 1, 2, 1, 8, 0, 4, 10, 1, 4, 5, 7, 1, 2, 1, 5, 6, 4, 6, 3, 1, 8, 1, 1, 8, 5, 9, 9, 0, 8, 8, 8, 1, 4, 6, 2, 0, 10, 10, 3, 10, 7, 11, 6, 4, 1, 8, 8, 5, 0, 1, 11, 8, 3, 11, 3, 8, 3, 1, 6, 0, 6, 5, 8, 8, 3, 2, 0, 7, 4, 2, 8, 4, 8, 8, 3, 8, 10, 1, 10, 1, 0, 1, 4, 8, 1, 8, 9, 4, 9, 4, 10, 5, 8, 1, 7, 6, 1, 3, 1, 2, 2, 8, 5, 1, 8, 5, 1, 8, 0, 1, 4, 8, 6, 8, 0, 6, 6, 11, 7, 4, 8, 8, 6, 8, 0, 0, 1, 7, 8, 8, 10, 10, 10, 8, 0, 5, 6, 5, 1, 0, 6, 7, 8, 6, 9, 8, 8, 1, 6, 10, 8, 4, 11, 8, 11, 8, 9, 6, 8, 4, 10, 4, 5, 10, 11, 1, 8, 4, 0, 6, 7, 10, 4, 0, 11, 8, 6, 1, 6, 10, 7, 0, 1, 2, 1, 8, 10, 8, 6, 8, 5, 2, 6, 0, 0, 1, 1, 3, 6, 0, 1, 8, 1, 6, 1, 7, 6, 6, 4, 5, 0, 10, 5, 8, 8, 2, 1, 7, 9, 8, 8, 7, 8, 8, 11, 1, 5, 10, 6, 10, 10, 10, 8, 1, 4, 1, 8, 1, 8, 3, 8, 10, 10, 1, 4, 6, 1, 5, 3, 5, 9, 5, 1, 3, 8, 1, 6, 2, 1, 8, 9, 5, 2, 6, 2, 0, 8, 8, 9, 2, 6, 10, 6, 6, 6, 1, 7, 6, 5, 10, 9, 0, 3, 7, 6, 7, 2, 0, 7, 0, 6, 3, 6, 6, 8, 1, 11, 10, 5, 1, 4, 8, 6, 3, 7, 6, 6, 5, 6, 4, 3, 5, 3, 4, 10, 8, 6, 6, 6, 6, 4, 1, 6, 3, 3, 1, 6, 0, 2, 4, 0, 0, 8, 6, 6, 1, 10, 7, 0, 6, 1, 5, 1, 6, 1, 4, 10, 3, 4, 5, 8, 7, 1, 0, 6, 7, 5, 10, 3, 9, 11, 1, 4, 5, 5, 1, 1, 1, 1, 10, 8, 0, 0, 6, 8, 1, 0, 6, 1, 4, 3, 6, 1, 7, 6, 5, 2, 3, 0, 10, 4, 3, 6, 6, 6, 1, 0, 1, 3, 6, 8, 5, 7, 1, 8, 9, 4, 4, 3, 6, 1, 8, 8, 6, 6, 0, 6, 10, 1, 6, 7, 3, 10, 0, 7, 6, 1, 6, 2, 4, 6, 10, 8, 6, 0, 4, 7, 10, 5, 0, 0, 2, 0, 5, 6, 8, 1, 6, 0, 8, 8, 11, 8, 5, 6, 10, 4, 8, 7, 2, 6, 8, 5, 0, 4, 2, 6, 10, 1, 8, 0, 4, 10, 7, 0, 3, 6, 1, 7, 8, 11, 1, 2, 3, 4, 6, 8, 0, 5, 0, 6, 8, 2, 6, 8, 6, 8, 6, 1, 3, 8, 9, 1, 4, 4, 6, 6, 1, 5, 4, 5, 8, 5, 6, 8, 4, 1, 0, 0, 5, 0, 1, 0, 3, 6, 2, 3, 8, 6, 8, 7, 4, 9, 0, 10, 4, 8, 9, 0, 8, 8, 6, 6, 1, 3, 5, 10, 0, 0, 3, 3, 3, 6, 10, 4, 8, 1, 1, 10, 8, 3, 6, 1, 1, 1, 1, 6, 1, 3, 0, 6, 8, 3, 3, 9, 6, 11, 8, 6, 4, 6, 1, 6, 0, 5, 3, 7, 5, 3, 2, 0, 4, 8, 6, 6, 0, 6, 2, 10, 1, 8, 0, 4, 4, 7, 8, 0, 6, 2, 1, 2, 6, 0, 8]\n",
        "\n",
        "\n",
        "# acts=[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "\n",
        "while True:\n",
        "    # state = torch.from_numpy(state.copy()).type(torch.float)#.to(device)\n",
        "    # value, logit, latent = model((state, latent), icm = False)\n",
        "    # prob = F.softmax(logit, dim=1) #from train\n",
        "    # action = prob.multinomial(1).data\n",
        "    # state, reward, done, _ = env.step(action.item())\n",
        "    try:\n",
        "        action=int(acts[x])\n",
        "    except:\n",
        "        action = 10\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "    x+=1\n",
        "    if done: break\n",
        "env.play()\n",
        "print(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated(device)\n",
        "torch.cuda.memory_stats(device)\n"
      ],
      "metadata": {
        "id": "On5etZ_H-uLr",
        "outputId": "5d8f9103-da99-4334-9d6e-1ad7f1aefaa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('active.all.allocated', 2713746),\n",
              "             ('active.all.current', 19278),\n",
              "             ('active.all.freed', 2694468),\n",
              "             ('active.all.peak', 155933),\n",
              "             ('active.large_pool.allocated', 20559),\n",
              "             ('active.large_pool.current', 13),\n",
              "             ('active.large_pool.freed', 20546),\n",
              "             ('active.large_pool.peak', 17),\n",
              "             ('active.small_pool.allocated', 2693187),\n",
              "             ('active.small_pool.current', 19265),\n",
              "             ('active.small_pool.freed', 2673922),\n",
              "             ('active.small_pool.peak', 155918),\n",
              "             ('active_bytes.all.allocated', 234375086592),\n",
              "             ('active_bytes.all.current', 2039463424),\n",
              "             ('active_bytes.all.freed', 232335623168),\n",
              "             ('active_bytes.all.peak', 12306110976),\n",
              "             ('active_bytes.large_pool.allocated', 65927118848),\n",
              "             ('active_bytes.large_pool.current', 545259520),\n",
              "             ('active_bytes.large_pool.freed', 65381859328),\n",
              "             ('active_bytes.large_pool.peak', 559939584),\n",
              "             ('active_bytes.small_pool.allocated', 168447967744),\n",
              "             ('active_bytes.small_pool.current', 1494203904),\n",
              "             ('active_bytes.small_pool.freed', 166953763840),\n",
              "             ('active_bytes.small_pool.peak', 11758855168),\n",
              "             ('allocated_bytes.all.allocated', 234375086592),\n",
              "             ('allocated_bytes.all.current', 2039463424),\n",
              "             ('allocated_bytes.all.freed', 232335623168),\n",
              "             ('allocated_bytes.all.peak', 12306110976),\n",
              "             ('allocated_bytes.large_pool.allocated', 65927118848),\n",
              "             ('allocated_bytes.large_pool.current', 545259520),\n",
              "             ('allocated_bytes.large_pool.freed', 65381859328),\n",
              "             ('allocated_bytes.large_pool.peak', 559939584),\n",
              "             ('allocated_bytes.small_pool.allocated', 168447967744),\n",
              "             ('allocated_bytes.small_pool.current', 1494203904),\n",
              "             ('allocated_bytes.small_pool.freed', 166953763840),\n",
              "             ('allocated_bytes.small_pool.peak', 11758855168),\n",
              "             ('allocation.all.allocated', 2713746),\n",
              "             ('allocation.all.current', 19278),\n",
              "             ('allocation.all.freed', 2694468),\n",
              "             ('allocation.all.peak', 155933),\n",
              "             ('allocation.large_pool.allocated', 20559),\n",
              "             ('allocation.large_pool.current', 13),\n",
              "             ('allocation.large_pool.freed', 20546),\n",
              "             ('allocation.large_pool.peak', 17),\n",
              "             ('allocation.small_pool.allocated', 2693187),\n",
              "             ('allocation.small_pool.current', 19265),\n",
              "             ('allocation.small_pool.freed', 2673922),\n",
              "             ('allocation.small_pool.peak', 155918),\n",
              "             ('inactive_split.all.allocated', 1728096),\n",
              "             ('inactive_split.all.current', 3),\n",
              "             ('inactive_split.all.freed', 1728093),\n",
              "             ('inactive_split.all.peak', 229),\n",
              "             ('inactive_split.large_pool.allocated', 7112),\n",
              "             ('inactive_split.large_pool.current', 0),\n",
              "             ('inactive_split.large_pool.freed', 7112),\n",
              "             ('inactive_split.large_pool.peak', 4),\n",
              "             ('inactive_split.small_pool.allocated', 1720984),\n",
              "             ('inactive_split.small_pool.current', 3),\n",
              "             ('inactive_split.small_pool.freed', 1720981),\n",
              "             ('inactive_split.small_pool.peak', 227),\n",
              "             ('inactive_split_bytes.all.allocated', 268114947584),\n",
              "             ('inactive_split_bytes.all.current', 1065472),\n",
              "             ('inactive_split_bytes.all.freed', 268113882112),\n",
              "             ('inactive_split_bytes.all.peak', 200965120),\n",
              "             ('inactive_split_bytes.large_pool.allocated', 65577156608),\n",
              "             ('inactive_split_bytes.large_pool.current', 0),\n",
              "             ('inactive_split_bytes.large_pool.freed', 65577156608),\n",
              "             ('inactive_split_bytes.large_pool.peak', 19922944),\n",
              "             ('inactive_split_bytes.small_pool.allocated', 202537790976),\n",
              "             ('inactive_split_bytes.small_pool.current', 1065472),\n",
              "             ('inactive_split_bytes.small_pool.freed', 202536725504),\n",
              "             ('inactive_split_bytes.small_pool.peak', 186285056),\n",
              "             ('max_split_size', -1),\n",
              "             ('num_alloc_retries', 0),\n",
              "             ('num_ooms', 0),\n",
              "             ('oversize_allocations.allocated', 0),\n",
              "             ('oversize_allocations.current', 0),\n",
              "             ('oversize_allocations.freed', 0),\n",
              "             ('oversize_allocations.peak', 0),\n",
              "             ('oversize_segments.allocated', 0),\n",
              "             ('oversize_segments.current', 0),\n",
              "             ('oversize_segments.freed', 0),\n",
              "             ('oversize_segments.peak', 0),\n",
              "             ('reserved_bytes.all.allocated', 12327059456),\n",
              "             ('reserved_bytes.all.current', 12327059456),\n",
              "             ('reserved_bytes.all.freed', 0),\n",
              "             ('reserved_bytes.all.peak', 12327059456),\n",
              "             ('reserved_bytes.large_pool.allocated', 566231040),\n",
              "             ('reserved_bytes.large_pool.current', 566231040),\n",
              "             ('reserved_bytes.large_pool.freed', 0),\n",
              "             ('reserved_bytes.large_pool.peak', 566231040),\n",
              "             ('reserved_bytes.small_pool.allocated', 11760828416),\n",
              "             ('reserved_bytes.small_pool.current', 11760828416),\n",
              "             ('reserved_bytes.small_pool.freed', 0),\n",
              "             ('reserved_bytes.small_pool.peak', 11760828416),\n",
              "             ('segment.all.allocated', 5612),\n",
              "             ('segment.all.current', 5612),\n",
              "             ('segment.all.freed', 0),\n",
              "             ('segment.all.peak', 5612),\n",
              "             ('segment.large_pool.allocated', 4),\n",
              "             ('segment.large_pool.current', 4),\n",
              "             ('segment.large_pool.freed', 0),\n",
              "             ('segment.large_pool.peak', 4),\n",
              "             ('segment.small_pool.allocated', 5608),\n",
              "             ('segment.small_pool.current', 5608),\n",
              "             ('segment.small_pool.freed', 0),\n",
              "             ('segment.small_pool.peak', 5608)])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del shared_model"
      ],
      "metadata": {
        "id": "IBF9frzpqrRX"
      },
      "execution_count": 70,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-qQjuJIepAvA",
        "FidkEuaA8HvK",
        "SPCCve3p2bL-",
        "sbpPda4YEv13",
        "GCHpcDteZdLS",
        "fj3tv7XHZmD9",
        "wFrRKvOwhYM_",
        "KlV2MvSK-aL_"
      ],
      "name": "curiousity_perceiverio.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
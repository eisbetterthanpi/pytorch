{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/curiousity_perceiverio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQjuJIepAvA"
      },
      "source": [
        "#### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C1GD7lk8H13h",
        "outputId": "47736895-6924-43cd-b61d-523ed4364b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting gym\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 4.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.4)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793153 sha256=a7b8ac64bf1e9a9633da0eb65f2101a12194224cd497e0316bde7252ddb87a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0e/54/63d9f3d16ddf0fec1622e90d28140df5e6016bcf8ea920037d\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.24.1 gym-notices-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=7a5fe3396f60daddabc8a952cb8054ab02b74109b98a8843e5391086fc6c7d35\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting nes-py\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (4.11.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (0.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (3.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp37-cp37m-linux_x86_64.whl size=438149 sha256=e8031cb873a31583c7e5f234530efa0caa3d5108398ba2838b2f4696dae7cf63\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/96/0e/22a8c7dbdf412d8e988286f223b223baf0f4ad90c9e699c56d\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.64.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3132 sha256=b71ba37dc9012e88b42fc607aa61f1ee7195fbadd9c2ceae872b21081cd91b52\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/0a/2a/86955ea711b461ab7918236fed2568733f75ed677d0524b56c\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting perceiver-pytorch\n",
            "  Downloading perceiver_pytorch-0.8.3-py3-none-any.whl (12 kB)\n",
            "Collecting einops>=0.3\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from perceiver-pytorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->perceiver-pytorch) (4.1.1)\n",
            "Installing collected packages: einops, perceiver-pytorch\n",
            "Successfully installed einops-0.4.1 perceiver-pytorch-0.8.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# # https://github.com/kimhc6028/pytorch-noreward-rl\n",
        "# https://stackoverflow.com/questions/67808779/running-gym-atari-in-google-colab\n",
        "%pip install -U gym\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "# import gym\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "!pip install colabgymrender\n",
        "!pip install perceiver-pytorch\n",
        "\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            self.total_rewards = 0\n",
        "            return observation, 0, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        # print(\"MarioSparse\",life,score)\n",
        "        # if done: return observation, self.total_rewards, done, info\n",
        "        if life<2: return observation, score, True, info # lost one life, end env\n",
        "        else:\n",
        "            # self.total_score = 0\n",
        "            return observation, score, False, info\n",
        "    def reset(self):\n",
        "        # self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "log=False\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.login() # \n",
        "# wandb.init(project=\"curiousity_simple\", entity=\"bobdole\")\n",
        "# log=True\n",
        "\n",
        "!pip install einops\n",
        "from math import pi, log\n",
        "from functools import wraps\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# functions"
      ],
      "metadata": {
        "id": "uYNDFUK_cp1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### helpers"
      ],
      "metadata": {
        "id": "ncK_m9R1Pxsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OvAaHBGgK2qY"
      },
      "outputs": [],
      "source": [
        "# helpers\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cache_fn(f):\n",
        "    cache = None\n",
        "    # @wraps(f)\n",
        "    def cached_fn(*args, _cache = True, **kwargs):\n",
        "        if not _cache:\n",
        "            return f(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = f(*args, **kwargs)\n",
        "        return cache\n",
        "    return cached_fn\n",
        "\n",
        "# helper classes\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        if exists(self.norm_context):\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context = normed_context)\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None):\n",
        "        h = self.heads\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIO"
      ],
      "metadata": {
        "id": "DsVq7W_oPzzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PerceiverIO class save\n",
        "class PerceiverIO(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        dim,\n",
        "        queries_dim,\n",
        "        logits_dim = None,\n",
        "        num_latents = 512,\n",
        "        latent_dim = 512,\n",
        "        cross_heads = 1,\n",
        "        latent_heads = 8,\n",
        "        cross_dim_head = 64,\n",
        "        latent_dim_head = 64,\n",
        "        weight_tie_layers = False,\n",
        "        decoder_ff = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([get_latent_attn(**cache_args), get_latent_ff(**cache_args)]))\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
        "\n",
        "    def forward(self, data, mask = None, queries = None):\n",
        "        b, *_, device = *data.shape, data.device\n",
        "        x = repeat(self.latents, 'n d -> b n d', b = b)\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        # cross attention only happens once for Perceiver IO\n",
        "        x = cross_attn(x, context = data, mask = mask) + x\n",
        "        x = cross_ff(x) + x\n",
        "        # layers\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "        if not exists(queries):\n",
        "            return x\n",
        "        # make sure queries contains batch dimension\n",
        "        if queries.ndim == 2:\n",
        "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
        "        # cross attend from decoder queries to latents\n",
        "        latents = self.decoder_cross_attn(queries, context = x)\n",
        "        if exists(self.decoder_ff):\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "        return self.to_logits(latents)\n",
        "\n",
        "def preprocess(X):\n",
        "    if X.dim()==1:\n",
        "        X=X.unsqueeze(dim=0)\n",
        "    X=X.flatten(start_dim=1, end_dim=-1) #(start_dim=1)\n",
        "    X=X.unsqueeze(dim=1)\n",
        "    return X\n",
        "\n",
        "def postprocess(logits):\n",
        "    if logits.dim()==3:\n",
        "        logits=logits.squeeze(dim=1)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "6_r7WRKMLcc2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PerceiverIOrnn"
      ],
      "metadata": {
        "id": "hqxWCz07-Yv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PerceiverIOrnn(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        dim,\n",
        "        queries_dim,\n",
        "        logits_dim = None,\n",
        "        num_latents = 512,\n",
        "        latent_dim = 512,\n",
        "        cross_heads = 1,\n",
        "        latent_heads = 8,\n",
        "        cross_dim_head = 64,\n",
        "        latent_dim_head = 64,\n",
        "        weight_tie_layers = False,\n",
        "        decoder_ff = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "        self.latents = torch.zeros(num_latents, latent_dim)\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([get_latent_attn(**cache_args), get_latent_ff(**cache_args)]))\n",
        "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
        "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
        "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
        "\n",
        "    def forward(self, data, mask = None, queries = None, x = None):\n",
        "        b, *_, device = *data.shape, data.device\n",
        "        if x == None: x = repeat(self.latents, 'n d -> b n d', b = b).to(device)\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "        # cross attention only happens once for Perceiver IO\n",
        "        x = cross_attn(x, context = data, mask = mask) + x\n",
        "        x = cross_ff(x) + x\n",
        "        # layers\n",
        "        for self_attn, self_ff in self.layers:\n",
        "            x = self_attn(x) + x\n",
        "            x = self_ff(x) + x\n",
        "        if not exists(queries):\n",
        "            return x\n",
        "        # make sure queries contains batch dimension\n",
        "        if queries.ndim == 2:\n",
        "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
        "        # cross attend from decoder queries to latents\n",
        "        latents = self.decoder_cross_attn(queries, context = x)\n",
        "        if exists(self.decoder_ff):\n",
        "            latents = latents + self.decoder_ff(latents)\n",
        "        # return self.to_logits(latents)\n",
        "        return x, self.to_logits(latents)\n"
      ],
      "metadata": {
        "id": "i8Lh8uAVLixh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### cnn"
      ],
      "metadata": {
        "id": "ZWHdTK3-efrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Conv(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.Conv2d(in_channels, 1, 5, stride=2, padding=3),\n",
        "            nn.Conv2d(in_channels, 1, 5, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.MaxPool2d(3, stride=2, padding=0),\n",
        "            # nn.MaxPool2d(5, stride=2, padding=1),\n",
        "        )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        x = self.conv(x)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [4, 1, 56, 56]\n"
      ],
      "metadata": {
        "id": "a7L9VGgY6NNY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### perceiverio + fourier"
      ],
      "metadata": {
        "id": "lErlrfaRpQmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from einops import rearrange, repeat\n",
        "import numpy as np\n",
        "num_freq_bands = 6 # num_bands = 4\n",
        "max_freq = 10\n",
        "# freq_base = 2,\n",
        "\n",
        "\n",
        "input_axis = 2 # 2 for images, 3 for video\n",
        "input_channels = 3\n",
        "# fourier_encode_data True\n",
        "fourier_channels = (input_axis * ((num_freq_bands * 2) + 1)) # 26\n",
        "input_dim = fourier_channels + input_channels # 29\n",
        "# print(\"input_dim\",input_dim)\n",
        "\n",
        "\n",
        "def fourier_encode(x, max_freq = 10, num_bands = 6):\n",
        "    x = x.unsqueeze(-1)\n",
        "    device, dtype, orig_x = x.device, x.dtype, x\n",
        "    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)\n",
        "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
        "    x = x * scales * np.pi\n",
        "    x = torch.cat([x.sin(), x.cos()], dim = -1)\n",
        "    x = torch.cat((x, orig_x), dim = -1)\n",
        "    return x\n",
        "\n",
        "def fourier(data): # https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py\n",
        "    b, *axis, _, device, dtype = *data.shape, data.device, data.dtype\n",
        "    axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))\n",
        "    pos = torch.stack(torch.meshgrid(*axis_pos, indexing = 'ij'), dim = -1) # [32, 32, 2]\n",
        "    # print(\"fourier pos\",pos.shape)\n",
        "    enc_pos = fourier_encode(pos, max_freq, num_freq_bands) # [32, 32, 2, 13]\n",
        "    # print(\"fourier fourier_encode\",enc_pos.shape)\n",
        "    enc_pos = rearrange(enc_pos, '... n d -> ... (n d)') # [32, 32, 26]\n",
        "    # print(\"fourier enc_pos rearrange\",enc_pos.shape)\n",
        "    enc_pos = repeat(enc_pos, '... -> b ...', b = b) # [4, 32, 32, 26]\n",
        "    # print(\"fourier enc_pos\",enc_pos.shape)\n",
        "    data = torch.cat((data, enc_pos), dim = -1) # [4, 32, 32, 29]\n",
        "    # print(\"fourier cat\",data.shape)\n",
        "    data = rearrange(data, 'b ... d -> b (...) d') # [4, 1024, 29]\n",
        "    # print(\"fourier rearrange\",data.shape)\n",
        "    return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHwWgPmYH1JH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 224 -> CNN -> 32\n",
        "# images = torch.randn(4, 3, 32, 32, device=device) # batch, rgb, dim_x, dim_y\n",
        "# seq = np.transpose(images, (0, 2, 3, 1)) # [4, 32, 32, 3] batch, dim_x, dim_y, rgb\n",
        "\n",
        "images = torch.randn(240, 256, 3, device=device) # dim_x, dim_y, rgb\n",
        "seq = fourier(images) # [4, 1024, 29] [240, 256, 16]\n",
        "print(seq.shape)\n",
        "# batch,_,h,w= seq.shape\n",
        "# batch,axis,input_dim=seq.shape # [240, 256, 16]\n",
        "h,w,input_dim=seq.shape # [240, 256, 16]\n",
        "\n",
        "model = PerceiverIO(\n",
        "    dim = h*w*input_dim,         # 32*32 dimension of sequence to be encoded\n",
        "    queries_dim = 10,            # dimension of decoder queries\n",
        "    logits_dim = None,           # dimension of final logits\n",
        "    depth = 6,                   # depth of net\n",
        "    num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 128,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        ").to(device)\n",
        "\n",
        "seq = seq.flatten()\n",
        "seq = preprocess(seq) # [4, 1, 29696]\n",
        "# print(seq.shape)\n",
        "queries = torch.randn(1, 10, device=device)\n",
        "logits = model(seq, queries = queries)\n",
        "pred = postprocess(logits)\n",
        "# print(pred.shape)\n",
        "# print(pred)\n",
        "pred_probab = nn.Softmax(dim=1)(pred)\n",
        "outputs = pred_probab\n",
        "\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "id": "g9egXpRiwdkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model simplier"
      ],
      "metadata": {
        "id": "TDFTp2wVc-5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGpJGeDM8HvU"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/model.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, in_shape, action_space):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.in_dim = in_shape # mario (240, 256)\n",
        "        self.conv = nn.Sequential( # embed pi\n",
        "            nn.Conv2d(in_shape[0], 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(), # added for RuntimeError: Input batch size 2 doesn't match hidden0 batch size 1\n",
        "            )\n",
        "        self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        num_outputs = action_space.n\n",
        "        self.critic_linear = nn.Linear(256, 1) # -> value\n",
        "        self.actor_linear = nn.Linear(256, num_outputs) # -> action\n",
        "\n",
        "        self.inv_lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        self.fwd_lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        self.inv_linear = nn.Sequential( # inv learning, predict at\n",
        "            nn.Linear(in_shape[1] + in_shape[1], 256), nn.ReLU(),\n",
        "            nn.Linear(256, num_outputs), nn.Softmax()\n",
        "            ) # cat(phi(st), phi(st+1)) -> athat\n",
        "        self.fwd_linear = nn.Sequential( # predict phi st+1\n",
        "            nn.Linear(in_shape[1] + num_outputs, 256), nn.ReLU(),\n",
        "            nn.Linear(256, in_shape[1])\n",
        "            ) # cat(phi(st), at) -> phihat(st+1)\n",
        "\n",
        "    def forward(self, inputs, icm):\n",
        "        if icm == False: #A3C\n",
        "            st, (a3c_hx, a3c_cx) = inputs # [1, 210, 160, 3], ([1, 256], [1, 256])\n",
        "            vec_st = self.conv(st).view(-1, self.in_dim[1])\n",
        "            a3c_hx1, a3c_cx1 = self.lstm(vec_st, (a3c_hx, a3c_cx))\n",
        "            critic = self.critic_linear(a3c_hx1)\n",
        "            actor = self.actor_linear(a3c_hx1)\n",
        "            # print(\"forward A3C \",critic.shape, actor.shape, a3c_hx.shape, a3c_cx.shape)\n",
        "            return critic, actor, (a3c_hx1, a3c_cx1) # [1, 1], [1, 18], ([1, 256], [1, 256])\n",
        "\n",
        "        else: #icm\n",
        "            (inv_hx, inv_cx), (fwd_hx, fwd_cx), st1, at = inputs\n",
        "            vec_st1 = self.conv(st1).view(-1, self.in_dim[1])\n",
        "            inv_hx1, inv_cx1 = self.inv_lstm(vec_st1, (icm_hx, icm_cx)) # inv model\n",
        "            fwd_hx1, fwd_cx1 = self.fwd_lstm(vec_st1, (icm_hx, icm_cx)) # world model\n",
        "\n",
        "            inv_vec = torch.cat((icm_hx, vec_st1), 1) # predict at\n",
        "            fwd_vec = torch.cat((icm_hx, at), 1) # predict vec_st1\n",
        "            inverse = self.inv_linear(inv_vec)\n",
        "            forward = self.fwd_linear(fwd_vec)\n",
        "            # print(\"forward icm \",vec_st1.shape, inverse.shape, forward.shape)\n",
        "            return vec_st1, inverse, forward, (inv_hx1, inv_cx1), (fwd_hx1, fwd_cx1) # [1, 320], [1, 18], [1, 320], ()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# curiousity perceiverio"
      ],
      "metadata": {
        "id": "KxNSVT9rurZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, in_shape, action_space):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.in_dim = in_shape # mario (240, 256, 3)\n",
        "        self.conv = Conv(in_shape[2]).to(device) # embed pi\n",
        "        # self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        # print(in_shape[0]*in_shape[1]/4)\n",
        "        self.encoder = PerceiverIO( # conv -> PerceiverIO ;encodes state for everyone\n",
        "            dim = int(in_shape[0]*in_shape[1]/4),# dimension of sequence to be encoded\n",
        "            queries_dim = 256,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 2,                   # depth of net\n",
        "            num_latents = 64,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 64,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # st(240*256) -zeros> phist(256)\n",
        "        self.encoder_query = torch.zeros(1, 256, device=device)\n",
        "        num_outputs = action_space.n\n",
        "        self.lstm = PerceiverIOrnn( # latent + phist1(256) -zeros> latent1 + vec_st(256)\n",
        "            dim = 256,                  # dimension of sequence to be encoded\n",
        "            queries_dim = 256,            # dimension of decoder queries\n",
        "            logits_dim = None,            # dimension of final logits\n",
        "            depth = 2,                   # depth of net\n",
        "            num_latents = 64,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 64,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # st-> phist ; 240*256 -zeros> 256\n",
        "        self.lstm_query = torch.zeros(1, 256, device=device)\n",
        "        self.actor_linear = nn.Linear(256, num_outputs) # vec_st -> action\n",
        "        self.critic_linear = nn.Linear(256, 1) # vec_st -> value\n",
        "\n",
        "        self.inv_lstm = PerceiverIOrnn( # inverse model, predict taken action\n",
        "            dim = 256,                   # dimension of sequence to be encoded\n",
        "            queries_dim = num_outputs,   # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 64,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 64,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 32,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 32,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # inv_latent + phi(st+1) -zeros> at +inv_latent1\n",
        "        self.inv_query = torch.zeros(1, num_outputs, device=device)\n",
        "        self.fwd_lstm = PerceiverIOrnn( # world model\n",
        "            dim = 256 + num_outputs,     # dimension of sequence to be encoded\n",
        "            queries_dim = 256,           # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 128,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        ).to(device) # fwd_latent + phi(st) cat at -zeros> phi(st1) +fwd_latent1\n",
        "        self.fwd_query = torch.zeros(1, 256, device=device)\n",
        "\n",
        "    def encode(self, st):\n",
        "        st = torch.transpose(st, 1,2)\n",
        "        st = torch.transpose(st, 0,1) # [3, 240, 256] rgb, dim_x, dim_y\n",
        "        # vec_st = self.conv(st).view(-1, self.in_dim[1]) # [15, 256]\n",
        "        cst = self.conv(st).flatten() # [120*128]\n",
        "        cst = cst.view(1,1,-1) # [1, 1, 120*128]\n",
        "        phist = self.encoder(cst, queries = self.encoder_query) # \n",
        "        return phist\n",
        "\n",
        "    def forward(self, inputs, icm):\n",
        "        if icm == False: #A3C\n",
        "            st, latent = inputs # [240, 256, 3]\n",
        "            phist = self.encode(st)\n",
        "            latent1, vec_st = self.lstm(phist, queries = self.lstm_query, x=latent) # \n",
        "            critic = self.critic_linear(vec_st)\n",
        "            actor = self.actor_linear(vec_st)\n",
        "            return critic[0], actor[0], latent1 # [1, 1], [1, 18], \n",
        "        else: #icm\n",
        "            inv_latent, fwd_latent, st1, at = inputs\n",
        "            phist = self.encode(st1)\n",
        "            inv_latent1, inverse = self.inv_lstm(phist, queries = self.inv_query, x=inv_latent) # inv model; inv_latent + phi(st+1) -> at +inv_latent1\n",
        "            fwd_latent1, forward = self.fwd_lstm(torch.cat((phist, at.unsqueeze(0)), -1), queries = self.fwd_query, x=fwd_latent) # world model; fwd_latent + at cat phi(st) -> phi(st1) +fwd_latent1\n",
        "            inverse = nn.Softmax()(inverse[0])\n",
        "            forward = nn.Softmax()(forward[0])\n",
        "            # print(\"forward icm \",phist.shape, inverse.shape, forward.shape)\n",
        "            # print(\"forward icm \",inverse, forward)\n",
        "            return phist[0], inverse, forward, inv_latent1, fwd_latent1 # [1, 320], [1, 18], [1, 320], ()\n"
      ],
      "metadata": {
        "id": "9xCONztqwRdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "h7kyJMKO9uPJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpPda4YEv13"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erf14x8KEv2A"
      },
      "outputs": [],
      "source": [
        "# train.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/train.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def train(env, args, model, optimizer=None):\n",
        "    # torch.manual_seed(seed)\n",
        "    # model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "    if optimizer is None:\n",
        "        optimizer = torch.optim.Adam(shared_model.parameters(), lr)\n",
        "    model.train()\n",
        "    for x in range(num_episodes):\n",
        "        # model.load_state_dict(shared_model.state_dict()) # Sync with the shared model\n",
        "        latent = None\n",
        "        vec_st = torch.zeros(1, 256).to(device)\n",
        "        inv_latent = None\n",
        "        fwd_latent = None\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropies = []\n",
        "        inverses = []\n",
        "        forwards = []\n",
        "        actions = []\n",
        "        vec_st1s = []\n",
        "        episode_length = 0\n",
        "\n",
        "        state = env.reset()\n",
        "        # state=state[:,:,0]\n",
        "        state = torch.from_numpy(state.copy()).type(torch.float).to(device) # i added, change from int to float\n",
        "        st1 = state.float()\n",
        "        # print(\"#####www####\",state.dtype,hx.dtype)\n",
        "        while True:\n",
        "            episode_length += 1\n",
        "            value, logit, latent = model((state, latent), icm = False)\n",
        "            prob = F.softmax(logit, dim=1)\n",
        "            log_prob = F.log_softmax(logit, dim=1)\n",
        "            entropy = -(log_prob * prob).sum(1)\n",
        "            entropies.append(entropy.cpu())\n",
        "            action = prob.multinomial(1).data\n",
        "            log_prob = log_prob.gather(1, action)\n",
        "            oh_action = torch.zeros(1, env.action_space.n)\n",
        "            oh_action[0][action.item()] = 1.0\n",
        "            at = oh_action\n",
        "            actions.append(oh_action)\n",
        "            state, reward, done, _ = env.step(action.item())\n",
        "            state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "            # state=state[:,:,0]\n",
        "            # print(\"reward\",reward)\n",
        "            done = done or episode_length >= max_episode_length\n",
        "            # reward = max(min(reward, 1), -1) #why clip rewards?\n",
        "            st = st1\n",
        "            st1 = state.float()\n",
        "            vec_st1, inverse, forward, inv_latent, fwd_latent = model((inv_latent, fwd_latent, st1, at.to(device)), icm = True)            \n",
        "            reward_intrinsic = eta * ((vec_st1 - forward).pow(2)).sum(1) / 2.\n",
        "            #reward_intrinsic = eta * ((vec_st1 - forward).pow(2)).sum(1).sqrt() / 2.\n",
        "            # print(\"reward_intrinsic\", reward_intrinsic)\n",
        "            reward_intrinsic = reward_intrinsic.item()\n",
        "            # print(\"ep \",x,\", rwd ext: \", reward, \" ,rwd int: \", reward_intrinsic.item())\n",
        "            reward += reward_intrinsic\n",
        "            values.append(value.cpu())\n",
        "            log_probs.append(log_prob.cpu())\n",
        "            rewards.append(reward)\n",
        "            vec_st1s.append(vec_st1.cpu())\n",
        "            inverses.append(inverse.cpu())\n",
        "            forwards.append(forward.cpu())\n",
        "            if done:\n",
        "                print(episode_length)\n",
        "                episode_length = 0\n",
        "                break\n",
        "        R = torch.zeros(1, 1)\n",
        "        if not done:\n",
        "            value, _, _ = model((state, latent), icm = False)\n",
        "            R = value.data\n",
        "        values.append(R)\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "        inverse_loss = 0\n",
        "        forward_loss = 0\n",
        "        gae = torch.zeros(1, 1)\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = gamma * R + rewards[i]\n",
        "            advantage = R - values[i]\n",
        "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
        "            # Generalized Advantage Estimataion\n",
        "            # delta_t = rewards[i] + gamma * values[i + 1].data - values[i].data\n",
        "            delta_t = torch.tensor(rewards[i]) + gamma * values[i + 1].data - values[i].data\n",
        "            gae = gae * gamma * tau + delta_t\n",
        "            policy_loss = policy_loss - log_probs[i] * gae - 0.01 * entropies[i]\n",
        "            cross_entropy = - (actions[i] * torch.log(inverses[i] + 1e-15)).sum(1)\n",
        "            inverse_loss = inverse_loss + cross_entropy\n",
        "            forward_err = forwards[i] - vec_st1s[i]\n",
        "            forward_loss = forward_loss + 0.5 * (forward_err.pow(2)).sum(1)\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"invvvvv\",inverse_loss , forward_loss)\n",
        "        # ((1-beta) * inverse_loss + beta * forward_loss).backward(retain_variables=True)\n",
        "        inv_loss = (1-beta) * inverse_loss + beta * forward_loss\n",
        "        pol_loss = lmbda * (policy_loss + 0.5 * value_loss)\n",
        "        (inv_loss + pol_loss).backward()\n",
        "        # (inv_loss + 0*pol_loss).backward()\n",
        "        # (((1-beta) * inverse_loss + beta * forward_loss) + lmbda * (policy_loss + 0.5 * value_loss)).backward()\n",
        "        print(''.join([str(torch.argmax(a).item()) for a in actions]))\n",
        "        print(\"inv_loss: \", inv_loss.item(), \" ,pol_loss: \", pol_loss.item())\n",
        "        # if log:\n",
        "        #     wandb.log({\"inv_loss\": inv_loss.item(), \"pol_loss\": pol_loss.item()})\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 40)\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHpcDteZdLS"
      },
      "source": [
        "#### test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja6KZf13p--B"
      },
      "outputs": [],
      "source": [
        "# test.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/test.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def test(env, args, model):\n",
        "    # torch.manual_seed(seed)\n",
        "    # model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "    # model.load_state_dict(shared_model.state_dict())\n",
        "    model.eval()\n",
        "    state = env.reset()\n",
        "    state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "    reward_sum = 0\n",
        "    start_time = time.time()\n",
        "    actions = []\n",
        "    episode_length = 0\n",
        "    result = []\n",
        "    latent = None\n",
        "    while True:\n",
        "        episode_length += 1\n",
        "        value, logit, latent = model((state, latent), icm = False)\n",
        "        prob = F.softmax(logit, dim=1) #from train\n",
        "        action = prob.multinomial(1).data\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "        state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "\n",
        "        done = done or episode_length >= max_episode_length\n",
        "        # print(\"rwd ext: \", reward)\n",
        "        reward_sum += reward\n",
        "        actions.append(action[0])\n",
        "        if done:\n",
        "            end_time = time.time()\n",
        "            print(\"Time {}, episode reward {}, episode length {}\".format(\n",
        "                time.strftime(\"%Hh %Mm %Ss\", time.gmtime(end_time - start_time)), reward_sum, episode_length))\n",
        "            result.append((reward_sum, end_time - start_time))\n",
        "            torch.save(model.state_dict(), 'model.pth')\n",
        "            # print(''.join([str(a.item()) for a in actions]))\n",
        "            print([a.item() for a in actions])\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj3tv7XHZmD9"
      },
      "source": [
        "#### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjPLqIBgH9xJ"
      },
      "outputs": [],
      "source": [
        "# main.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/main.py\n",
        "# import os, sys, cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "\n",
        "lr=0.001\n",
        "gamma=0.99\n",
        "tau=1.00\n",
        "seed=1\n",
        "num_processes=4\n",
        "num_steps=20\n",
        "max_episode_length=500 # 10000\n",
        "# env_name='PongDeterministic-v4'\n",
        "# env_name='LunarLander-v2'\n",
        "# env_name='MontezumaRevengeDeterministic-v4'\n",
        "# env_name='MontezumaRevengeDeterministic-ram-v4'\n",
        "\n",
        "no_shared=False\n",
        "eta=0.01\n",
        "beta=0.2\n",
        "lmbda=0.1\n",
        "outdir=\"output\"\n",
        "record='store_true'\n",
        "num_episodes=10#100\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "# env = gym.make(env_name)\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "# query_environment(\"MountainCar-v0\")\n",
        "\n",
        "print(env.observation_space.shape, env.action_space) # (210, 160, 3) Discrete(18); mario complex (240, 256, 3) Discrete(12)\n",
        "\n",
        "shared_model = ActorCritic(env.observation_space.shape, env.action_space).to(device)\n",
        "# shared_model.share_memory()\n",
        "if no_shared:\n",
        "    optimizer = None\n",
        "else:\n",
        "    optimizer = torch.optim.Adam(shared_model.parameters(), lr=lr)\n",
        "    # optimizer.share_memory()\n",
        "args=None\n",
        "# train(0, args, shared_model, optimizer)\n",
        "\n",
        "# processes = []\n",
        "# import torch.multiprocessing as mp\n",
        "# p = mp.Process(target=test, args=(num_processes, args, shared_model))\n",
        "# p.start()\n",
        "# processes.append(p)\n",
        "# for rank in range(0, num_processes):\n",
        "#     p = mp.Process(target=train, args=(rank, args, shared_model, optimizer))\n",
        "#     p.start()\n",
        "#     processes.append(p)\n",
        "# for p in processes:\n",
        "#     p.join()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFrRKvOwhYM_"
      },
      "source": [
        "#### run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KuzIfVWPBIg"
      },
      "outputs": [],
      "source": [
        "max_episode_length=1000 # 10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dHLw7ewldCLm",
        "outputId": "28f9e10f-5381-4759-990b-cac8f7a39605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "10991110111010658106681136797101011081026993511510251969540101353110610453150833431110742112510712023961081643116106104107104128911471119843241125521066691109386611171110341951101967657110691332773771710511108101110345775106711595311252699639103710109116138099090351073671313610862776511101000995110910102510243711473102101287770910101011911002511210103991110117047771064983693105941849911808554791023532971071510451615111111087311106696931125910947611117101540119341121111102102237111771171676112011615111000100011101191107935578788721103011973437946710725101196514382175091173608114978109618991189104901371093\n",
            "inv_loss:  1524.8624267578125  ,pol_loss:  704.1838989257812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "39410731010611748410994797119503509685110104910901111750710774107147114039841010218448974110510581630101027189675212981012413183799541271010611110811116704211105294126956869933754043510337239212332604493810210721107102217012278111101168995065111111113111291174891131188111091010728607074001012537960071235996247087574957923410675101111111811924693311194110910661011107577261097103610414944151049235050563327964907771169336932574253611781059110562599139102911381195129154711532511175451030631008982133104947536551104101318210810438464071011901710511510778115029276911001021091001029102638\n",
            "inv_loss:  3423.57421875  ,pol_loss:  7497.77685546875\n",
            "500\n",
            "1010511110106481187944701119610997425568522113492101096989102907072671091089746538551069483410109716106106339060104891101041079711110581164831101011010662287109108711675101110010326801058315110941061019949178110101001091071110092610081010670118301003710670107231110110741110600324207451111724111067810568777910510611106210102935106493110037101043316811061034610110431097102721010506117510281104910850978102691465119101052114910106618180711109048611410119111091860109106107807794118110340229620479103981045910108356111078431152103481770101010104911092533810119947656233106285068910711610778101123401110927275103107103710851095\n",
            "inv_loss:  2420.135986328125  ,pol_loss:  2876.41259765625\n",
            "500\n",
            "431126661136357921111126106376404673911103088105956489736796010103791375977111007667491006107112650792939324109814702417114282598771810661079107748909611107101393652915931004056109761001091281651091006357923664103118111969010710108462901046853499101951010404411116410238077510310531786116210100910060232110111636611108931199100611811770110102510654711981031133022143411510115337411210403452018304161102871071012103491010527106571089101101110401094901010995671016046661011063510791147402100947116616103754106818836741059067107411102707202733961271011845691737111181721754102710851108969102164095105110\n",
            "inv_loss:  1713.472900390625  ,pol_loss:  908.08837890625\n",
            "500\n",
            "1101367801041081107161047102111010011069361084447108771073958720040391009932210116110101010010102611104111107103652581104207373104645811108381192916681110103111140115111015510103432101231110811601010116123810587117274288987610110391131085101036663069219964931022691079456710359710137715102104510741174124449401031111011462166101910120120600102381081042110001274117610827710510428109462694676175051110010707024710107911286176701082021077931167962623790111087332977910701111193118762661198106544211983102721869510911104091461710977021031172911910289911398353110810282790221091010931071110289385763991128413104790678222\n",
            "inv_loss:  1516.8955078125  ,pol_loss:  491.4244689941406\n",
            "500\n",
            "11989974100497151010591101040173106661037225691199710061116101110991091111410107210199211710810077978699971110324410787973106872109902691057114610400776477490356301110118623310211189808102610575103793236103077216621110611734266449262247410101105910106471046401010560249088781161050110010191033910911783961871091921010290109665321008191066097311334911035117666202798972202113116310260172116761601077103102101082107633147610399107102762410849798610652112330893706101061008777924109103451010504610884721151154431110102421131084410294681076107476620211281117011271061010216710962711960680344104888905321007010810\n",
            "inv_loss:  1366.474365234375  ,pol_loss:  223.5522003173828\n",
            "500\n",
            "6674010293410273211105076671011068101101004110211121053101177365897761241274210364107424101071120402703112221091021010910067631771085610102211220101130791004104910102273315106118411031038110108101110630481119261061198081469431138511025108337810698800318310396642102101101141110710410510103119069110769366204510741011401137112200747167108566361011711711031045467917101121016670103599107775021099321061027645429011111010111040410129114081040711711851196571341104096683611571071101041105112102646936711011101809810861163961011916239767407039030397009594105101101061396447301071011340103063371010216931010510311076767279110745\n",
            "inv_loss:  1277.736572265625  ,pol_loss:  69.91767883300781\n",
            "500\n",
            "1108763010319611524746738610710710707372140710718689710101033102117310107101010106611907966641910610010101620737610210613119151100115101010106567509489471194101001101101156664111101074715116166050106312261221189101036101099331449101036110211114724102694949376671030107693105114956366310812111077941066100106244951177101070210046734268579320831006101067102101611161041751073109119751097106681110118011091102492542956211951117910124097710102211710105063063710434117110721095115001194810107290675107610100361046107101030910310118610841021011666101026101131011091011761101142251010072210386432516165101010961101111910105311051110469783\n",
            "inv_loss:  1216.49169921875  ,pol_loss:  -12.752325057983398\n",
            "500\n",
            "8101086067118484103948101103111010106662741102101100746140101310610101011131010401031146396071000733310302906054077281568221034620611101010466100107101024875101879741003106771099781194969424103801175768837254103111668114920101049610301077103747177749920774437446104110101031486352351080915108721109729374528032110697900101013766371078665298241061070101018108936305973537910441100191000500610646100271093498309103632711341010116105607106342869347110671078741099394810101044511792201023587891061006310977735369369776911030810408108646115114622169411001006485510101010108961019233103101011546030156897910106610\n",
            "inv_loss:  1240.72119140625  ,pol_loss:  -23.34992027282715\n",
            "500\n",
            "1030107861084107771079437661007037279861044681011442511966311056973618610811091032111034471032837010568101027310171810108910939690810107223273910498066110110841100797927763468411600637461004130586286117101038266114171081010444116104655931162040104146313109053388811057088433111161078813710417377118417091073468107771128747109577101010391081063146114110104871040794871010310344610610061016933062201187096527677049366774452119109492701001010710113761693108101103204114747056111083937011118034710589767102113118838104107361829989984110700411104115810603891114777402707891610667064638420847861071611112\n",
            "inv_loss:  1253.1661376953125  ,pol_loss:  -8.548640251159668\n",
            "500\n",
            "80358387361189638744760561048114107108770377947041045063711078446742137109751621001172610100919751081111612511749183701047232776609773117010443367118192711193111006155675821088473110410110487066271466101170907511111088200940566106101098109167474911743105371084102111510696511580611861171708697986277410981124926108810104760101010011877103310966783002009221046538101093637610106100110110093747378110710467102901072121139460710107067127101096881064201764736210791183297761111103101424446961078101010824041105075162237548059115310107002710224876101089611170071024668207101011071093978911311011202\n",
            "inv_loss:  1248.1912841796875  ,pol_loss:  -0.14574508368968964\n",
            "500\n",
            "9801910495032479110011406774334241134846101949783058611641086331844846710576261110711631010027910710960108647206639264984865410101121641082187971004587849754777610863646734078010794881676647295186107777231095619210479087567880118911624101012107108203486981084909261796109710773710011007279881110411117310431163710226696104661047273766527598411010216702710261036024910764103103211006006106968109113310737492186861147679510909784163604010107777977692561677382417201010187479801068605960117101137107607163411774105201004101288986107295116771010102701219430873483438984836404\n",
            "inv_loss:  1220.15234375  ,pol_loss:  -2.616273880004883\n",
            "500\n",
            "066179442721033108731088061090709103636240935608031821049110469100770104010860911001077877718410421010171010560611711834711101075718117818997137610007200103110231623473671370910986117437511396108772106106770373732975573910483610691086713107782436426211413810107718314710878111524166638449019617767477757111072113658831198610939114221011106049584394600168107601118428108910480310797226871071064411839117607256411318306336110310392898774411178100765629816167201117710101441110193610010947710481179937186106984638778796301779575202471096625710887301105438346771078438394107\n",
            "inv_loss:  1207.3438720703125  ,pol_loss:  6.40935754776001\n",
            "500\n",
            "0376794971033113657768771011112471112136306106878233104510038703710666101035772310607691012911031120344879981704112097104695107413001017209262101767511610717788101002709661096687606466538210103117111104468010148118720116103110887638970101000839010101174338407410391090112414370627721073166841760380661761014734260106102766211710177220328741103107101142278520961031683011731110658741024275653759822010387116373110084493103174792977331103717111164262076610110148340950101108864783878872308511025377106177473844111631186976108101281064960986710427310019580410158654776157336\n",
            "inv_loss:  1200.5069580078125  ,pol_loss:  19.202552795410156\n",
            "500\n",
            "1810960717706108607447236663882310801742737366867561056442781052390474001041068762662117366373945846210109261099411298779671031110176871180610051657101110801014692662810110101081177260210746101072491163671968344761727117467195310610101138010421177640111106851112116277864119748068106447811910964735378479111667007446651106033901170311102463646909672176699757797610377751040731078111080119106364041138617211079672407710846106875707164317001713108537762267010430778673671075059367454831027748773106647438735702968104106728907921116868371816633677101413761010606877116314159\n",
            "inv_loss:  1176.4478759765625  ,pol_loss:  -37.152565002441406\n",
            "500\n",
            "066477611610510454116224621148109467210766110695407787381071941302767111081457372111007118099381142747754946601110201024430433385197136391031001041183431110711947610917115107634330811741305868100310149099765400839779133836611105948100409661803146066468211417221073511079777101041025721078210110797949750841126079311653011134677407111011011357390111003111071071003547107910173117745101767471077570615113026010676114327067386410711481289665827107630146449964206489107105113279103710226701106361171067711118757784933384627324112763810575287112144141577464171071110671101697430780111120\n",
            "inv_loss:  1198.02587890625  ,pol_loss:  24.12586212158203\n",
            "500\n",
            "67110645282419114411101104421163010885801076202101011387204106114310410325760010477711088060241005020878217710328596248410106106101000875007427927816510218741014571294246377138099781083442010113107116477850711117113732396527320742840031110191734918979410581176139228414541007407806611748867001710810710870651767106911810610841131189684747101138703710441186761070421181081087260011367141677106450105877179387100111510610101074416183410107461731091013801121895292088926106647795109037672070636671111071436377656475341086766416332602400116611070665161410345871106751039111475511011040\n",
            "inv_loss:  1189.4803466796875  ,pol_loss:  52.53452682495117\n",
            "500\n",
            "061010610436176723010432660114367674794437184670610277310076116247104644710700201068160991185610477264604261134041070371781142341077096426868693109144661545726649005420362386874901047908810613710073371110481111108361110761306733921186461692777720487606291804117806212607107731748117211710105936211101010061100111007100221106767245470936611004402160671791111011127757609449310724810011334617757405110771173907611711146811610765326105734768757145105047794877708497061010201103678109242761010214746761071069457806647894107057834170411306506753786232010776129070940102611116\n",
            "inv_loss:  1165.24072265625  ,pol_loss:  -3.88205885887146\n",
            "500\n",
            "14368744790431049210466100910725622009106731010033711067116311111010810611230731214679722710424110111010310874382568360656008104773931010126348714104170042736117353104611371004101018177748689779934106386054697470410100104010661074562831198104725111115084217734020106101086114800027303334440750776310577846021067011161111900106111470010847116795426241047107101082110100110737101070610701016384650081006431076710010230011473372870042821102855221181080393102367404104270104342100448081139742806894771411418681179810211063611713116427118103241061671778101831111011100088746141111761117048673\n",
            "inv_loss:  1167.6072998046875  ,pol_loss:  -35.72175598144531\n",
            "500\n",
            "7102712344271041197403117247775307066667710747034240010721050628081137771073904224111037160463924608082710106109410111081097771036458942310751762257786781111077251040211104116611821881154628407633142070143501464500114111683240710975104221160100421080948787811531114077107210201053114177086270074208689687210044771010109647210710229219511261114746710224434927404850610101034103048838471044360500107108437510811811926968104338100401181044657777567447446910408241061617104407767807807901091168494462887444502659977731010061080844112264861111861077210132111040038253810761010809440105\n",
            "inv_loss:  1158.9249267578125  ,pol_loss:  -36.99810791015625\n",
            "500\n",
            "71017280110477063936710320101097107810761172771247242114087471610111561091051011912484087760541063104972711094530370604792118111781011038576103271010010237461073432768710501118434876310390792321115184524390671007101024767533101193716497010440474610897115402101099510287702518210617148460274842010132606800745172107711840244007867118317106573011110114749663942107734232740114056104716307075894107103401134109274077411567291041038710311129911066010100708117672112748647210084972105604871198930360644071163779777107107310107741110511436265769451077116864801019680411082288772238761021\n",
            "inv_loss:  1145.8065185546875  ,pol_loss:  -10.877748489379883\n",
            "500\n",
            "4775104474710760577876451011000941057828472937542571090166101031752021170100204032210061010707010315120840926027761104112410111036114478048364511611139446610001261021091897111710710128581079034626114723711710117001088910811370241061149840333138119874101068410435573410317326306081615711133771039237940952614414811971144101810647774029247701504064111211110537878090106370912103258310303215410104371141100701062114285121103011110100010606501177316010279066104327810610454071046082741090011904306610088537336410761097210548009078633974610660698710981110771162301710606710154061011726760\n",
            "inv_loss:  1147.074951171875  ,pol_loss:  30.301137924194336\n",
            "500\n",
            "2600003241061072414752100901011118014114710684636643107961291111741083692107471107481173477347013722011426710171563129841263107811101010180312441060710108477694117131928072410880374234622810071026118101061110410113810705410301100872717062110975625722160110763404410961131111710141029910901197611424747194084253083368783347677410108108134917810710821118941010216109510964933347773386105576011099711311378772116762112111104101002477472114111176046405111111100796779110107111324737271170810837511678096381089112874869370411004874103050976801086119101132276113768321064471170910704281010471636001030\n",
            "inv_loss:  1140.322021484375  ,pol_loss:  27.066207885742188\n",
            "500\n",
            "7096411410133101101010807110941001971011044102261074348839911742431094335511037121767783112971061110770442723470024601144103410610510840471117118301110930777110463944734520441184424650103260617167717501078980510428494281770100110726880365811111011681114109267104750861710511214861004421016910081008215033025491040661010947810110510310440119116118703939261045800408004411211056831001611091035736711713910119711011702111102842111001140166101184109011101773170943261083662876444801006054717394704776619037134431040325107117277701010713107261071111734011027111482665787471011465774770101111110249\n",
            "inv_loss:  1138.173583984375  ,pol_loss:  7.492081642150879\n",
            "500\n",
            "1090101109041124631268937790701010410097751010557781010136910071031010741471047403150944904729226898674711830672033101443650117234804705305711110934105842112121031141068114711301090710076211797810440632100310044341104598138301321010720611026743740713300201187242245713327110103871114099815431101002037019844731077471805071010457041051105379472400324109710012391294439914404411601177610119421003234001728470108967114890461161145436911718971638110346206007644251040675210039711101031031194341011337413430203117029906672907227110190739117025103443710510244101016001104871111741073118\n",
            "inv_loss:  1133.6923828125  ,pol_loss:  -5.874894618988037\n",
            "500\n",
            "973708948333571108071071006104321004102071144010772114020403343101351043741020770710109391384063811331038211732102424910811330737076703442920340101006642292510210469706424992710034504441113111040494469641011010117280110105879678764109197819063111653044324004417609747803622171466473698013010148381119371010842674594401974107666011511107101181105020796110726925927450773102361066140311710780111473411248112682110303794209257931711744911001880380101175110021010792451010100841933471071071410112929370325934110691011010104270780201054743102110043026101177764800942210103771021031080822\n",
            "inv_loss:  1114.1177978515625  ,pol_loss:  -24.27469825744629\n",
            "500\n",
            "9700961077519409011108490831794541013221107034768104020310505529011341170446607910010101007107073420101788010110371111029649611973943677611301837792937101084444278211379111010005478449393495301144913730783620010611200517987210009206729000801078110210019778763947110116012191082731160071171004691561064104711094535111077634841102117270111107101310646394111030250310001421102917451035721010010118373758241011114910820304771691178516404860029840631101014868071006105701110649310811001006619924110211074104105122421051034210630040405374644100611761039770149461010472430113181101171041170555\n",
            "inv_loss:  1113.2103271484375  ,pol_loss:  -23.98789405822754\n",
            "500\n",
            "9292611404411211381081030454701174101027104422101100101128109925247804976632410336787237411408880442264119111020011835400010711099107109963210633811303537366114301030000742391733421012771076104894448105691111131111086412261032112762623372401947710281911493311093028101111938212386113772511108492411191010702118311101114490011104509976101090030071121130811601059910549101511004627672114763238740454101131191127578967005783101157111071110661010528884872611643098872112033011471020379410881409768827934041361041110348107072011118502116044732721010242330009890971043741098111081620109007011100\n",
            "inv_loss:  1111.5626220703125  ,pol_loss:  -5.874576091766357\n",
            "500\n",
            "24104913283710430629437410986603910494111169811310102579168470101750903171136211110882394981131415710444438386333010882506411778090210117062171122911270460103100983032744310711115711139611217410551030911234510061110210911203101281010111056109102060011341043841088410410404420425770118760551111175822071011107011111053470807100642301170104561810530710916883535107066077106899100037101071081010098204010301116570510811221131482119104541034224857325210731157610211311114363741912106392809627741015211014953112884982294603472631024563101007083001094281041574113199741887289671111019728910301033\n",
            "inv_loss:  1117.6494140625  ,pol_loss:  16.969886779785156\n",
            "500\n",
            "1306634361062391000032961284101010418309039111013151639823949707911104785021044684107211371051135129791328830971104731711119492703349111034721277483991885941711110330103880920010356811022731097113107270101008104723428271011481101189546741156620004272722671743714837111410110111021191812101048214771264021001111501698301140109221019437039301011110739407609100107101111410369776760726174611044291110118879711229813221071007105086990409711510320994310744965111810729409292107067112804019260171026810367811117703993203111114110606657369910503210302280172104111021011751204111021043163\n",
            "inv_loss:  1104.485107421875  ,pol_loss:  31.08500099182129\n",
            "500\n",
            "7547841033994101440292641110104211711260112102370949611111201079111003108416304680847239317907983110441380831191075022702101432692741110854047531092183179115411113381081011192276171242943411483845080911511907710222731111100241084710102168257171111320991410519436109702072263201040192111084401243210113901121102108970341022110107103075475921180141804673017102030171103003011711121768449104887937217411711071202111201721285961006569113411431614113798759106010329042231146318074014491001042202070741143031000534472040010109081010052108610109327821103103070810881171333100751120011\n",
            "inv_loss:  1095.5059814453125  ,pol_loss:  30.021102905273438\n",
            "500\n",
            "01161163428010073610298211789911004741066472119295103910601954226794497517382350314949012990119174922171023010745022110103179208417121021017271061111174350144189775299810153669893441797431068327758314972910005394790140107119669511101025026113011017102403804237112787110116483044464503073477100064131079114106472341919099010727543113627104734602422109108810411109108223154931146723753503240111192929107185111211100747101072734319102121048158771090820117283187308503109914171144270210310101049521011320220263111307802492211136804071090073111141005741239028454911344310251011118232\n",
            "inv_loss:  1103.0113525390625  ,pol_loss:  30.36760902404785\n",
            "500\n",
            "106603491094357253349940231111103023110254302191008491229911611193218601098020711259107119710111342416731099114437026364742827170112105491010220104108263705983332031011111080111042044701107052105431077741001886874180578673911114439874011994034478511281199842110419254280297811110139051542210568389837103101124107147610532662793110573336690923103910949486936603110700107311371038102810211033110987106200011222731011010834177116011921101074926290310241017491110103239931170096101675119371045232811704741042108303659422712191912211804112910104770731011200371110071038741014202114235975\n",
            "inv_loss:  1086.60791015625  ,pol_loss:  12.768983840942383\n",
            "500\n",
            "421102340800449502121148487764482023528107794091062606058101120136410590107310374601111129003118121068439762423937003101101021010010001037861967773111425116782502210118470289907701037372102966753310049931910101090108892111091132491670311811104038010119109111641711663881329141314811927057261111101144290192670711170871181960911078211111114104111069110310411421109969362371102274635968541101770611047111801329990250206800577191120109204110111171171064510261131264101010711111110102138811611061972111069313119172451011710440411808178410107114833701188493229112750958834753958797996468100121\n",
            "inv_loss:  1084.0169677734375  ,pol_loss:  -2.0192582607269287\n",
            "500\n",
            "7100692114095283824111021161011727966284112691160107484216421073581610511151019210103273372773187411112297733103113729101091148115828111163540810104941796101111310710211158112400321029000114924877643101251004101111791402111013106230620673401324101211122310711111210214443021180109477105011110109521010971092211324342251179393424339381189049413109420114037947448104111107049118048100361221484754102679591011112110031681001161904319104973151058218034339011434271010993190711419432757971068710029851133847321111352011131114779992191078001810202741096010721402270511091199927110100111496310321095102\n",
            "inv_loss:  1073.7354736328125  ,pol_loss:  177746048.0\n",
            "500\n",
            "228911211641107010104440217480263010702014132341010800011047411110397390100490181011324472741811311119701101077674830469105726730511631491140410171937109947034621111179111871068628195941041043100108272771622115632432294383790876900241023112114041109041033116068898211779094033114073324580729702895101107292290646115111024790107710026311900246114918646342011803410106490311047324107309643991103981571065962102422810973102210932114747414210346110807823433102111177494376931097710107911004414101212114106327288922321747310429405913103932119103297742249101119410462471193947442090\n",
            "inv_loss:  1069.688232421875  ,pol_loss:  428418208.0\n",
            "500\n",
            "4107720104111107108991012110011099101019739115119072298107009322361192608093424992373924735117241143411397348445014119109845435610381991832270332121169797243361309209269710791010960782417524040372477300910101011474620115103709998930306709721908910011306110103010411200911144901100591011267211631072839414109010103449107071194241122374021147601010340671210367109333062411011523491021241110807463973938734113321231010190071159110043645221072528702113003112445436872002410102726514111641048100109628321642042992971796390322010378241863411103102474111671037991143744370710337410\n",
            "inv_loss:  1076.606689453125  ,pol_loss:  -90.20774841308594\n",
            "500\n",
            "8441039003479546311900331003927405372043921174057367243391011107261093221022194391098462810722893211022042109010103410033810410118321194101111932377119062091026101110110737892111124142449112311326040954101307694114101949211902211936741142411229600905102284520332707108974100740211994749041111031376436440771162972762426049107294633423934108107860074714233910404930424471991197411224911767104910200101094140247443861193733910491024314441126741019474102427163973177600111179442713710039394491061010717102271073101104422111713461007167091072241101188911101122464900707940010410044411100\n",
            "inv_loss:  1075.9912109375  ,pol_loss:  2103560064.0\n",
            "457\n",
            "42110423432022660007522040343665724029249745710070971070106631071049240701843720259722911245303329410022419277101231072749004641007329739491170443491141100439107110411014079114094116911643927470781122464774700831411410370798909412127871069261494479211799234474201102946792102910748413177047119113160026974744071020776043392100410441110117410122130440447221164476220884479443941077003201102310747245492920211003774286111019944837644449891102222102641106101012414840997649111193393932211021097461040100049382304107\n",
            "inv_loss:  985.3785400390625  ,pol_loss:  -142.50177001953125\n",
            "500\n",
            "273962913691172244109291147974776149449211031242144367116477472936044741190711126203272977444472428211047373774113392110921070691027528291143110731111324444467186109630074992989301177044110792940111160271034943611226411491058432010901142724239984105102104429118434463107920410241193994410100114942210310710992290443410700149104911104210711240710104849923243740727630464100100104194912261144444910371433947941171000103426102470249741099160710114304409610443464746649282743177919112411429711609210949984321171167110443902610902923376743672639972120101044923179437100033\n",
            "inv_loss:  1065.3809814453125  ,pol_loss:  -179.9998779296875\n",
            "500\n",
            "100410275780314491144410424921162407242911044245270104973433474729446104237114799490102091011971040999236029121090791087231034731736948796703114443264104101014640674111101070117037999774114646244243109010271076794631079411009024479102497106104043997742600464111194210399020977471004114741007649717611009773970024107494494102209441043400426114100411770491071010770106332244410422423994100024119101442410910110041039412072310209070610411771131424101127439034403740606942149074117124977346469044324010414410942032796977091674311011709406224292744119444010744103104480112724\n",
            "inv_loss:  1044.110107421875  ,pol_loss:  120968936.0\n",
            "500\n",
            "444210377479410746411730000114108977119100024744441029422112012423743119310910113449274479411962471127307074707911979427344231076111041724441111406761142414447410101111720721091010207474779711442031010411224423221936394779444147922627091199047114104771110871128031104710074779117444947791040104111073107467117724977102011010114104743118843794677410771072101910447471111241071039039371121747791072441810744109244709104210010219624441104421024710306494901009711218117397419791121091171030794110474471001040072424210712277033494104474474103094999174942117417710244211494966494743463471172100\n",
            "inv_loss:  1040.0264892578125  ,pol_loss:  -202.4656524658203\n",
            "500\n",
            "1144713971196710109941102249044114407400444299491144994103246010360704744463444942477794244244047062332410107449072911477444448190718149943444644374741146111091064944447117411972610741044142474104784434119441149724371010610133119101121024647410340444629640964449430946494679610031049710447791130311602743107247437641092222227444044476911747479684210119343941077434011400402119644112210041117426919103242944493103241404607096619103601010793943241024904796794141149740041144347469249010434407110101170792671024117446447444243099941041044479104149449440364100342920273\n",
            "inv_loss:  1023.5917358398438  ,pol_loss:  506995776.0\n",
            "263\n",
            "202417014447104472107060210272490710944104924013304364219011704109343702104439203911024309421040432117704447109349101844211031041041063117492310911640104071144044764442411610274244947709044444192441007114441102447464423917961920044411690779344910771093442431410971144097993001424499442431034045\n",
            "inv_loss:  529.865966796875  ,pol_loss:  -116.5307388305664\n",
            "175\n",
            "44442471074444422144422111349747470409444111070191032433492277109004094011126716649447744104644714101164744014010741171097144477410934701071149344044740432903937394449434944104624772342944924\n",
            "inv_loss:  341.1992492675781  ,pol_loss:  -72.09039306640625\n",
            "500\n",
            "791004044247610411923044434744644679264161140414734044442973244494411766649704342444244644201141010979464441096946410444494340941024400410741194144773740441144091071024613460404610917444277410734111111444964464269491044344441036944447304404610949477119464210704474464704477114644644103469971036046307407411010474040444441041043047441074011493444474411440444497444104114964764114447344444463274447443704471034932114410444421040064747444376464947404109046244494901144360103434104040100044764744444911424111004100469410444744449442410399404103941024449240449\n",
            "inv_loss:  954.302490234375  ,pol_loss:  520840192.0\n",
            "500\n",
            "4611010460474041044041090971044496404442444831174444441144441341434744444449410404464010099444442731114444434444242444464421063911104444344494449444037102044477114444102104444117114246964940610294744749444363794444346437444972444910299744996411021124444410249410011694107441004664410410104004447034443437204446464410414443494444330443144041049444744441044074464994105996741144934444410761043427044449467490637911449460444411446404294443444442464444426104107304441169423440449907461049449449449346973444467441004476311094496104944272434623419446044\n",
            "inv_loss:  919.4257202148438  ,pol_loss:  -159.2590789794922\n",
            "431\n",
            "49924044113664647941111601110461034244444474471494194491074494444437444397411429104474040410441044446446469411104444441024311407443490441040424944644414474424412444444441179944047494404442410940744446710944746244444424444724474444441141044477444443241031044944611441143947964644347444104440109474743494994117970641164006114449466947490444044944444494644444746114421010444466442644464444443401044444142404446424744104479440410494044404440471046441174944144401037770444744\n",
            "inv_loss:  745.179931640625  ,pol_loss:  1950792064.0\n",
            "500\n",
            "604104104104444410410904444414447444074444447344349644444444944454444044444444446927644747494704766911447494396344470411940411904710974114446494744744664444401149434464410410444410444444424404644444444464447101044444244063643744046474444444104427444944446409464104441166494607446649494444740446444434444446443106114404410741644204744446442444941041074404441174446474464446044444411467494444344441173404446344444444490404911444444444644446440444449649431136444441064674364444747311410742464144210410464404447444411443444049410464064460114\n",
            "inv_loss:  822.8218994140625  ,pol_loss:  -99.92581939697266\n",
            "500\n",
            "110114441141444446696944447644444464443444463474474446444461074640103474410444444461141094444441194461046444117444441144444644445444440724044474444444464444447439604444941044444101144076474444844444444479410644464446079444404431044444244440444410104444411640410104410441044494444104944106664744444744449110444464114764444443440471104410444644114410744444499444444047731044494760946444444444944664401041044944414444444443441044644640444444044104444145104167644344441044844446444434740041041111444494444441044344410446444474744461044444711646444464\n",
            "inv_loss:  770.8643188476562  ,pol_loss:  5679845888.0\n",
            "163\n",
            "444627740444444646444104944444667440444444446444404444444610446440444246443444444113444444443444744441161134444444917439444474444444634444444704443104410044444447444044104\n",
            "inv_loss:  226.14736938476562  ,pol_loss:  -11.338958740234375\n",
            "131\n",
            "40444444444461044244444444444444444444446494404444444444644744446401044444660444010440444494444444444410944104094441049444444444444444444\n",
            "inv_loss:  157.60023498535156  ,pol_loss:  61908636.0\n",
            "119\n",
            "44474441010444444744444444444444444442464444444474444444744474441044406703101044104436441044444944944941046106444444444494464440\n",
            "inv_loss:  148.79859924316406  ,pol_loss:  1.22308349609375\n",
            "369\n",
            "44447442444444444444444444444444044444246444441141146444449444444044641024444444444404444440416444049404441044446444411444344949444444494444444104104464446474474744424744411774444444444044441044403444664444644444444644644444444444444446444410044464444444441040441044104444406104442749447444404644644444441144934474444744444444444444444444444446441044434444444449404444444444447248641044\n",
            "inv_loss:  407.83343505859375  ,pol_loss:  355119328.0\n",
            "500\n",
            "40444494104444474446444410444446464444473104440410444444444444064444410444474410444444444464444400444410411446440446444444710444444444434444444444404744444114444444444444444444041044114449444447410444404114494444444444464444444010444444404444444744644044444441044440044646664444444444444411440444494444444444104444114444444440444444446444444444444444444444041144414464444044446474444494444444446444444446441044644411444444444944441044444744494444444444444444444446444444444641144444444444044444444044444444444444494444444444\n",
            "inv_loss:  492.80218505859375  ,pol_loss:  249965824.0\n",
            "500\n",
            "44444444464444610444444444444444444446444444464491044444446011444444704644444444444444444444444444446444444449444414414440102444044444104410464444444464494464444444444744494444444444446444444444114444404444446444444444444444444444444484400104441144444444444444410444464464444444447444744444411444949444410444444944444444444444444444444444044444440444444410444444444744444944444444444444496444444104444446444444444744444449444444104444444047410449444444944444401044444440491061144744444444411104444449440444444444444444643\n",
            "inv_loss:  467.9886169433594  ,pol_loss:  2720730624.0\n",
            "500\n",
            "4044444444444444404974444464446444446449444410444444444444444441074444496444441144444449444446401144444449044444444444444444440444444444444444444444444444444444444444101144444444444109444404444444444446444444444444441144444441047444444114401146444444044444444444444444410410444410444444444444444104444444444444444740410404446444444444444104464444449444444444464444044444444344444414444444444444694444449444444447444444644444444494444444444444446444444644444444444471044444644444444444444441149644474444440444410444444444\n",
            "inv_loss:  441.3410949707031  ,pol_loss:  11526093.0\n",
            "115\n",
            "4411444104474444404444444444444444441164644441044447444444449444444444444444444444444440444010444444444464444444444410444\n",
            "inv_loss:  97.61947631835938  ,pol_loss:  57.25566482543945\n",
            "325\n",
            "444444444440104444444464444444410444444444444444440464444444446444444444444449444444444444444446444411444494444411844469464444444444444444644444444444474444444444446444444441147444444444464470444444444444444444444441011411444444444444444444444444444444404444444444444444444444444444444444444144444444444444444444444444444444444444444\n",
            "inv_loss:  236.48043823242188  ,pol_loss:  200.08506774902344\n",
            "115\n",
            "4104410044444444444444447444444444444444476411444644444444440444444444444444444444444464444444444404444444444444444444\n",
            "inv_loss:  78.59783172607422  ,pol_loss:  23038924.0\n",
            "123\n",
            "44494444444444444444444444444444444444444444494944444444444444444444444444444444411444444444444444440444444444444411444444444\n",
            "inv_loss:  67.97065734863281  ,pol_loss:  110.8139419555664\n",
            "500\n",
            "444444444444444444444444444444444444440444446444444444744444444444444444444444444444444444444444044444444424474449404444444444944494444444944444444444404444444444444404444444444444644444444444444444444444444444444441044444444644410444444444444444444464444444444104444444444444424444444104444444444444444444444444444644444444044444411444444449444464474444444444441144444444444446443444444444444444444444444449444444444444444446444444444344444447104444044444744444444444444404444104444444444444444101044444444444\n",
            "inv_loss:  299.8734130859375  ,pol_loss:  3021565952.0\n",
            "115\n",
            "44444444410464444444444444444444444444444444446446044444444444444444444444444444444444444444444444944444444444444444\n",
            "inv_loss:  57.393280029296875  ,pol_loss:  116.11400604248047\n",
            "500\n",
            "4404446444464444044411441144444444444444444444444444444410444444444444444444944444104444444446444444444446444446404444444410444444444444444444444444414444444444444944644444444449444444444446444444444444440444114444444444444444464444444444444044444411410440444444944114444444444444444410444444444444744444444444444444444444444494444444410444464100440444444044410444444444444440444494444446444610444444444444464604444444114744444444744444444444404444604444444111046444444444444444444444444444444444444444444644444446444\n",
            "inv_loss:  318.7311706542969  ,pol_loss:  672722432.0\n",
            "139\n",
            "444444444444444444464444444444444440444104744444444444444444444444444440444444444444446444444444444444444441046444044444444444444444444444440\n",
            "inv_loss:  69.74391174316406  ,pol_loss:  141.86000061035156\n",
            "343\n",
            "4440044444006444446444444444444444444040404444444441144444444444444444444444444444644444444444444444044446444446440444644464444444464444444444444444494440444114444410444444464444440410114644444444444449444444404444406444444444444444044444444640444441164449444444441044444444404444440644464444444944444444404444411410444464446444444944444444444444944444\n",
            "inv_loss:  235.02561950683594  ,pol_loss:  1157328512.0\n",
            "500\n",
            "4444444444444644444444444104447101041044447404444411411444044444464444444444444404444104444444444410044444444444104444444464944444446409644644464444444444444444444444440404644410444449044744444444440444460444444440444444444444444410444444414744494411404410444444674444444444644467464444444444494447441044444446444444444444444444410441140444444404246444444444644104104446444444444444444449444744410444444644410404109400444444444406449441104444444444444444464444444044444444444464941141044444444444444041010444044494444644444444\n",
            "inv_loss:  397.7173767089844  ,pol_loss:  157971376.0\n",
            "500\n",
            "04446444424444444744449640447100444444474444444744446444444449044444444474464444444944644110444644444444441046444444046444444444444444444464444444106410444444444444446444444494410444444464444444094410444444444444446444444944444444404444444441044444449444444041106444446044444444444444444449444444404444444464494444444104749444444444449444644444410444049404114444441144444404444464444444444447466444444444446404494444411644470444449444440444444044444494106444444464444444410444444944446444644444444444114440446411040494\n",
            "inv_loss:  399.65753173828125  ,pol_loss:  1809723264.0\n",
            "500\n",
            "444044444444444444444911010444444444444644444494444404444411444764441094411444440444444446444444444444449444464444441044444444446494661044410444944441044644441044044410441011444464104444044444444946446444464440444944404444496410444444444649444444644444444440444444944444444444444444044444464444411464044449444944444104444444404460449444949410441144749444444444444644444044444444444464644464644440444444476446464444444744441144444441044446444444644444444444444444244444441044444444044444944444444404444444444444464444444444\n",
            "inv_loss:  400.4932556152344  ,pol_loss:  1591904000.0\n",
            "117\n",
            "444400444410447741044444444444444449444444444444494444444111044444444440444010444006444444116464411444444104444474464449941044\n",
            "inv_loss:  110.97466278076172  ,pol_loss:  118.65093231201172\n",
            "500\n",
            "449049444664444611444494444404444444444440464441044106104444641064044444404444410404064694444114441076444641044444411446744444446444444404444944114474444464444100444444444440604464410444444444444469444444064444644644444644444644444444444464444444644444444444444440444446444464444464404444444474441044444464440174464114641044444669446644444444444944964444446444444106444444444044444444644404401044444444464444404444044444944944441094406444444441044444410464404444104040444444440449447644444464440411444104741044944444444444444\n",
            "inv_loss:  480.1698303222656  ,pol_loss:  220626160.0\n",
            "417\n",
            "44464444644444464444440444444444101044444446101144446411644444444446444444444444444444440444444444444444444940449444404661044444464444444494404444444444444446444044666446110444444444410444444444464444444464444444444644444444444444410466444447444444446444494444444444444464944100444490444644446441149644444604644444699444444444404464446449404444400104444441069444444444446446640404444064444444444430444404464641144444664444444444444\n",
            "inv_loss:  361.3870849609375  ,pol_loss:  1235489152.0\n",
            "500\n",
            "44604444444644104444461140404464404444664404444446444444044446444446444444444444444114499444446446944440644444944444464444444466441110044934440444449464474444944441044446494664444444444910067444444044444460410444444044444746446410410464744444444440444460444644444446444444444444404641066404444444444404444444444440444694444446444444646464404444444444444467144441144441044461046444444446044444444446496444046044464441044444464064444644449449444796444404006964494444644644460461046444449411644444644444446444446444444411\n",
            "inv_loss:  520.0650024414062  ,pol_loss:  2746245888.0\n",
            "500\n",
            "444444446444444644404464446444444494404944444646444404044444494444446474444404644644444440644494444464460446444410444494401141114444444444444444444400646444494649400444644444440444444664444444441064444400644444066449044644441044440461044444644409444444444446444044444444444444444444444464444640440404444494444494440404444444411444440444104444446444664114444464440446644444444444444444444446464464444444404444446464494444041040449444444406444444744646446944444444444944444444444644464444644644444444444404444444\n",
            "inv_loss:  454.14288330078125  ,pol_loss:  855.1972045898438\n",
            "137\n",
            "4444444644444444106944444444611410441044644464446444446966444664104444464440444460444044444444444644104441046444964104446444464444464644446644444\n",
            "inv_loss:  145.5236358642578  ,pol_loss:  260.20208740234375\n",
            "500\n",
            "44444410444104444644940464446644646464446440444644441010964464404444647664444644104466444444664444444644404441040464444404464694694494444410944444444444441046460444444904494446444444464444444446944464444444444444444444446446646944444746444044444444444447444464444446646664444646444444444100410444440444664444441044411444444644494464114494604444444444404464644464744404649444466464946444464444660444449444444446444424466444444444446404444444444444444644474444611446440464644444966446449444444644444444110446944646446\n",
            "inv_loss:  525.2672729492188  ,pol_loss:  649449408.0\n",
            "155\n",
            "644464444440444644469444644444774496441041049446446444949644644444644460104446066444446466444494644464444464640464444449444444446444644004464444464444444444104\n",
            "inv_loss:  174.87294006347656  ,pol_loss:  17595564.0\n",
            "127\n",
            "6444444464444044444404464044444046690114749444444494464644946441064444444444604444466444644441041090116444040660644046940444444664610\n",
            "inv_loss:  169.4285888671875  ,pol_loss:  257.80865478515625\n",
            "500\n",
            "46446444064464449444444410444444444444496444444444464444444444664444666410444444646464444446444104444010044644446444604464464644444446444444464410444440444106496496444446441044444499464644444044444664444449446946664446444446446464644494446106444690910444644044044444444444464444464664644444644944404440444404444444444444446444444644644644446646494444640446444449444444694444446644644496444444444446944444464046449446644449444464494464440444444466944644944404444444494444964444446044444644944464644446446744444\n",
            "inv_loss:  504.632568359375  ,pol_loss:  1235.3304443359375\n",
            "171\n",
            "9466444444446444446464444644494444411464044444444464444646644444664664964440444444440444646046446646444664444444699444444464444444464444444444444644440404446444444494444464\n",
            "inv_loss:  156.3355712890625  ,pol_loss:  455.0538024902344\n",
            "500\n",
            "44666444446464444644444644444669444466469440444464114444444444474464464444464444444444649444444464944410946404464446644404444646449944446444494444444446444464444064644644444494444446449106644464444404446446944494444949464064464646044644444444444464444944644446644449444646446444476444664746644444644744444944644449444446464444644444464664666464444444444464949449644064444474604444644466104444464444444464644444446466466449466444446944449444404444466440044907460411464646446944444444494044444946446466011464\n",
            "inv_loss:  527.7725219726562  ,pol_loss:  575984768.0\n",
            "500\n",
            "446464449464646104464444446611444464444646644444646444444666444444444443446444446464666644440444467604104444446441164644411464964444646444469466944404464444940444446644464444946640464646444646444446446464444464446446444464664444444664444046644444644444446444644496444444464464444446694666444444446646464494446946644444644444646444446644646449444104666106469666444469464644664444410644644464464464446446464944644666444964444466461164444444644114444446449444464644644444449664664644494444449464644664444444446644\n",
            "inv_loss:  540.693115234375  ,pol_loss:  2763296512.0\n",
            "500\n",
            "944444464644644444441644444041066664464446444449444449404474964464644646444444644649664644444646641164464944466404446446444444994444444444449644444446446644444444466964644444464494446446444404449464464446444464664444444664644446444646446446464446406464464464444464646444464449674446646944464464444610644466464446944449446444946444966494644644444444464464444694464946644964444446444446464466674444444446444944446444104444444444446444644444444464466666066494664446464444664444646746644444696044444644696444\n",
            "inv_loss:  521.767578125  ,pol_loss:  579912640.0\n",
            "500\n",
            "61046444446444444644444664494694466644444464646244669644464646646469444444444490444464449476964644946444644464664644464444444444944644646646464464464444444464446444464946646444664446449694446464106444646646644444644660447464644646444444446644444444644444944644444464664104444694446444444460464646944464494644464444666464664466644444446444446446649446444444644444444960646449446664644446444444944464444646466446444946604944444444644644444466449444944644446664444469444644499464466494446644464644446466444\n",
            "inv_loss:  526.3667602539062  ,pol_loss:  1406.6451416015625\n",
            "313\n",
            "964466664444464444644464464464644444464666666464446464444464999446444446066666406466646644646464664466964644646447666444444649444946444464464664664446446946444996444464649644644464644046644446644644646444669444444410464664664446644444466446464444449466446494964694444444446944669449444444964664644464446646441069644\n",
            "inv_loss:  371.041015625  ,pol_loss:  894.8229370117188\n",
            "271\n",
            "4464646644646644444644444444444674644466446460446694494664464444664444694444664444444466444464696446444646666446466664444666446444946464444466464464649664444444446444464444664464464494646666646444444644666466444444666666466464644964666644744664644446440494464444944446444\n",
            "inv_loss:  279.0264587402344  ,pol_loss:  802.0995483398438\n",
            "500\n",
            "4669664104946464666646464444964444644464444696464464466444446446444464444666444444666464441064464464444949446444666466446464664466464644666446666106644446444466464666449664644044744649446464466444644644646444466444466646664664949644646646664666466664644666664646444646646664460464666644644444466644106466464444449446464644640464664944446446114444464446669966446646446664469644644464664464460644464666664444466666446466646464664646464664644946446641044944491164664664464464966664664410266446464494646444446466\n",
            "inv_loss:  560.5812377929688  ,pol_loss:  3235419904.0\n",
            "500\n",
            "646446444646446669446646464666644664664464644644444446646464464646644946644446446646464066646464664666466446646666464646666464466646646664444694444666666446446666996666446466666466666944666666066644944646466446646666464644646494466664946646466669464644646644699666464666646464644666644464446646694644464494466664446444469464466464464444664466646464646466666644449496446666440666666446666444444644469644464666466464664664444446464444466646464444466666116444664646666944644669444664664644664446646446496\n",
            "inv_loss:  551.2674560546875  ,pol_loss:  1399.230712890625\n",
            "500\n",
            "66464794964466446644676464644646664466666666444666666666666464466660664646644646664666464644946666646646049666464666666446444647664649646644466464664666644944664466666444696646446964644666664466466466444666644964446646646666666666666944446644464646666664644646669664646666664666466646616446666466646464664410246946446666646446444666466464664466664669944646666664466644446466666466666466666646464466966666464666114401066444646666946444644944646646646646464464446446464466664449646664666664664464646646646\n",
            "inv_loss:  544.3894653320312  ,pol_loss:  1316.4366455078125\n",
            "500\n",
            "66444466447666467646646466466496666966446694674664666966464666944466644996644106664664746464669664669964644644610444696466464466496466446646966446969669667644446466964644664444666646466679464664666666467646666466466446666646664444664666646466666946664464666946664664666666469646694696469446444664664666696694446696464666666666466664644064664646666649644669667467669690664766466644644469647644666641144646604966649669464466744646664464666466494496664666644646644649464666644444466696446464664444644444666\n",
            "inv_loss:  580.36083984375  ,pol_loss:  1116.0555419921875\n",
            "500\n",
            "66664666644644466664644464964664666664666466666664469604644466444966966664466644466646464666994466644444646696696646666646446664644646666646446666646466946646410641046446944666644664646106466464646466666646466664664644966664666646466664646666444644664464666664646464499644464464666644666606666646466644644666664606666646666446666666666666667406464666466466666669664696666446444644696766666666644666664464469644669446666666464664946644646446496446946466469666666664446466467446669664466466449646664664669\n",
            "inv_loss:  529.2848510742188  ,pol_loss:  1035.5765380859375\n",
            "500\n",
            "6446664466446466666766666626646466446464444444664646664464446666464440696644466116646464494446664666946461144944466446644466610461446666466966646661046944644464446646664446664469666694664466964649496664944466466646666696466646409646666466476646646449466466664464696466664644444661044144746466446666664694946444464644944664446964114446646660466664446666666466666444666466666444966466146646446646444444666466446666647644610664646666644464664441194666466946644646666646446666406464446499646446666664466444664646\n",
            "inv_loss:  541.316162109375  ,pol_loss:  756.6095581054688\n",
            "500\n",
            "41044449464446466646466466464644464644660744644666696464494946644646644444666644444644664996606644611964664694444666676644464444664443661044944440646444104446664496911444444464697446441064644466646644464464441046946694444696666446464496446694646666464644666444466464664446644666994461174461069961066446466444646469946464946666116664644404464464669449204646664646464694964466966466604664446444640666464446446669666664466104444464449441144966674446494446964666446444644461044664446464649449944444694466461164446964666\n",
            "inv_loss:  605.66552734375  ,pol_loss:  444.74835205078125\n",
            "500\n",
            "4944446604464644446446464669446114644449466644646669649404464404647464944967946066679444611411444440444664646444464644946649994944466464444866446747964444646464401044649464664646411060444674466610410946964464949644464660444444446946464446664466469640446466999966464664601094404666940646466944946464694449444104944644444474463444464444644446649646661044664466466646666464474946946446466496104969664649666446644664464666464444664664464746446104644647464677669107449446944964461094640666464444449944466644463446474409\n",
            "inv_loss:  672.9144897460938  ,pol_loss:  121.5899429321289\n",
            "500\n",
            "80024116041066547410895821174104149011911568422440426324686505073937241110933104610110117684106400081196415444134789944990904103107016021841410193644962646810610411799310101054837097113260110903414111661043322430610084511749421110943787311630424021111174810110611953991121164951047112886307484114101148119806110211110586109457234910911991033490461198371174100949961112094208118749001010486981197011821110461137311097411109501099045614109311611818733141144119471046780907711461112674914160102411148169119341171010104869481511221461149710760938710891131410431110910118941040843979107521144\n",
            "inv_loss:  1453.4642333984375  ,pol_loss:  3.271623373031616\n",
            "500\n",
            "1055281188518831003315353551088231187151518718155211101153588838111031551581081535258515158381585585510338855135988111151815110585581558038551811058102218513108853888558185538535231833558115481851113181110858311525108531558002108835303585335510655505583811311833282281023810158571105754113881125111851181871555755108533325855838253283718558118385110387522188388855551103535852528815565535855182313352105555351155281575871528352115837805588525885185585510888253355858381878818585111155185115138115811551351785825838252585588558531553102358138558\n",
            "inv_loss:  2035.7239990234375  ,pol_loss:  775.5847778320312\n",
            "500\n",
            "310538180115118804310310968851001010281397411151137111107252111009113111378583884591811582111852358731113111518205811861218732833153579933011233888321232110551131138851153758110111111711100083849310680589221581318582449610100011310816398338810863010331171127810111898310801122398235311381590551215851305838610817082810175828371181118815537861707858861170740183111111193711934811411086333411831001110853883891111103211121713585111021153100115338093815422573211808554510413877260182155101568510885011021141059275854117035810101185118379311521082438117501027111183511127093275\n",
            "inv_loss:  1721.9615478515625  ,pol_loss:  298.0481872558594\n",
            "500\n",
            "10361040681934878076419260991069411692101010387804108119816811111069141116711700194104118103410411344462321696113447946984507810712054004476744106895469040441030470441401811943115674101014910109940878984731008238451041011711340123105491095431013016639077410888611481004104646296104660745321431574431171082911411101095224611411314910204011740601140974227114494039464944268974853468136110310810100810694059901070104989029109735001091957649842114541906871111710731097111149025517081176420432911109995104107711810145410084111100219610192117106911710131110437904911580114638111026344942601111\n",
            "inv_loss:  1289.4384765625  ,pol_loss:  -17.51342010498047\n",
            "500\n",
            "8980311211006446111441124162994645411910114105486114116101099108811104404979406317103310401169608691010134548428811444110304006364341167246160471009109424111411610418111430107999761144410900811661071091111107644010411630329418414410161041147301017298789494810100917441791031160306460105109101018109011298624116107867611108099411761146610104010470478468446630116910101111839631402111111114411464119117564641471018992928464667949810211101960031044610743610911014101110104117104101110467201044971081445107116464911641164619217109434904116674403561144114617638482310064448611044104174741110351044973100041035\n",
            "inv_loss:  1118.5205078125  ,pol_loss:  -107.9863052368164\n",
            "500\n",
            "66611943941139684919411941087131171660010344517444936185111841111761118499523240261199064426011119146010410644367311784079311939118711477293100949469118344626609269046431111893844350114343110114624210641143901104396610404689904711424739106118119887081600489941084019400661010114343911949610129911195414113406444444111100011491090910943109765076944617910446104681118903478671046213696440107436644644471010111611404437100103496111101026804114810010449449101194101021145906441199442612668699104699771081112680933961126234990710181017671171047410901010449471044641009210461031129497106696\n",
            "inv_loss:  1032.7779541015625  ,pol_loss:  -119.21160888671875\n",
            "500\n",
            "14466994746480211976109814448900061304399531049004636880948249700901108741611110109184601049096743644114963361144896494899193410241509999810101904048710104411474843119875114111077103841604911910109421011101075044340769761106014802108107044101066949110113100103886717443990104110418111111344180724009910117499036081191091310310410411220104956410101101146013145211436059113471011019647072101182178081102879974594111114182101210105104108241738110106107584814620444094347717921174708111081010411101110043109108410086040102214113344397434493047074700146979811442181011499634421011741136091948104\n",
            "inv_loss:  1030.254638671875  ,pol_loss:  -84.83829498291016\n",
            "500\n",
            "386860011888407111119119391020901159991110491109610431061017363904201173014810752849911825107608112801310101093011111110101133310429111911088211310039611111441177741808983611111173710894300019442846906051001066210411111108118310110817107278806106100473104111801930101119101011624188691040640233669244710285771101107824811011101789956101096111197121110112116111175910510501041193810072509301111811113541010411948736367481490429906290711041049400311698517378117796015047140047830578884695631039460310041061110810211261178674504593311014801108711111081015828103098227977811424710984711971110749710671081010907461022\n",
            "inv_loss:  1038.330078125  ,pol_loss:  -2.4329993724823\n",
            "500\n",
            "71034101111064254801004395183970101314337118131111910321101016251089511311981108111781010107831110791151310100441010834310710221510118119588102111171058184104388311810314911500354434101301106103431067670100818317883480311811054391119103183115193125011130117511100113102670323914281010734053906501708115063499547280710674311131011923894100494846811142810125407106707907307243105100511111010281016250144141191047010511087352083114210110700343111093881157140551317510838497851411111058508411180836935610110565342430115811789711812898131077069391110105118261011810387101158168334201111042618887888\n",
            "inv_loss:  1040.7239990234375  ,pol_loss:  84.48456573486328\n",
            "500\n",
            "972108115871151108783281580011505489119781114011812584810181183439356561091070591117842034183412439599118106864101189134100411974657266101176901031061079781109433861190820254180781011964118560723393483899603839810105478211951110811031186823297348410578175857210117000378171171285711117141104100077970552986343017338231310531141810478538106781081811384483234489100117711740901091063111011828118113238101158811973711855119010125041814731100413181182655150114449115491081008511564571011104111011810785105107710040851033391077398881185178981001181173815117103237957008305868151578012\n",
            "inv_loss:  1031.12060546875  ,pol_loss:  114.90443420410156\n",
            "500\n",
            "302118071061826107164104128118510009110103858508111083910114830671060116258450071893188975857111130109611087811901111211185598401075491871163777751081052431111125101181084790117107172106451151185418400480988111019710876116852511770188310312063510184116088119105533412733007106811001171171111111147517084113810751013110081181114915552106273118411581071110821088451179111091511014112811103110350810310511273102101114091101111093981108218103581101011103451881186750112411108547102851783751119113381791111090846911170010837601463105111896108251750811571118340188011786101181100081133111959511107298828811571077\n",
            "inv_loss:  1018.0159912109375  ,pol_loss:  111.68871307373047\n",
            "500\n",
            "102107411531086889751010401038381143011388743028828511337108889691110103831018311101111411115108410867358889258479510111811355704034417787194531135981851183037111181011388879032568218131121081135810611101111081010113841097104251038701080112174111624134888871511811036217110111100321821088101119113558810239103866688411111011119817875418103314110985117079301115808561010611171328207002119109118510801128910131007210101011944103198861084341783285111511943121089331839112780864119571090110851110493411101883748113140112724361111888144357105147131681088845711083011081211291328014278468569910107\n",
            "inv_loss:  1007.0514526367188  ,pol_loss:  77.88106536865234\n",
            "500\n",
            "7546204168910210385113821101111808311910094511021052774129129610239708234769116561089118103111091174109905086184804114210109083771104447345009940011071101084411165104861198710188141185414113480982580100801711431076088470710510321910103511011510310365995310410110105066384987370631080687101041810101145810111034115801027873921171031041114105158448158543260411343118110110006129110164521310731481060952831131111176431921011100859108102108420104490611719398901110503432410344110391011040399111174114363948161093847711116209847770462014115981100010118144480808118540287799040158710118119003809776728311\n",
            "inv_loss:  1027.2247314453125  ,pol_loss:  33.57389450073242\n",
            "500\n",
            "47701101068109304108689116986484810381106385307410345201011107219730100210061310110118411341186413310859108241081071118410441076441073100046820023101118914115784681108775100003561014576113110110215106106618116113826344102664316116840010474113119104310867287649385254293976708941873651691101314117528801101110161100611101104908941011680911011110893413769411168741061030101149597830211118498101198221173115831058441941310460100432909821020111084190101081416441511101190911100311333710110916334718110113711110105012047401020401641110104849044012176911383892449831110411111116977140419761084211944614081073\n",
            "inv_loss:  1024.071533203125  ,pol_loss:  -6.655053615570068\n",
            "500\n",
            "2101477897181006891141028101974668382109071442713604647510100129701111130109810742610111161041094104408102611107284114964410111170111491115434794071307994690711665562140780103119511101141011910361160284460108668176746041111168447162101011117414112341154461104350108431170134956986657681110454201111443990120011410911066977738908101173698116101411641111118543921193210049741126010983010710183111110811598119003410941123378118610411297718111311811941110116891169117657968466311118100410811106510619504410159860471141101163710112904818044534108514710184101783271103798851071679901011691144414411435480106\n",
            "inv_loss:  1021.3449096679688  ,pol_loss:  -29.981826782226562\n",
            "500\n",
            "04301291193211793051710663036545670404118041541377111133117104115116115167417532634111111285102956491184022110810371618146668101104494325490079900716480481311806841835401764110151082509661194661179791031010471054494611691134511103611131184647141844117040901071011900143578110014624111010610466101131487104511110891041009110575115765163141123111071149793611081081143125761115141806895589101032148879010112320411994411311834497510411860511493201030119718681058101174105110101010444010011109113410111041143498219102687489104148410941114101117001131463079471011573110034101125511496412992576311141010\n",
            "inv_loss:  1021.2542724609375  ,pol_loss:  -31.70088768005371\n",
            "500\n",
            "887158112411910343933285101320301171710699114101011101109641089046141106118419041010089861083966248263131147810741106101140051045827296107938861113710411104108495811994674710104104928161055115844461118661199810711141271149211641106371044813591061111947989710723108101111011751104112116410445210467491531144113898116012119112481070791189711494408497941180011094371111126001191071310012500104711095886101044010116841118639160716447410101111992109217110598100511801054917101111074854756811055118411671019741411476281518971841085984911403398116119374418107481111110611101092794111080113721141111107301010911810191198\n",
            "inv_loss:  1020.2247314453125  ,pol_loss:  -24.800277709960938\n",
            "500\n",
            "1011081180461121043347108461049106106111191077671486900331092321051161191192106233744041001011141077100429176128371141024749110501079722798398656684100588111768881141195144104101175108236719211446211109789071138888104681088114471111103951414956118600341010602542169524114968990641111074102921341110107821081103941183107992701581111113105589108410448514491064101111999587316498454610317935642117810971131009471010481088800144611451071192104491084410584117956118679995018483111101110418281111010588408107989043980111150281027064110329768314431034101138041104894143111087881145007943311891919886649\n",
            "inv_loss:  1018.13330078125  ,pol_loss:  -18.247915267944336\n",
            "500\n",
            "10111054894461091189308429101010115552895196034610103130491041110155410779621111512010931344983114110811911057081194958307140814801148001115731148101124770176506410116009916691011121167813027111110031511811083310614477111041163081110114107238391059051101068915167811111037211966111110881811574103101160918113418034830118464831910811311949131029901110903244815180989247711903110633958299810101174106487824088242110426901334241046861080109611101779111110988910341010101144086444077096909126938118997243042448081560015611108102870813511084300021158611100102081044010112021081064134938261010898\n",
            "inv_loss:  1018.1419067382812  ,pol_loss:  -16.63737678527832\n",
            "500\n",
            "491030411185110681240111928041054917038789400884210741148921141991105070081179732010811381984114869810602104347715410549211110711474119843111110141185001195102002100531448103406301024132810041110471110754989260109711811227119110888811101173431110041191189114411845316113611151004335101009080581114936411391108988164700077541111392541066470918371749011184445337115166811481111061103311410391010611101119178811423111106158101098882531063063711621161085210264101040419564110410114650610607395414908107107490086857408052183074796450688073018541111138096068319140511610106080130111184101143\n",
            "inv_loss:  1011.3480224609375  ,pol_loss:  -18.167638778686523\n",
            "500\n",
            "1161234474641168379119121170410044011630101108111011103106962811049814980434861041321143160140114111431071186741011211810654208151011101088136827994152746101187371814414771118872108517997707101130181108147101146124110749581610743310374173881097405787811861007307111010931111541088684447108011793101187578549094701111010804616200881101100445281025934047010511867259611403419399111001010311068258101070710446971111844971191001110101104693770199481791111637408625731151068017831011084119441019551051060118661174468101089114410746310081101041001118211050110193737711952314840150991044719210334111134579471\n",
            "inv_loss:  1000.849365234375  ,pol_loss:  -14.315673828125\n",
            "500\n",
            "10910451093110538711810370601151193991133113982268111114811859003446011566115711107411104114195044111601164141058458410763590082511554474337697710101013118911138641128801110474204029541110534811121119071681105111114138102119081610195116110701469317016011741563114911341264186210932389448398991000204098011810771111254581001010118483538108321110389469599410111049786911259145061954829311791104989050806480002834106254163971151617998723194148992816118105210711871074310011198110901998010886730310117411811608711243032421112100111810961081118810168598452864091105830121038344384568119111011101\n",
            "inv_loss:  1008.9725341796875  ,pol_loss:  -4.3391194343566895\n",
            "500\n",
            "111065846104212141111568010911911711771404687910181731882701054811908811096411782108110459148109871548100854383512144541183492394810911873109910784134070015394404137311562811101141080845041021042777200110147512119078641054871208495803046108011114848114118904710411110510947211871191310010887388911111744310111164108639238511361014410841110594081180041071199110147771119451118568365183579451181041911118210383110983319948310543093610465089610499639103901105782118351110871117481198986001010141110239639911801094902861025911100871010484701858610643404447110010108671024114001110104117108717801111\n",
            "inv_loss:  992.9100341796875  ,pol_loss:  4.102417945861816\n",
            "500\n",
            "534844384145091710947609321111110863761059383131774543114710841867114118777761162963343062113811011911041011107108310511011810448111111611011911115118018751119111108108910894824888119981040121081031100118304710384069002311168888055310741002111011639108810729510945168790418337976971038078860590019385101110588419621105810410341010106415680085151197117108657419910461278875701846283100877810311011481491107581408962620311390087181104645106512410110357481051031111108411071155143221080309849643187478083898387717631045101633085110088950183118116112086518847169481711411352101561730047111181010\n",
            "inv_loss:  994.60693359375  ,pol_loss:  10.000481605529785\n",
            "500\n",
            "21722418100111641171011178685928341031111048210711181011216910119115431810832159310342114734622891001038109210536410411108101543847948118510981918681059857411178491041161008108294948101011894800311583101184108107108710116410142103001191113148010711367310106112788102496119448771047810311711214641081150875848111110411280868411711103190111015108111114100443101103451140481040201294100101140689415451095801415851116109441911515337821091111017996413055119100748911003441001054410479849391010101104337221109407510111398981310116971112805975511847124114687910471011479109111803101960471111111111082410118441812886\n",
            "inv_loss:  991.0196533203125  ,pol_loss:  15.33261775970459\n",
            "500\n",
            "506656891095790113115831108829331047811101010945414339624186110488103733049117879100117232707102718119101105089511111331906733738101910749863104402010052891092411750591149610711702911889659577046078704730711144117333821003300104108841044410134791804848312311180411054389811088741181110101111641103101882389182442011065109101133111500513102870121066490522475117710783400710210541141091061481177209302978810459301107012111181010911101119610610510680110888048744131066344751411790118151173511331141133108578101145311107053111014581003211704103116783310891191011218489675887447813718108901332358904\n",
            "inv_loss:  1003.0440673828125  ,pol_loss:  13.4488525390625\n",
            "500\n",
            "10010011817296442678068471101111111074510031272847510110101090118101810191118811877781010541068111339710116393061648253879051161809114921111281843111111311073881112109646631748601021111711100437794901030108110111027111111111001011071069745854403139538481054756301191810999191166075100536876301008089021783501845410811610755100845666510100947610114118102115712402118447411391308710511844990140119111521043108311985010678836006783668101411361131049310084975310738101878711909945801139777111111347116810449881310506438313911861786101177560888860339100177021003843101101021171032198625977488251049420\n",
            "inv_loss:  999.4287109375  ,pol_loss:  11.695019721984863\n",
            "500\n",
            "68567984468810311011610205701319218010966979161081131082611401172310781171482289010311711410104181411118324118879117857485102841138878211611011041094101048010882881105184483110881011181110835815113811033022311786115810608411611682746310111010033115331089116743118441102051106115881164110880282610810102841051197886310090856095411411387234368054281812521111803114610498931100114313004181049441091181189508452430499803791809360111841198130301084174311101541066771857594210279048289410108871014111105111961014989101110471641123317117611098081114149849111110036452410825361428996536752351101011808\n",
            "inv_loss:  995.1741333007812  ,pol_loss:  4.382809638977051\n",
            "500\n",
            "710041881651191089883410088179711866127109401031071119018113511630020360887910410111161187213701811011119410189903981081551178321095638868879310982311711151121112810118913510037911111077401642109076910711210041769107174110810704911111006410309030910513537891073023861104711821111037448721002310188049114118547748878117110111118185761713107139844211046157145528101147601084101441084837069869104011409114091104801113101018108100100957801105107532391028136119501011077551088311763837881113126771044001154711210710241105989905351092480810259479199583401856812941119368064840078380107368111033\n",
            "inv_loss:  992.1430053710938  ,pol_loss:  -4.467841625213623\n",
            "500\n",
            "1177106114082101011211810811932112534210110214811004101599847407369058806945828411293808110688677111710101010913710670951188108411899900853311795460107611974881009581011256841010514002855610843508424311210112100124118419897871010731111048113851091170210826411128194111042777809261038380611898911481110109101447880811111011433404811113148102130301110195030101011041088266374818101107841211041110785433301113488940573090143191047010102842880445810373394112334229013831080111018818021131010106848411109746803211047202641010811017532411110111101424201310421188990874431117610043781419808210211310107901230\n",
            "inv_loss:  989.8622436523438  ,pol_loss:  -6.350737571716309\n",
            "500\n",
            "11336881112817504191091006831761071087707664980111934156187156917703903881111108104881019119111011910385103012118991111911677610113710104411750600622988101144164811731193304799231421175847881093105101113115924211053108380928110344811710680871153930349895910841085841113103389199949757950311431110710181410465861011232111668911111010101011110918718117300461140961161088731010114727115264101510111137111303939897118541110168141130010391101023251011357397410869870310107407100910146388161115304494114170388841110525630108113953062233960299068224111111421191187841174083953259082851148731815111731010101095\n",
            "inv_loss:  995.8455200195312  ,pol_loss:  -7.101931095123291\n",
            "500\n",
            "891128981010110110891022930807821115610142458873974685436360481891188725610993481041383740111155091128340311396111890432119731489591180053102010911444854101111732396258610911081032270520051414787174108896468099198308119971114111200110111511118011991032853757869311610382810311963171001042108210811111107438108756871185471684991010161161152431830082309031971001580408041102318710103106555981301107710086013505386118311401051078546672141001075132614103778171031315714361111060575711988109011101044710108175041944886870149044107881111110140847891008819038311110110329104319523101011184794\n",
            "inv_loss:  989.2716674804688  ,pol_loss:  -8.137316703796387\n",
            "500\n",
            "410111101036863908902810888105910104011104111008310431325104911460101042585427885101117011111025141792448109107789101160011010132533010911141190911101510101025808171011724111010475118825111104101033703076303100117515008757747116910460891090370400962210988843139761801131179930102068898184113311115910110310066881491151010237113231174136590888041014731110598171111551111081169106711983108831086739679589041101093084391080069105380418115614601587911711161021010272787189961156081110549841011114888611425188118107738441461155038107594101123111181191001088131010810410011611103581161579981110549411483811118720641433\n",
            "inv_loss:  989.734619140625  ,pol_loss:  -6.8087286949157715\n",
            "500\n",
            "991038100111948810611656803911108310327318108101145041064830107211010109400933811018844110318861102710774150507910621371117358171168011948248371074111091076087154510141371016697831610100685499022711748101110281028148090484203107301022888311084271175911811110773050448479101070480310998211010641161141071111044311046510105114002710911301101838058391699821738711063110611450111984310101191134310111114435107706787110884011112791861108810111110110105107981191061194811510408081154050101173114471191015111180443587381102860110156748941011971085101889777741061113110942610080108801171045945101011118892780309\n",
            "inv_loss:  982.5469970703125  ,pol_loss:  -3.931281328201294\n",
            "500\n",
            "11108035838108471111923191159116586111041103487978281113185229407211010115552116810111617114342791111921010044851130710116577361117110871010060110101155110909333105188161246103431011198158081035909089711233107411411213390105119110011014811741349826107168728104111151009410033801141188101107511114369260023624763113423610711610484323281001141023101131110988100103711111017030778111068074811101858119103379710318410118658311128111081140110710109610711234940938011481111103423823397187781105106498121928339716041010233014017101090944508181410113117070478434164181081072113107115421010111126571080131168738108\n",
            "inv_loss:  991.1153564453125  ,pol_loss:  2.145031690597534\n",
            "500\n",
            "10831980010118051119113210210912551081024119010895032333610847110101040111171118410031861178081178134811344310101166342804451311413991010011104815393310840956111790983175116256610971074707133746210728514101128111248510211083141113110117110113744910513880110171131044470411103444592394563448031182532127903971091177510683601740144817408943114869410046534251111069986079751211048105571031283511643118511211114158112137077491111103067615186810106061116109110781033108711021050111110101107176074610014851011970787974478975538734112011101076935108210102211059311811018123519135491243678061749759011\n",
            "inv_loss:  1003.1673583984375  ,pol_loss:  9.133695602416992\n",
            "500\n",
            "901084811000811078841091203511677311110357440385118810114117443711843013103401033408808393117400810737511601101108102995893106305229810871001027010874471010104410780109488514941001006645111323310910312104111181589853260104431043410311353364477551143111105451119101118104118349091168631104901710610510611604117453041787504113459187108991013106711072111643655579610511111111011847407711016101010100789381188101041043459478113438100821047116610811046101049910210111140911102105681157432109581063111308844115913552106710787881480010104960619811380001194479810310424119587778083181127730314031001197621023104\n",
            "inv_loss:  982.9895629882812  ,pol_loss:  2.677807569503784\n",
            "500\n",
            "875510634010611758108080810789799607406821131124312113539811007101155927010114378112381188747101047947111178109927119118101111978013105909810310738711788081101185488048118711384991108310017064910671064158718010741047945871194181098737101152103837712100100561010114410840374210087541110341068658952410411224883111810713506958086078354325434411114397126333411380110792292610621107810310805115191110846104703985116601110394111118199111110310109818022763278101154109697841144110741127108961101071121008041036116191131084810427100271185770316127145216800933119014489921141116938895298732910105211488\n",
            "inv_loss:  991.376220703125  ,pol_loss:  -1.8813772201538086\n",
            "500\n",
            "4104088538747459711632991024458104285287110889743766941111038840106591139367810832561021010727116113134114101078641187910118172349102299340684016075212410115101194104141111511751071091187338101910914113011138981011107631065112215310011211343811109611741058845192114734810011071111316641081088971104996112490810244492010361393361041110054562845721011041011506110128051039103829753301104110748411150131179910085210171051371002824871708247927101021026710793659693180394378911811211981014110051102051121010783286114811101810941113711831046108911411908741311038108118101010075614310109871141019821108821022171139\n",
            "inv_loss:  996.52490234375  ,pol_loss:  -0.9447157979011536\n",
            "500\n",
            "61044373108451087781311193111010383113791911101153620510111994881106610710100608410188183281010934101171110113111118411581910411410371141034443371011010101721180897411611482096102831019710101131008711785411898119001069851600100005410922700731001161047118109724116101511791028101001066105589845751010031041011084010553094108017501022117910111087847910881110811374117711888625111576810721241510111141023117091040110109087405101045911885067710110941000006299440241035521197881948202159911917703609110085899511402512745310397011411011110871170311988754006060879119875815607857110083407437421010104657623173355103508\n",
            "inv_loss:  990.2521362304688  ,pol_loss:  -0.7889237999916077\n",
            "500\n",
            "391810811109930215254114844910911183514862863377691831681143108192710941089101008671172478411014848102610996741958610582411959071091710110079971014011114875158100117611315948460101138853871477104387211121141173405343008757041111041010151181010110113535181131870851341081081001617088480611311783111101043116418010111731044111764113815291011227080114410828572381610518130358424119018810511119481121133187384410044360114111066311185101111950974894443275880481031110478711711101511172411068327161030211310103924111048917040811101951831043710101037541085117610811087107038377383295106261000118173610115136\n",
            "inv_loss:  981.85498046875  ,pol_loss:  0.3585962951183319\n",
            "500\n",
            "1186110681041011110989110111045911852511104553113195113145784435981101035108101024511278508842109811191011111981010588101115741361077881110591103818498101002611006105100591011106104500833341111060113120011154056878118241762051061904603112112113811118417109285490776485100373099171112071187927964208472091029348297041011111199391121090101049911992788576693105820210011979041029561757051011117481026099317411113101107108109461252086111935002336718109291198811410116341171021010811111954119107470764100313128101001941427487710710491171052116093111786400232447118100110231111311495858833813105367410059756101165\n",
            "inv_loss:  999.5453491210938  ,pol_loss:  2.476794481277466\n",
            "500\n",
            "2173512771010511826942103731401111100108111011115311310038829984112645701161196610471385587841041061006421110652211418918618849774829112314111107274743670741103028151500930111034030386047187811487611105178558238547111011118210804114447783411848904931077110411108119105101110086841151061781079745791539667510138111306886001101910210808838459113338811711174010018170744992451385510410439119981021119254817930137116111010107895511910550021111199951100111311841511057109311118680027087912266437731011098871214110911103780661011086888101045404281007101073578103749101107891119101742010814647449510\n",
            "inv_loss:  990.6073608398438  ,pol_loss:  0.5054688453674316\n",
            "500\n",
            "911811113340115101191091041395361838381004897735710111799314809750957821111210107101002511732007019115791019741094317481701461053640911677102515132195576810013111003601091018741101483293210643101191102810910381310116298069480748811114117311301183427711310510425111111078755681756278118010942868910911550439711421031110118153111780115150107810111276111115109051110548971075441144105196310328751058981197960597111010101111211101155508551117310108570909278114329190811540171061111111255664118076228351174511100010101001041931058811210571171784160011941011649041111045704684586871193871335311061155928988\n",
            "inv_loss:  994.2308959960938  ,pol_loss:  -2.8897271156311035\n",
            "500\n",
            "11771110192554070151010100778561110401107109649115046752501111011009109101153109111110848111174679373861104234040111102711110106101011110151181481684872110106094117849637556311300911019071109721071088554533357031164447111135767111155178881011838051110746443078647314528119175946314108511991011105101064008110818301037191123063141048269310734310073510875307747119010251010507851111105288836191110961084879568975803482403104712101002223818311411103104261126603711311111141125119104111110058104610271181111011110101115123118103011188051131697411861004211181073110310410100409911167773301111110420111058710811710010114325041\n",
            "inv_loss:  986.2098388671875  ,pol_loss:  -6.522275447845459\n",
            "500\n",
            "108117777111110311105101111041981723500711811018068111011149811585299876522825448164210610087103911834910101092510734119117932201133511107986111094291110594451110334351148318075111778113104101011314111935010116998800477908451111091144557111691132986111080102104157438048101017099101000192880011191804488917042011115101181178101191037047111096414203718333830111111011093110402993118520588610115116101193070641093865405683456411118700521135115981174040811159897101881181711040271103101179110601088727730100768110101104611114680936116891169114952851128793581111719747591103048981000033221281074104771107456316\n",
            "inv_loss:  987.716552734375  ,pol_loss:  -5.914804458618164\n",
            "500\n",
            "7114910281610111121081110081109112299117710448621433444108911094063145410107111611711116492781114299101963313541191100146107110105281186191071137911230210710141101104688730112835571811138169381905341010911401717114064119778116311711728431108738735894103188111034006910618016710441397090143870801005671114110810442248114410211044486124110111073867111148907368671301131181210077408549811981061006010641191005100157504903830105531112046031011711652301075764368419051081910982105858461010108000541162510150195071378718118711891110775153857060111160717001199527900683311508494897223921490402663\n",
            "inv_loss:  992.685791015625  ,pol_loss:  -6.07796049118042\n",
            "500\n",
            "195708410804880569186739100810610560118102711102011041046801071057081135969386511097827961101101105710910447303513970100211118204071007071919011043141101092319147110685109210190478810978891012818101015809578099310051197138811545011111149954691011031011312426790610018117071058067410350103868801178884205968280733947721114148101988731110411011101061163104680911810082172039484011311417204491710491321116034864811881010194104944345710091912645116479310697451919101021110283406117448981011108105117795530511110118178011111171131156157108887754108434631181191077811211111498619911691008101071168041897\n",
            "inv_loss:  984.45654296875  ,pol_loss:  -6.6159234046936035\n",
            "500\n",
            "010011610110107411102011414710010110981078080137085404871103310113471107110113591003511110038115310711035876103772643102119807051178101147792109781818319192288039911111411097100837108111025418101048093748090420957480119549141109311101010681110470391010103711108101318111110691111841054030107502349801169811010113979286211118118801054840670854878536531384911281410048708773117310683223841100732921348473110015111101310344541540107101211031140102309581093931076781191183795517751139810041014107980438211091097689111188341011841110941051171150410244748631055411766019611011911501110811111351090771712057\n",
            "inv_loss:  980.2605590820312  ,pol_loss:  1.188838243484497\n",
            "500\n",
            "71110701081126978981811047652116975119410881111362378731101078146253911281181033111100118301182917110711114346110110010791810168441813046515757181111827671460411180491011195630017117463914563511271031011810113311710532450044607796825108111038514244411311882271901115119110493649878118104411778315115589702509738410192111061088424397687107772701010856411194339241536310555114079338115811010431103410291107401111110511790173507010751048288401014911114431767211256480110850071079311110648821013513041074680410471144107771110713101013911081341075711117619194058316105101010010503463799719388417032\n",
            "inv_loss:  990.536865234375  ,pol_loss:  7.194838047027588\n",
            "251\n",
            "0110590164379809410316821064115100746510108511011114011101186818831131481111410105111141848104711105107103110431121136269111034201173811185141054002104100897180109282141110013211981336111127160591601000702109354131010210166141190501336179798109204700310411814520118493131041110781797284119421189710106486\n",
            "inv_loss:  497.4831237792969  ,pol_loss:  5.092296600341797\n",
            "500\n",
            "42491168654610111133100103311081159014108607911084051611184418841311910115788014835001008438101141011074104765938106464111141010598101071137310308667105906480570781122166747057075581011119901781113491034379431121089181010589831082811474106106571071111106010810001164589031111048114254071011510103804108170239231040033113100289011467103936971131010110047885667111111138651137118402528614166612108251182332527310445110061001164455944862114116796848841790610871103910309110610319311105251034285671121033735675113779114612546346410399418061407081111111308787561011111431113111998811117384626173944341151009\n",
            "inv_loss:  998.784912109375  ,pol_loss:  7.59529447555542\n",
            "500\n",
            "3068514118411840438424582791010391011010611137000100971311810101011110100731998110995981419377161420356279071012610464588712565731108079445008971099786540074127159111017811116802104102103106306211287186920910324181111671110143486810101031101041102572018218711038184390115100119287210115483001111010911101286139986102117703065538103371101031105113115601011477615601047970831155526486071113356101001091107648111031007111051910132531083106104313511355103502521181827750859927111104180411711011931116219351039113710101081848101137111099476115443091011159100820703111810241011211966211107519389910610131111\n",
            "inv_loss:  992.583740234375  ,pol_loss:  3.855639696121216\n",
            "500\n",
            "119381071940111111032109429855064112078375411117104698837864965497171115101149681024510965805811399751140944102111981010431192101011074821058010106148761191251104618010915182951677611105112981111980117487811118103310114583881013014116193593544411112411758334397462211100219444531048631335511848429710221480132784610019110107370621095107011151086283114111170100181138811058100311973094861911083311720344410011841217619311040342479201710820384611110438810161184931048411847281011142438513937101191021401006440555121747111051094561172650441011110413101081028478694184311739194111011120131010863011\n",
            "inv_loss:  991.3480224609375  ,pol_loss:  -4.1345977783203125\n",
            "500\n",
            "810105578940101010411109594114366361114670407114191149386104810111188611181096429106011821042873710411101087113441217748411481033677971111610803104287310972111741331150780911098911108411110101061001430670928910910711890107894034051091049100814100011908101118581063118718410011111115103391028085071151178309310108331390837100709846116110400045729301181099840117338273107351118113059946111031184481107275110107114447111115511169710053621490810581112151084157511116088960149558011751441855411807521034108011769788095101180010111003910108369573474410571089444209020466111041005487825978101382665191565811901011\n",
            "inv_loss:  985.2535400390625  ,pol_loss:  -15.752588272094727\n",
            "500\n",
            "110751951384011784407109904685989836731050993392261113344146411094104701514100030102468668101188210219578584848851101010994111481050058101874446811010061807211121110106740074641014920391101052761154105440575311187689707918997114910401001196310028757947110048111161080811784111148395320836051410705876415544981135511105596481157628572100154021171964513310106173675811183053991414217811411407112509947101089851601051011158119801011185384118117780118634411141098451199471511331031111011031001093538911141030481008111111108993472597941131110104710751079710111060390101439371180148101031478410105\n",
            "inv_loss:  987.1826782226562  ,pol_loss:  -22.928464889526367\n",
            "500\n",
            "1057371011671010401196441081158307910411998801101011010188345291118666243811110387107481491010287711034109110863183811211951011901023745921347118077094441101061304110119117101010431027871671411111801041010431131108010433451111055306363214185211211110113400620434204991110781006483710171040871010959030075117236758157092104119715111090108047118108074711762904738410110491112311171884083107310107109820112144062511101111119811975887784131111047812561151043101101468649011113630471086111088111381051161010638910387105361323108711011218905703311153541139584827581311511871010701105130111368380410010607099\n",
            "inv_loss:  986.3410034179688  ,pol_loss:  457912.40625\n",
            "500\n",
            "1041108316430710411127822482911141090944118987107808101136113453017071071198193481921181181187449098310865731091083878410914983730302104410741131131651094797470331037657374408911789293791030536117611711132204489881922011119141247568110427113410701109809546151211553804451111118692117118840826901031131144611073106842524441148801174381035103116111112116393116113410901034531111044190747081011210610637801123441110637103110810079100121061051061081043479858168861949340681033108610193931099110234096371087104228311373210411478268821468213690910837427880111000131711119101108847787690651441\n",
            "inv_loss:  990.8549194335938  ,pol_loss:  -46.75755310058594\n",
            "500\n",
            "610115437341087310166102100251032044600103616419111039343144630611081103711011110874210911970911831607010601044111065115110941031113790117111163494910007104113991010231101110310711337184897335010498111061107111101054666791937109996310114110201091190100821172154098641109410109093111114111071087911669956410843108310841108118743804111064131191111927911083101000101010446118103035410404811900164360490747571010781310444011101140591032100756041365847114010211810888023013934100141047118027107111105011110984941054711110107249023673116348011486031910101429381101038353110599910061610302568115410741677031079106110\n",
            "inv_loss:  992.1723022460938  ,pol_loss:  205337776.0\n",
            "500\n",
            "41810464116450468109340137905644710118418964100404101857107004673464016403179118476116111011061088105411769016611044834603831056156671143096440111011051178341199097464101647108926388586668006918933490291047840901119910111587501053967462908401161152940410678765074011104111110273087171107710806946311110100401114731196411071049310207886411611100123710394717381113198117904101094011063349791001410106048030091001119431071175665611721003464434109244101191007961164101004711063011910519631111914184661086344101449111103864163991016484101091143469431114411118016701044390449761114446041111704\n",
            "inv_loss:  991.6203002929688  ,pol_loss:  -113.66783905029297\n",
            "500\n",
            "076711010064618647670871111765104400893610941051011113100100904401041014660600480824431064481010631621131140740116131484484417410111010400774229901141163914446403011104346910311171049660962490110100104433111910442670611113841171710446421071410941063365117410810781140694011999490811747404742710004410471147710611741003410370100463118769911104414961105739101011410100431147914111194061009100106731010110577110344621098111761111010610107444104101190947790411010410971890050606820709476496061044724432909106711400441090310491001192400910400491045841091091110103699437664124104881161083746101001141074110104\n",
            "inv_loss:  978.7913208007812  ,pol_loss:  -139.15415954589844\n",
            "500\n",
            "2510280101111774768989410420104141070430791049194116476901040048111401567609774111049469410110374648847099410431061841010110104010387010339011944111900466079572431144124468611344810109102664119406910563936774741045681041103990804106114410618163111067766947931111401370468516411061069454303296114571146311467341095110931749104214147100891110441177109089911611844684990106610610800010104106441041110101017480676954645170346499611093749611100263414909101110647303901059627031048781110360954113471054108444018601910106410008736481031104781011190064225101011091149041010903846411344707111749780\n",
            "inv_loss:  972.8987426757812  ,pol_loss:  -153.1581268310547\n",
            "500\n",
            "02034010177701411107421030101109143471164697104960084849101169141441110851144873461047296041140437746046204304270474740044310910119816320109114164774114391144410193961196010909304711042724851003034669497710774101060049171095634011115010841011043684237559034597491062811670107410386861171141344646401196409646610146741069114916441174688649847106009498604100644674411647464139309410046465774104783691149001791040010411177110263079794499301061101610405449010496200247118462751040211804997110740339106108861094769944144686636103069629444111079041141047791471161010360379141011\n",
            "inv_loss:  968.2055053710938  ,pol_loss:  49450176.0\n",
            "500\n",
            "4610179210445099401097421113114610449843108110118111156710106911110704105648711101111010113104810211091014741011940109441010141121641000043826104741941106918910810473061061110810603767106461608109969494103077961065114211106114941094011114310090446997109925403644611010211010014106671111349524070461003710704441040941041194467498496310241010440411010610111083410104064121028112459391107371038070516910191141040760466350400610600611111010424610911461124960010943974098764911110411106114110074405699410764460761079961114413004894021044017469141007432938644064240104110110114414970111031070410410091296111110411\n",
            "inv_loss:  959.55810546875  ,pol_loss:  -149.8185577392578\n",
            "500\n",
            "44310464104107281111421403306004991354119119910058391046108396102507107249090308909710944110103284710910190101198611101391147083610031041624602611310841108111131071010044118696112611811837911707844771381131137971061450067901032027874210047667590101060210660738001131011464116134494023103810100477464972904902311110404831044114410106411849109011434100611110064634141080999617011311148691082117354210643679937046431110004844918704400110111110113405231139440119091420705786011129161110710983117127806751049891113684610443669370731071094111104406151110411669354612349113410426911119011111119119157764\n",
            "inv_loss:  976.8025512695312  ,pol_loss:  -150.2164764404297\n",
            "500\n",
            "048045610616864011411880648101063007048473144722444014603411026437303810989101145619081116304310911404891100117641010061144410947117819068944107561098807410104381141107206774210400331110711811107911991050959415644096769410711111164018319009004104103107080918499837811108321011011115410604945954100871827101036116591091710101011104591108936191046731610171041077704410814410211491041101044741032410414654741101101068441038611082607109104118700510634674760984611910911693941710103310101044006106011434101197410101124045944441162344930104086409040478867421121147790100298309061097400926101682100241110\n",
            "inv_loss:  961.28662109375  ,pol_loss:  -127.290771484375\n",
            "500\n",
            "5147810510001188310344811894259737843111547763832372995211108383310311920420110139610411484916839848811231107132186107360481141385006071116430889016471011035611606353361000947670910157118105346010424338845849111112510750265989478114911014468273388410333380369610114101106103311411503701197830104784011093114461040140110434810019211421079100944764160484296881055110009073971631810344613890887031192054105469048113024080898018011381149106731111118710911741063648640410411145118001101141154541037211831511111169450389114044115288119587384110114630106074838103115104911108310255158710348\n",
            "inv_loss:  994.7786865234375  ,pol_loss:  -98.38916778564453\n",
            "500\n",
            "45411070970483309219079113310847884610730081104488107924004469170141911003861089903103043428511140590761061052881181140337141000101010813591501092111183201101674036467738911103141411301130578839411111803811376166410846831064184531144701421008103561011031110561061011938811391110114106891101233411605008310818867853001085487476078373559197218797354210416911861509001119310510710102650132404111104819038003889503718311635860372383501101983866391816913320941045781365710118001130310481310451118110900346241068101843316611313110927921061111159118401339079118341080035204104210108\n",
            "inv_loss:  999.45703125  ,pol_loss:  -56.32169723510742\n",
            "500\n",
            "1151301083411410381110933115061031143469610740154917964854485331993361713711828103510510100775018310164006081003485725102513101390301110173517818331101073753460118853335488993081838113810438069688107510332310550210698052478101061010101180830231293108174106165331101084336303674752121088880601081138353110853581157836281001550853389231154000860532433933510101310810337116463331113862789779113310808805119030091010820757383103016041031135801010511105184439233740709101001641011000110101881103011331851011710101136889110043401141194119511075861136838104810881830170115168113180317\n",
            "inv_loss:  1003.7293701171875  ,pol_loss:  -17.7499942779541\n",
            "500\n",
            "493138735113135811118748261477395341015635131100231054846705883911359337333104101183132510011770108118384341190237611474381110101115907081036803610405284408811838333390611128151308896181168315010621575878401112751189116110616831100011076800031053585337754768257110112217109188238528208600751010838941018333323101430910820428101051713894188315171910101905231101111311018601733486257108733018810588301113108011411250311943103710811361111003963810328108580273108101111351045388313328981313910101801468001169367457116087010116872163340102630411104083485810081059820418\n",
            "inv_loss:  1002.0365600585938  ,pol_loss:  11667803.0\n",
            "331\n",
            "3087104858011107280118153136310554301010955041310481861160061038004283488106111069310510730646181838119118871198034411351181818368223935811007305310661610711113939387106351340104353280051137421108531333303860151191418116589100113101300537106773331038348812431180382568396060861040289408105811262901111184563443310612589510110122109138385523833358275181982311111381001193\n",
            "inv_loss:  663.34912109375  ,pol_loss:  15.540072441101074\n",
            "375\n",
            "111539710179710118460033153104121281132130801053497105101789810724333080334436483109810561011911610106293191041085103883831033101183103526049284549423173100908111065187340116021140101418254103683111611083809104311168111138374206068314114186751184030108862011102141015241015831141333810073271015891036311011332875074188238711118091019834108293501183705310021810114135151085663380118397310108874811281014449235531062418138389341053336710\n",
            "inv_loss:  757.5733642578125  ,pol_loss:  21.160202026367188\n",
            "500\n",
            "32811101010085744141130540810103433801183828103108833887101113865311097961026381110850511880658338111080111883810105331310118110123106301145551071013610511488100311198141110143416831015111001131010113333701751080831115434841385181413011334850878839610108839081112860545971038390877101123994108331061110151109582142811940305618884430878724411872119315383381001011163811187115311713841077344813921080100503103317135803388510801101135811331363305100610052801039035111113111103497104438981021118841011421934108130310731010388101081071071151013109744103914108626101103311026400335050710802010\n",
            "inv_loss:  987.6898193359375  ,pol_loss:  133054040.0\n",
            "500\n",
            "1111110017361027101010346110310481562103496870031101101152767718158181889048118129610114124163100111113538210801010118316183116336838663108581131139107411108101141836355148610219221033101024771111580150180231080105106995108803101018301380489485141191020427391080481000119074103333210498910545044100034061105455161017693103130310933710351061026810381093104433295280383363010168081864101110010311744311540071911021164161803173156340583833114583111037043498106600304335069331134118470681011010504104103037811081101005611058384411910453041010113801111474111072808193311163999103181710\n",
            "inv_loss:  1001.4839477539062  ,pol_loss:  2517467904.0\n",
            "500\n",
            "61001049108685910206109054077401083378810180011218010310111100711415380003610867411141010188484106441035481101031708113384103047403105581497101191010716600331000110168654611403646431050310693604911860410193019410011610342411623331014414615551101043111924160100110440608727068210211721456083110081026168101841511611348271986780634391113831591071011638401150936008093816111021511229364104311100004611449408101511011018403472106310112931011160736531081052374711101044809106105794143869108481310120103009101046111013835838118108841041014714484836111111843906943118840410105873111101083101009810105\n",
            "inv_loss:  1017.3614501953125  ,pol_loss:  -49.92119216918945\n",
            "500\n",
            "4098674634579446112639701006410646108710101121378101082991016910981044110011101017773543893101361574248109008347101011393311421510101033109331010126048708910103801074100571210331130998167733104491948048538100684824667107241191114410110943790057111160681068110631102733683336013454143243864010031926843110940099411106933011006110111910830469440894101119101341102011179949187506415336624111071561044310014367791010441110028004693008421173066108411962189289871131660047477083109364196949113910047340811010710100041010613637101103393101010331010011469431034101193306646748663343014804\n",
            "inv_loss:  1019.1694946289062  ,pol_loss:  254833520.0\n",
            "500\n",
            "644901487719113930741070106299031414511843116938811186409410334710610102101636810064031110910104543845761679106108449441003693491492234901101923431099904981646133011240438696404038123393300063426116208484820115101078043111049910181749318865161110843748010004221003810811141049604344710721164110809441444299155144109634104570706404444818491149471440410436449403403166969611181042644943608161161011010279210096446083104361891020111309661111049109041069378474116640106644432186411073981010521061146071011697380714010639109299791106194011961011020161051103476310673464114438\n",
            "inv_loss:  1024.0150146484375  ,pol_loss:  -105.4856185913086\n",
            "500\n",
            "0624610916341110461440108101844494113141064040169119941040367441010499101061094949411414943061149641110410319440931431140986864006609698964008410400326669046711069621134100106319033101146006105147383408440710941246110454403931010112648101168341047981091010104679104644045391641489464911114611044741190349641010460107401044646103989941010810411840452698496108448111049963108606443610914383078301049391341103002743841346397010874237741346361464271496474941100940966666104610907053046741167664444676044030116466690463100126106016148009106302441431323934806530207486647327\n",
            "inv_loss:  980.5260009765625  ,pol_loss:  -142.92071533203125\n",
            "500\n",
            "4419471048691640117446649774660648103644639044119440441197044924878410018447934471101067441069104211091666446610414444341117414610340946649064421176690086001667942864247101190699464101044761046173461074061644649111610411076111443044101165446734010162104241041067911662490466890842597643941144613474401039497431048106161060789441161099106444248900346400447440018944441141180100946600641744499444461141643610844944114310104431083602741041011461069410159861110126693404040446010161001091068836101361000990396664679084900643673376711364671346800336113109107631021136446\n",
            "inv_loss:  955.8529052734375  ,pol_loss:  -160.608642578125\n",
            "500\n",
            "30348401311646171018111106040119319986367934497460491164104104346444464104966601110066449864476244004108849083449010707106494379400484104496964994910410910106407407611097049114469010000100604104111445398464994100999866684168179662104110466496100446610311491010101108304641497774644934604411040943798643441076790118474100631147910392954106106210910447666110409101166044910109584906774067001010049066471041046439479610426644634964114410160446846679064411641010000139610901401066681078106418374096101097438710970216441034409946473103444996093064110691091046610100429460116\n",
            "inv_loss:  931.2642822265625  ,pol_loss:  -172.58050537109375\n",
            "500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6e125978fd02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b7e3ef6849e2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, args, model, optimizer)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0minv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minverse_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mforward_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mpol_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0minv_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpol_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;31m# (inv_loss + 0*pol_loss).backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# (((1-beta) * inverse_loss + beta * forward_loss) + lmbda * (policy_loss + 0.5 * value_loss)).backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# train(env, args, shared_model)\n",
        "\n",
        "for x in range(5):\n",
        "    train(env, args, shared_model, optimizer)\n",
        "test(env, args, shared_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV2MvSK-aL_"
      },
      "source": [
        "#### save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-XW-LvZ8Xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "# PATH=\"/content\" # for saving on colab only\n",
        "name='model_mario_perceiverio.pth'\n",
        "model=shared_model\n",
        "# torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "model.load_state_dict(torch.load(PATH+name))\n",
        "shared_model=model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6PNxVu-W6K"
      },
      "source": [
        "#### video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fkhEcBB3tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# # env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "device='cpu'\n",
        "model = ActorCritic(env.observation_space.shape, env.action_space)#.to(device)\n",
        "model.load_state_dict(shared_model.state_dict())\n",
        "model.eval()\n",
        "latent = None\n",
        "# torch.manual_seed(6)\n",
        "x=0\n",
        "\n",
        "# acts=[8, 0, 3, 5, 8, 3, 8, 7, 3, 6, 11, 8, 9, 6, 3, 8, 7, 4, 4, 7, 6, 0, 5, 6, 10, 4, 8, 11, 4, 10, 7, 10, 8, 7, 7, 0, 3, 7, 7, 9, 4, 7, 0, 4, 10, 4, 5, 0, 6, 3, 7, 1, 10, 7, 8, 4, 4, 6, 7, 4, 2, 1, 3, 7, 10, 9, 7, 5, 1, 6, 2, 10, 0, 11, 7, 2, 6, 10, 10, 0, 9, 1, 9, 7, 5, 10, 8, 11, 11, 6, 1, 2, 5, 11, 7, 4, 9, 1, 8, 3, 7, 0, 1, 0, 4, 7, 2, 3, 2, 7, 7, 6, 6, 0, 9, 7, 7, 3, 11, 7, 0, 10, 4, 4, 3, 3, 6, 7, 11, 8, 1, 9, 2, 7, 11, 1, 9, 3, 1, 11, 0, 0, 6, 1, 5, 5, 6, 7, 5, 8, 2, 10, 8, 8, 4, 7, 3, 1, 10, 4, 10, 1, 10, 4, 8, 7, 0, 6, 6, 2, 7, 1, 4, 6, 6, 10, 11, 7, 0, 9, 0, 7, 5, 11, 11, 1, 0, 8, 8, 2, 0, 0, 9, 4, 0, 5, 6, 6, 10, 6, 10, 10, 9, 8, 10, 9, 1, 6, 7, 4, 7, 4, 9, 11, 7, 4, 3, 10, 5, 3, 7, 10, 8, 4, 1, 0, 2, 1, 11, 5, 10, 6, 9, 6, 5, 11, 5, 8, 0, 6, 11, 8, 6, 11, 7, 1, 7, 0, 8, 6, 9, 7, 9, 8, 6, 2, 7, 7, 4, 10, 9, 8, 11, 2, 4, 9, 2, 6, 10, 8, 8, 10, 10, 4, 7, 6, 0, 10, 10, 10, 0, 11, 8, 7, 7, 10, 3, 3, 10, 9, 6, 6, 7, 8, 3, 0, 0, 2, 0, 0, 9, 2, 2, 10, 4, 6, 5, 3, 8, 10, 10, 9, 3, 6, 3, 7, 6, 10, 10, 6, 10, 0, 1, 10, 1, 10, 0, 9, 3, 7, 4, 7, 3, 7, 8, 1, 10, 7, 10, 4, 6, 7, 10, 2, 9, 0, 1, 0, 7, 2, 1, 2, 11, 3, 9, 4, 6, 0, 7, 10, 10, 7, 0, 6, 7, 1, 2, 7, 10, 10, 9, 6, 8, 8, 10, 6, 4, 2, 0, 1, 7, 6, 4, 7, 3, 6, 2, 10, 7, 9, 11, 8, 3, 2, 9, 7, 7, 6, 1, 1, 11, 10, 3, 10, 1, 4, 2, 4, 4, 4, 6, 9, 6, 10, 7, 8, 10, 10, 10, 8, 2, 4, 0, 4, 1, 10, 5, 0, 7, 5, 1, 6, 2, 2, 3, 7, 5, 4, 8, 0, 5, 9, 11, 5, 3, 10, 10, 7, 0, 0, 2, 7, 1, 0, 2, 2, 4, 8, 7, 6, 10, 10, 8, 9, 6, 11, 1, 7, 0, 0, 7, 10, 2, 4, 6, 6, 8, 2, 0, 7, 10, 10, 11, 0, 7, 10, 9, 3, 9, 7, 8, 9, 11, 3, 11, 0, 11, 2, 0, 2]\n",
        "\n",
        "\n",
        "while True:\n",
        "    state = torch.from_numpy(state.copy()).type(torch.float)#.to(device)\n",
        "    value, logit, latent = model((state, latent), icm = False)\n",
        "    prob = F.softmax(logit, dim=1) #from train\n",
        "    action = prob.multinomial(1).data\n",
        "    state, reward, done, _ = env.step(action.item())\n",
        "    # try:\n",
        "    #     action=int(acts[x])\n",
        "    # except:\n",
        "    #     action = 10\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    # state, reward, done, info = env.step(action)\n",
        "    x+=1\n",
        "    if done: break\n",
        "env.play()\n",
        "print(x)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-qQjuJIepAvA",
        "FidkEuaA8HvK",
        "SPCCve3p2bL-",
        "sbpPda4YEv13",
        "GCHpcDteZdLS",
        "fj3tv7XHZmD9",
        "wFrRKvOwhYM_",
        "KlV2MvSK-aL_"
      ],
      "name": "curiousity_perceiverio.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
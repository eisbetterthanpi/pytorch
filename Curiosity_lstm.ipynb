{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/Curiosity_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ruZVKH4n5n"
      },
      "source": [
        "# Curiosity-Driven Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "tI88ewSgEPXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "5WUwDp1a4n5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba7a39f-df6e-4255-fed7-f60b7aebde7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting gym\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 5.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793151 sha256=a6f18ffb83148370f1d6d88b3e07054b94473418cd0162c89f87294487d05dd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0e/54/63d9f3d16ddf0fec1622e90d28140df5e6016bcf8ea920037d\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.24.1 gym-notices-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=36e38064ceb42a485d5e9549c2b9c9a33c3bd39ee57b76dc457e91988dc7a29a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting nes-py\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (4.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (0.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (3.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp37-cp37m-linux_x86_64.whl size=437151 sha256=80d2a727ff342a935258ce224ae7ca4b7fe8d9155e147ff5da6b364cfa6f0faf\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/96/0e/22a8c7dbdf412d8e988286f223b223baf0f4ad90c9e699c56d\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.64.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3132 sha256=aa9e489444fa89569740efad8b98f4d6c4f285bcacc2fb9d26ee99eede428464\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/0a/2a/86955ea711b461ab7918236fed2568733f75ed677d0524b56c\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/Yangyangii/Curiosity-Driven-A2C\n",
        "# https://colab.research.google.com/github/Yangyangii/Curiosity-Driven-A2C/blob/master/Curiosity.ipynb\n",
        "\n",
        "%pip install -U gym\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import collections\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "\n",
        "!pip install colabgymrender\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gym wrappers"
      ],
      "metadata": {
        "id": "Q6rIxoaDeT4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/2_gym_wrappers_saving_loading.ipynb\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: reward = self.total_rewards\n",
        "        else: reward = 0\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioSparse, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            # return observation, score, True, info # lost one life, end env\n",
        "            done = True\n",
        "        # else:\n",
        "            # self.total_score = 0\n",
        "        return observation, score, done, info\n",
        "    def reset(self):\n",
        "        self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioEarlyStop, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            # return observation, reward, True, info # early stop\n",
        "            done = True\n",
        "        # else:\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "L4wnbtD_eRxa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model"
      ],
      "metadata": {
        "id": "1DV2N6GfFPB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conv_Encoder\n",
        "class Conv_Encoder(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv_Encoder, self).__init__()\n",
        "        self.conv_encoder = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "            nn.Conv2d(in_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((64,64)),\n",
        "            nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "            nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "            # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        # print(\"conv forward shape\",x.shape)\n",
        "        # x=x.squeeze()\n",
        "        # x = torch.transpose(x, 1,2)\n",
        "        # x = torch.transpose(x, 0,1)\n",
        "        x = torch.transpose(x, -2,-1)\n",
        "        x = torch.transpose(x, -3,-2)\n",
        "        # print(\"conv forward\",x.shape)\n",
        "        x = self.conv_encoder(x)\n",
        "        x=x.flatten(start_dim=1)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [batch, 256]\n",
        "\n",
        "conv=Conv_Encoder(3)\n",
        "# x=torch.rand(1, 210, 160, 3)#.squeeze()\n",
        "x=torch.rand(2, 210, 160, 3)#.squeeze()\n",
        "# x = torch.transpose(x, 1,2)\n",
        "# x = torch.transpose(x, 0,1)\n",
        "print(x.shape)\n",
        "out=conv(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "PMzj00fxGYdJ",
        "outputId": "d8d7d8dc-36ae-4342-81c5-d020b7614c89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 210, 160, 3])\n",
            "torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "-0j7HgLc4n5y"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, n_actions, space_dims, hidden_dims): #cartpole 2 4 32\n",
        "        super(Actor, self).__init__()\n",
        "        # print(\"init actor\",n_actions, space_dims, hidden_dims)\n",
        "        space_dims=256\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(space_dims, hidden_dims),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hidden_dims, n_actions),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # print(\"forward1\",x.shape) # cp [1, 4] mon [1, 210, 160, 3]\n",
        "        x = self.conv_encoder(x)\n",
        "        # print(\"forward2\",x.shape) # cp [1, 4]\n",
        "        features = self.feature_extractor(x)\n",
        "        policy = self.actor(features)\n",
        "        return policy\n",
        "    \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, space_dims, hidden_dims):\n",
        "        super(Critic, self).__init__()\n",
        "        space_dims=256\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(space_dims, hidden_dims),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.critic = nn.Linear(hidden_dims, 1)\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_encoder(x)\n",
        "        features = self.feature_extractor(x)\n",
        "        est_reward = self.critic(features)\n",
        "        return est_reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InvModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(InvModel, self).__init__()\n",
        "        self.inv_lstm = nn.LSTMCell(phist_size, hidden_size)\n",
        "        self.fc = nn.Linear(phist_size+hidden_size, n_actions)\n",
        "        self.inv_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.inv_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "        \n",
        "    def reset_hidden(self):\n",
        "        self.inv_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.inv_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "        \n",
        "    def forward(self, phist1): # [1, 512]\n",
        "        features = torch.cat([self.inv_latent[0],phist1], dim=1)\n",
        "        features = features.view(1, -1) # (1, hidden_dims)\n",
        "        # print(\"InverseModel\", features.shape)\n",
        "        athat = self.fc(features) # (1, n_actions)\n",
        "        self.inv_latent = self.inv_lstm(phist,self.inv_latent)\n",
        "        return athat\n",
        "\n",
        "class FwdModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(FwdModel, self).__init__()\n",
        "        self.fwd_lstm = nn.LSTMCell(phist_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_dims+n_actions, phist_size)\n",
        "        self.eye = torch.eye(n_actions, device=device)\n",
        "        self.fwd_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.fwd_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "        \n",
        "    def reset_hidden(self):\n",
        "        self.fwd_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.fwd_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "\n",
        "    def forward(self, action, phist):\n",
        "        self.fwd_latent = self.fwd_lstm(phist, self.fwd_latent)\n",
        "        # print(\"ForwardModel\",action.shape, fwd_latent[0].shape)\n",
        "        # print(\"ForwardModel2\",self.eye[action], fwd_latent[0].shape)\n",
        "        x = torch.cat([self.eye[action], self.fwd_latent[0]], dim=-1) # (1, n_actions+hidden_dims)\n",
        "        phihat1 = self.fc(x) # (1, hidden_dims)\n",
        "        return phihat1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #     forward_pred_err = 1/2 * self.forward_loss(forward_pred, enc_state2.detach()).sum(dim=1).unsqueeze(dim=1)\n",
        "    #     # calc prediction error\n",
        "    #     pred_action = self.inverse_model(enc_state1, enc_state2) \n",
        "    #     inverse_pred_err = self.inverse_loss(pred_action, action.flatten().long()).unsqueeze(dim=1)    \n",
        "    #     return forward_pred_err, inverse_pred_err\n",
        "\n",
        "    # def update_ICM(self, forward_err, inverse_err):\n",
        "    #     self.optimizer.zero_grad()\n",
        "    #     loss = ((1. - self.beta)*inverse_err + self.beta*forward_err).mean()\n",
        "    #     #print(loss)\n",
        "    #     loss.backward(retain_graph=True)\n",
        "    #     clip_grad_norm_(self.inverse_model.parameters(),1)\n",
        "    #     clip_grad_norm_(self.forward_model.parameters(),1)\n",
        "    #     self.optimizer.step()\n",
        "    #     return loss.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=None):\n",
        "    # return torch.tensor(x, dtype=dtype).unsqueeze(0)\n",
        "    return torch.tensor(x.copy(), dtype=dtype).unsqueeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "_ePcqu9yEL8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "zzY629Gx4n57",
        "outputId": "f4305884-bdf1-437c-de34-c36c6ed1365c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        }
      ],
      "source": [
        "beta = 0.2\n",
        "lamda = 0.1\n",
        "eta = 100.0 # scale factor for intrinsic reward\n",
        "gamma = 0.99\n",
        "lr_critic = 0.005\n",
        "lr_actor = 0.001\n",
        "lr_icm = 0.001\n",
        "# max_eps = 1000\n",
        "sparse_mode = True\n",
        "\n",
        "\n",
        "# env = gym.make('CartPole-v1')\n",
        "# env = gym.make('PongDeterministic-v4')\n",
        "# env = gym.make('LunarLander-v2')\n",
        "# env = gym.make('MontezumaRevengeDeterministic-v4')\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7mcTd9Al4n58"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Actor Critic\n",
        "space_dims = env.observation_space.shape[0]\n",
        "# space_dims = env.observation_space.shape\n",
        "# cp 2 (4,) 32 mon 18 (210, 160, 3) 32\n",
        "actor = Actor(n_actions=env.action_space.n, space_dims=space_dims, hidden_dims=32).to(device)\n",
        "critic = Critic(space_dims=space_dims, hidden_dims=32).to(device)\n",
        "\n",
        "conv_encode = Conv_Encoder(3).to(device)\n",
        "phist_size=256\n",
        "hidden_size=512\n",
        "\n",
        "fwd_model = FwdModel(env.action_space.n, hidden_size, phist_size).to(device) #256 phist_size(no lstm) hidden_size(lstm)\n",
        "inv_model = InvModel(env.action_space.n, hidden_size, phist_size).to(device) #32\n",
        "\n",
        "# Actor Critic\n",
        "a_optim = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
        "c_optim = torch.optim.Adam(critic.parameters(), lr=lr_critic)\n",
        "\n",
        "# ICM\n",
        "# icm_params = list(feature_extractor.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "# icm_params = list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "icm_params = list(fwd_model.parameters()) + list(inv_model.parameters())\n",
        "icm_optim = torch.optim.Adam(icm_params, lr=lr_icm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "id": "CKFwyTLe4n59",
        "outputId": "276a9bb7-d64b-486c-e4f3-656e1326edb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MarioEarlyStop: early stop  595\n",
            "Episodes: 0, AVG Score: 3780.000, Score: 75600, AVG reward i: 1163.808709\n",
            "MarioEarlyStop: early stop  80\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f4b7b76cc2e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mfwd_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mavg_intrinsic_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mireward_lst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mireward_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mmva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmva\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mreward_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mavg_ireward_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_intrinsic_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mva' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def pg_loss(action_prob, reward): return -torch.mean(torch.log(action_prob+1e-6)*reward)\n",
        "mse_loss = nn.MSELoss()\n",
        "xe_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "global_step = 0\n",
        "reward_lst = []\n",
        "mva_lst = []\n",
        "mva = 0.\n",
        "avg_ireward_lst = []\n",
        "\n",
        "actor.train()\n",
        "\n",
        "for n_eps in range(100):\n",
        "    st1 = to_tensor(env.reset(), dtype=torch.float).to(device).detach()\n",
        "    done = False\n",
        "    score = 0\n",
        "    ireward_lst = []\n",
        "    loss=0\n",
        "    while not done:\n",
        "        st = st1\n",
        "        a_optim.zero_grad()\n",
        "        c_optim.zero_grad()\n",
        "        icm_optim.zero_grad()\n",
        "        \n",
        "        # estimate action with policy network\n",
        "        policy = actor(st) # [18]\n",
        "        # action = select_action(policy.detach().numpy()[0]) #only for lame\n",
        "        # print(policy.shape)\n",
        "        prob = nn.functional.softmax(policy, dim=-1) #1\n",
        "        action = prob.multinomial(1).data\n",
        "        \n",
        "        # interaction with environment\n",
        "        # st1, reward, done, info = env.step(action)\n",
        "        st1, reward, done, info = env.step(action.item())\n",
        "        st1 = to_tensor(st1, dtype=torch.float).to(device).detach()\n",
        "        advantages = torch.zeros_like(policy)\n",
        "        # extrinsic_reward = to_tensor([0.], dtype=torch.float) if sparse_mode else to_tensor([reward], dtype=torch.float)\n",
        "        extrinsic_reward = torch.tensor(reward, dtype=torch.float, device=device).view(1,1)\n",
        "        \n",
        "        val = critic(st).squeeze(0) #[1,1]\n",
        "        val1 = critic(st1).squeeze(0)\n",
        "\n",
        "        phist = conv_encode(st)\n",
        "        phist1 = conv_encode(st1)\n",
        "        # athat, inv_latent = inv_model(phist1, inv_latent)\n",
        "        athat = inv_model(phist1)\n",
        "        # phihat, fwd_latent = fwd_model(action.squeeze(0), phist, fwd_latent)\n",
        "        phihat = fwd_model(action.squeeze(0), phist)\n",
        "        # print(phihat.shape, phist1.shape)\n",
        "        # forward_loss = mse_loss(phihat, phist1)\n",
        "        forward_loss = mse_loss(phihat.detach(), phist1.detach())\n",
        "\n",
        "\n",
        "        # print(\"2\",athat.shape, action.view(-1).shape)\n",
        "        # inverse_loss = xe_loss(athat, t_action.view(-1))\n",
        "        # inverse_loss = xe_loss(athat.squeeze(1), t_action.view(-1))\n",
        "        inverse_loss = xe_loss(athat.squeeze(1), action.view(-1))\n",
        "        icm_loss = (1-beta)*inverse_loss + beta*forward_loss\n",
        "        # icm_loss = beta*forward_loss #problem\n",
        "        # icm_loss = (1-beta)*inverse_loss #prob;em\n",
        "        \n",
        "        # Reward\n",
        "        intrinsic_reward = eta*forward_loss.detach()\n",
        "        # if done:\n",
        "        #     total_reward = -100 + intrinsic_reward if score < 499 else intrinsic_reward\n",
        "        #     advantages[0, action] = total_reward - val\n",
        "        #     c_target = total_reward\n",
        "        # else:\n",
        "        # print(extrinsic_reward.dtype , intrinsic_reward.dtype)\n",
        "        total_reward = extrinsic_reward + intrinsic_reward\n",
        "        advantages[0, action] = total_reward + gamma*val1 - val\n",
        "        c_target = total_reward + gamma*val1\n",
        "        \n",
        "        # Loss - Actor Critic\n",
        "        # print(\"3\",policy.shape, advantages.detach().shape)\n",
        "        actor_loss = pg_loss(policy, advantages.detach())\n",
        "        # print(\"4\",val.shape, c_target.detach().squeeze(0).shape)\n",
        "        critic_loss = mse_loss(val, c_target.detach().squeeze(0))\n",
        "        ac_loss = actor_loss + critic_loss\n",
        "        # Update\n",
        "        # loss = lamda*ac_loss + icm_loss\n",
        "        loss += lamda*ac_loss + icm_loss\n",
        "        # loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        # # print(\"passed\")\n",
        "        # icm_optim.step()\n",
        "        # a_optim.step()\n",
        "        # c_optim.step()\n",
        "        if not done:\n",
        "            score += reward\n",
        "        ireward_lst.append(intrinsic_reward.item())\n",
        "        global_step += 1\n",
        "    loss.backward()\n",
        "    icm_optim.step()\n",
        "    a_optim.step()\n",
        "    c_optim.step()\n",
        "    inv_model.reset_hidden() # i added\n",
        "    fwd_model.reset_hidden()\n",
        "    avg_intrinsic_reward = sum(ireward_lst) / len(ireward_lst)\n",
        "    mva = 0.95*mva + 0.05*score\n",
        "    reward_lst.append(score)\n",
        "    avg_ireward_lst.append(avg_intrinsic_reward)\n",
        "    mva_lst.append(mva)\n",
        "    print('Episodes: {}, AVG Score: {:.3f}, Score: {}, AVG reward i: {:.6f}'.format(n_eps, mva, score, avg_intrinsic_reward))\n",
        "    del val,val1, st1, reward, done, info, st, action, prob,athat, phihat\n",
        "    del intrinsic_reward, total_reward ,extrinsic_reward\n",
        "    del ac_loss , actor_loss , critic_loss, icm_loss,inverse_loss ,forward_loss\n",
        "    del avg_intrinsic_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSQ3EoiY4n5_"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szv_Z9-s4n5_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title plot\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(reward_lst)\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mva_lst)\n",
        "plt.ylabel('Moving Average Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV2MvSK-aL_"
      },
      "source": [
        "#### save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-XW-LvZ8Xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "name='curiousity_mario.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "model=actor\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# actor=model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6PNxVu-W6K"
      },
      "source": [
        "#### video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fkhEcBB3tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "# model=actor\n",
        "\n",
        "model.eval()\n",
        "while True:\n",
        "    # state = to_tensor(state, dtype=torch.float).to(device)\n",
        "    state = torch.tensor(state.copy()).type(torch.float).to(device)\n",
        "    policy = model(state) # [18]\n",
        "    # action = select_action(policy.detach().numpy()[0])\n",
        "    # print(policy.shape)\n",
        "    prob = nn.functional.softmax(policy, dim=-1) #1\n",
        "    action = prob.multinomial(1).data\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    # state, reward, done, info = env.step(action)\n",
        "    state, reward, done, info = env.step(action.item())\n",
        "    if done: break\n",
        "env.play()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Curiosity_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tI88ewSgEPXQ"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
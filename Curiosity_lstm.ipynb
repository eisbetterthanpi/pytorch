{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/Curiosity_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ruZVKH4n5n"
      },
      "source": [
        "# Curiosity-Driven Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "tI88ewSgEPXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "5WUwDp1a4n5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2dc8246-c87e-4a69-84f9-e991d114ec86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.7)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym-super-mario-bros in /usr/local/lib/python3.7/dist-packages (7.4.0)\n",
            "Requirement already satisfied: nes-py in /usr/local/lib/python3.7/dist-packages (8.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.24.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (0.0.7)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (4.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py) (0.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: colabgymrender in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.64.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/Yangyangii/Curiosity-Driven-A2C\n",
        "# https://colab.research.google.com/github/Yangyangii/Curiosity-Driven-A2C/blob/master/Curiosity.ipynb\n",
        "\n",
        "%pip install -U gym\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import collections\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "\n",
        "!pip install colabgymrender\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gym wrappers"
      ],
      "metadata": {
        "id": "Q6rIxoaDeT4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/2_gym_wrappers_saving_loading.ipynb\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: reward = self.total_rewards\n",
        "        else: reward = 0\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioSparse, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            # return observation, score, True, info # lost one life, end env\n",
        "            done = True\n",
        "        # else:\n",
        "            # self.total_score = 0\n",
        "        return observation, score, done, info\n",
        "    def reset(self):\n",
        "        self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioEarlyStop, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            # return observation, reward, True, info # early stop\n",
        "            done = True\n",
        "        # else:\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "L4wnbtD_eRxa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model"
      ],
      "metadata": {
        "id": "1DV2N6GfFPB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conv_Encoder\n",
        "class Conv_Encoder(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv_Encoder, self).__init__()\n",
        "        self.conv_encoder = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "            nn.Conv2d(in_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((64,64)),\n",
        "            nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "            nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "            # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        # print(\"conv forward shape\",x.shape)\n",
        "        # x=x.squeeze()\n",
        "        # x = torch.transpose(x, 1,2)\n",
        "        # x = torch.transpose(x, 0,1)\n",
        "        x = torch.transpose(x, -2,-1)\n",
        "        x = torch.transpose(x, -3,-2)\n",
        "        # print(\"conv forward\",x.shape)\n",
        "        x = self.conv_encoder(x)\n",
        "        x=x.flatten(start_dim=1)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [batch, 256]\n",
        "\n",
        "conv=Conv_Encoder(3)\n",
        "# x=torch.rand(1, 210, 160, 3)#.squeeze()\n",
        "x=torch.rand(2, 210, 160, 3)#.squeeze()\n",
        "# x = torch.transpose(x, 1,2)\n",
        "# x = torch.transpose(x, 0,1)\n",
        "print(x.shape)\n",
        "out=conv(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "PMzj00fxGYdJ",
        "outputId": "1ef84bbf-1195-4838-9926-2b6bc1553ed4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 210, 160, 3])\n",
            "torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "-0j7HgLc4n5y"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, n_actions, space_dims, hidden_dims): #cartpole 2 4 32\n",
        "        super(Actor, self).__init__()\n",
        "        # print(\"init actor\",n_actions, space_dims, hidden_dims)\n",
        "        space_dims=256\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(space_dims, hidden_dims),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hidden_dims, n_actions),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # print(\"forward1\",x.shape) # cp [1, 4] mon [1, 210, 160, 3]\n",
        "        x = self.conv_encoder(x)\n",
        "        # print(\"forward2\",x.shape) # cp [1, 4]\n",
        "        features = self.feature_extractor(x)\n",
        "        policy = self.actor(features)\n",
        "        return policy\n",
        "    \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, space_dims, hidden_dims):\n",
        "        super(Critic, self).__init__()\n",
        "        space_dims=256\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(space_dims, hidden_dims),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.critic = nn.Linear(hidden_dims, 1)\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_encoder(x)\n",
        "        features = self.feature_extractor(x)\n",
        "        est_reward = self.critic(features)\n",
        "        return est_reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InverseModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(InverseModel, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_dims+phist_size, n_actions)\n",
        "        \n",
        "    def forward(self, features):\n",
        "        features = features.view(1, -1) # (1, hidden_dims)\n",
        "        action = self.fc(features) # (1, n_actions)\n",
        "        return action\n",
        "\n",
        "class ForwardModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(ForwardModel, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_dims+n_actions, hidden_dims)\n",
        "        self.eye = torch.eye(n_actions, device=device)\n",
        "        \n",
        "    def forward(self, action, features):\n",
        "        # print(\"ForwardModel\",action.shape, features.shape)\n",
        "        # print(\"ForwardModel2\",self.eye[action], features.shape)\n",
        "        x = torch.cat([self.eye[action], features], dim=-1) # (1, n_actions+hidden_dims)\n",
        "        features = self.fc(x) # (1, hidden_dims)\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InvModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(InvModel, self).__init__()\n",
        "        self.inv_lstm = nn.LSTMCell(phist_size, hidden_size)\n",
        "        self.fc = nn.Linear(phist_size+hidden_size, n_actions)\n",
        "        \n",
        "    def forward(self, phist1, inv_latent): # [1, 512]\n",
        "        features = torch.cat([inv_latent[0],phist1], dim=1)\n",
        "        features = features.view(1, -1) # (1, hidden_dims)\n",
        "        # print(\"InverseModel\", features.shape)\n",
        "        athat = self.fc(features) # (1, n_actions)\n",
        "        inv_latent = self.inv_lstm(phist,inv_latent)\n",
        "        return athat, inv_latent\n",
        "\n",
        "class FwdModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(FwdModel, self).__init__()\n",
        "        self.fwd_lstm = nn.LSTMCell(phist_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_dims+n_actions, phist_size)\n",
        "        self.eye = torch.eye(n_actions, device=device)\n",
        "        \n",
        "    def forward(self, action, phist, fwd_latent):\n",
        "        fwd_latent = self.fwd_lstm(phist,fwd_latent)\n",
        "        # print(\"ForwardModel\",action.shape, fwd_latent[0].shape)\n",
        "        # print(\"ForwardModel2\",self.eye[action], fwd_latent[0].shape)\n",
        "        x = torch.cat([self.eye[action], fwd_latent[0]], dim=-1) # (1, n_actions+hidden_dims)\n",
        "        phihat1 = self.fc(x) # (1, hidden_dims)\n",
        "        return phihat1, fwd_latent\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, space_dims, hidden_dims):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        space_dims=256\n",
        "        self.fc = nn.Linear(space_dims, hidden_dims)\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv_encoder(x)\n",
        "        y = torch.tanh(self.fc(x))\n",
        "        return y\n",
        "\n",
        "def to_tensor(x, dtype=None):\n",
        "    # return torch.tensor(x, dtype=dtype).unsqueeze(0)\n",
        "    return torch.tensor(x.copy(), dtype=dtype).unsqueeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "_ePcqu9yEL8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "zzY629Gx4n57",
        "outputId": "f94d240b-8709-4dfd-daec-5678ef77658b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        }
      ],
      "source": [
        "beta = 0.2\n",
        "lamda = 0.1\n",
        "eta = 100.0 # scale factor for intrinsic reward\n",
        "gamma = 0.99\n",
        "lr_critic = 0.005\n",
        "lr_actor = 0.001\n",
        "lr_icm = 0.001\n",
        "# max_eps = 1000\n",
        "sparse_mode = True\n",
        "\n",
        "\n",
        "# env = gym.make('CartPole-v1')\n",
        "# env = gym.make('PongDeterministic-v4')\n",
        "# env = gym.make('LunarLander-v2')\n",
        "# env = gym.make('MontezumaRevengeDeterministic-v4')\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7mcTd9Al4n58"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Actor Critic\n",
        "space_dims = env.observation_space.shape[0]\n",
        "# space_dims = env.observation_space.shape\n",
        "# cp 2 (4,) 32 mon 18 (210, 160, 3) 32\n",
        "actor = Actor(n_actions=env.action_space.n, space_dims=space_dims, hidden_dims=32).to(device)\n",
        "critic = Critic(space_dims=space_dims, hidden_dims=32).to(device)\n",
        "\n",
        "conv_encode = Conv_Encoder(3).to(device)\n",
        "phist_size=256\n",
        "hidden_size=512\n",
        "# inv_lstm = nn.LSTMCell(phist_size, hidden_size).to(device)\n",
        "# fwd_lstm = nn.LSTMCell(phist_size, hidden_size).to(device)\n",
        "\n",
        "# ICM\n",
        "feature_extractor = FeatureExtractor(env.observation_space.shape[0], 32).to(device)\n",
        "forward_model = ForwardModel(env.action_space.n, hidden_size, phist_size).to(device) #256 phist_size(no lstm) hidden_size(lstm)\n",
        "inverse_model = InverseModel(env.action_space.n, hidden_size, phist_size).to(device) #32\n",
        "\n",
        "fwd_model = FwdModel(env.action_space.n, hidden_size, phist_size).to(device) #256 phist_size(no lstm) hidden_size(lstm)\n",
        "inv_model = InvModel(env.action_space.n, hidden_size, phist_size).to(device) #32\n",
        "\n",
        "\n",
        "\n",
        "# Actor Critic\n",
        "a_optim = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
        "c_optim = torch.optim.Adam(critic.parameters(), lr=lr_critic)\n",
        "\n",
        "# ICM\n",
        "# icm_params = list(feature_extractor.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "icm_params = list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "icm_optim = torch.optim.Adam(icm_params, lr=lr_icm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "CKFwyTLe4n59",
        "outputId": "b07b6644-6e80-43d8-eb32-fd034f605cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-34be834b3518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# loss = icm_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# loss = lamda*ac_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0micm_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0ma_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def pg_loss(action_prob, reward): return -torch.mean(torch.log(action_prob+1e-6)*reward)\n",
        "mse_loss = nn.MSELoss()\n",
        "xe_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "global_step = 0\n",
        "reward_lst = []\n",
        "mva_lst = []\n",
        "mva = 0.\n",
        "avg_ireward_lst = []\n",
        "\n",
        "actor.train()\n",
        "\n",
        "for n_eps in range(100):\n",
        "    st1 = to_tensor(env.reset(), dtype=torch.float).to(device)\n",
        "    done = False\n",
        "    score = 0\n",
        "    ireward_lst = []\n",
        "    inv_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "    fwd_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "    \n",
        "    while not done:\n",
        "        st = st1\n",
        "        a_optim.zero_grad()\n",
        "        c_optim.zero_grad()\n",
        "        icm_optim.zero_grad()\n",
        "        \n",
        "        # estimate action with policy network\n",
        "        policy = actor(st) # [18]\n",
        "        # action = select_action(policy.detach().numpy()[0]) #only for lame\n",
        "        # print(policy.shape)\n",
        "        prob = nn.functional.softmax(policy, dim=-1) #1\n",
        "        action = prob.multinomial(1).data\n",
        "        \n",
        "        # interaction with environment\n",
        "        # st1, reward, done, info = env.step(action)\n",
        "        st1, reward, done, info = env.step(action.item())\n",
        "        st1 = to_tensor(st1, dtype=torch.float).to(device)\n",
        "        advantages = torch.zeros_like(policy)\n",
        "        # extrinsic_reward = to_tensor([0.], dtype=torch.float) if sparse_mode else to_tensor([reward], dtype=torch.float)\n",
        "        extrinsic_reward = torch.tensor([reward].copy(), dtype=torch.float, device=device).unsqueeze(0)\n",
        "\n",
        "        val = critic(st)[0]\n",
        "        val1 = critic(st1)[0]\n",
        "        \n",
        "\n",
        "        # # ICM\n",
        "        # obs_cat = torch.cat([st, st1], dim=0)\n",
        "        # features = feature_extractor(obs_cat) # (2, hidden_dims) [2, 32]\n",
        "        # inverse_action_prob = inverse_model(features) # (n_actions)\n",
        "        # t_action=t_action.reshape((1))\n",
        "        # print(\"t_action\",t_action.shape) #[1]\n",
        "        # # est_next_features = forward_model(t_action, features[0:1]) #[1] [1, 32]\n",
        "        # athat = forward_model(action.squeeze(0), features[0:1]) #[1] [1, 32]\n",
        "        # # Loss - ICM\n",
        "        # # print(\"1\",est_next_features.squeeze(0).shape, features[1].shape)\n",
        "        # # forward_loss = mse_loss(est_next_features, features[1])\n",
        "        # forward_loss = mse_loss(athat.squeeze(0), features[1])\n",
        "\n",
        "\n",
        "        phist = conv_encode(st)\n",
        "        phist1 = conv_encode(st1)\n",
        "        athat, inv_latent = inv_model(phist1, inv_latent)\n",
        "        phihat, fwd_latent = fwd_model(action.squeeze(0), phist, fwd_latent)\n",
        "        # print(phihat.shape, phist1.shape)\n",
        "        forward_loss = mse_loss(phihat, phist1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"2\",athat.shape, t_action.view(-1).shape)\n",
        "        # inverse_loss = xe_loss(athat, t_action.view(-1))\n",
        "        # inverse_loss = xe_loss(athat.squeeze(1), t_action.view(-1))\n",
        "        inverse_loss = xe_loss(athat.squeeze(1), action.view(-1))\n",
        "        # icm_loss = (1-beta)*inverse_loss + beta*forward_loss\n",
        "        icm_loss = beta*forward_loss #problem\n",
        "        # icm_loss = (1-beta)*inverse_loss #prob;em\n",
        "        \n",
        "        # Reward\n",
        "        intrinsic_reward = eta*forward_loss.detach()\n",
        "        # if done:\n",
        "        #     total_reward = -100 + intrinsic_reward if score < 499 else intrinsic_reward\n",
        "        #     advantages[0, action] = total_reward - val\n",
        "        #     c_target = total_reward\n",
        "        # else:\n",
        "        # print(extrinsic_reward.dtype , intrinsic_reward.dtype)\n",
        "        total_reward = extrinsic_reward + intrinsic_reward\n",
        "        advantages[0, action] = total_reward + gamma*val1 - val\n",
        "        c_target = total_reward + gamma*val1\n",
        "        \n",
        "        # Loss - Actor Critic\n",
        "        # print(\"3\",policy.shape, advantages.detach().shape)\n",
        "        actor_loss = pg_loss(policy, advantages.detach())\n",
        "        # print(\"4\",v.shape, c_target.detach().squeeze(0).shape)\n",
        "        critic_loss = mse_loss(val, c_target.detach().squeeze(0))\n",
        "        ac_loss = actor_loss + critic_loss\n",
        "        # inv_latent[0].detach()\n",
        "        # inv_latent[1].detach()\n",
        "        # fwd_latent[0].detach()\n",
        "        # fwd_latent[1].detach()\n",
        "        # Update\n",
        "        loss = lamda*ac_loss + icm_loss\n",
        "        # loss = icm_loss\n",
        "        # loss = lamda*ac_loss\n",
        "        loss.backward()\n",
        "        icm_optim.step()\n",
        "        a_optim.step()\n",
        "        c_optim.step()\n",
        "        if not done:\n",
        "            score += reward\n",
        "        ireward_lst.append(intrinsic_reward.item())\n",
        "        global_step += 1\n",
        "    avg_intrinsic_reward = sum(ireward_lst) / len(ireward_lst)\n",
        "    mva = 0.95*mva + 0.05*score\n",
        "    reward_lst.append(score)\n",
        "    avg_ireward_lst.append(avg_intrinsic_reward)\n",
        "    mva_lst.append(mva)\n",
        "    print('Episodes: {}, AVG Score: {:.3f}, Score: {}, AVG reward i: {:.6f}'.format(n_eps, mva, score, avg_intrinsic_reward))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSQ3EoiY4n5_"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szv_Z9-s4n5_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title plot\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(reward_lst)\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mva_lst)\n",
        "plt.ylabel('Moving Average Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV2MvSK-aL_"
      },
      "source": [
        "#### save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-XW-LvZ8Xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "name='curiousity_mario.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "model=actor\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# actor=model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6PNxVu-W6K"
      },
      "source": [
        "#### video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fkhEcBB3tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "model.eval()\n",
        "while True:\n",
        "    # state = to_tensor(state, dtype=torch.float).to(device)\n",
        "    state = torch.tensor(state.copy()).type(torch.float).to(device)\n",
        "    policy = model(state) # [18]\n",
        "    # action = select_action(policy.detach().numpy()[0])\n",
        "    # print(policy.shape)\n",
        "    prob = nn.functional.softmax(policy, dim=-1) #1\n",
        "    action = prob.multinomial(1).data\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    # state, reward, done, info = env.step(action)\n",
        "    state, reward, done, info = env.step(action.item())\n",
        "    if done: break\n",
        "env.play()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Curiosity_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tI88ewSgEPXQ"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
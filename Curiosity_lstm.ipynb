{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/Curiosity_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ruZVKH4n5n"
      },
      "source": [
        "# Curiosity-Driven Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "tI88ewSgEPXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "5WUwDp1a4n5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c4fea6-fbda-4c9f-f03a-330f226d2702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting gym\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 4.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793152 sha256=f9975e58674834bdb620cd048b4fe68178d64db85d86a3275dd710d7b2505455\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0e/54/63d9f3d16ddf0fec1622e90d28140df5e6016bcf8ea920037d\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.24.1 gym-notices-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=796aac9d0d305f3d5b6987de4ec5bde2a55ee0f1bd0ee11298841d77e3ee6a39\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting nes-py\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (0.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (4.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py) (3.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp37-cp37m-linux_x86_64.whl size=437251 sha256=f8553b3175e2f8a07f1a7296b12a63fd23f231deb9174a86ca6a7b71ddbac7f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/96/0e/22a8c7dbdf412d8e988286f223b223baf0f4ad90c9e699c56d\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.64.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3132 sha256=f993fe3667702119d2813b33ac12a214bd38231000c98c91e4ff880a1f1f5b3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/0a/2a/86955ea711b461ab7918236fed2568733f75ed677d0524b56c\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/Yangyangii/Curiosity-Driven-A2C\n",
        "# https://colab.research.google.com/github/Yangyangii/Curiosity-Driven-A2C/blob/master/Curiosity.ipynb\n",
        "\n",
        "%pip install -U gym\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import collections\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "\n",
        "!pip install colabgymrender\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### gym wrappers"
      ],
      "metadata": {
        "id": "Q6rIxoaDeT4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/2_gym_wrappers_saving_loading.ipynb\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: reward = self.total_rewards\n",
        "        else: reward = 0\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioSparse, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            # return observation, score, True, info # lost one life, end env\n",
        "            done = True\n",
        "        # else:\n",
        "            # self.total_score = 0\n",
        "        return observation, score, done, info\n",
        "    def reset(self):\n",
        "        self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        super(MarioEarlyStop, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            # return observation, reward, True, info # early stop\n",
        "            done = True\n",
        "        # else:\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "L4wnbtD_eRxa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model"
      ],
      "metadata": {
        "id": "1DV2N6GfFPB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conv_Encoder\n",
        "class Conv_Encoder(nn.Module):\n",
        "    # def __init__(self):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Conv_Encoder, self).__init__()\n",
        "        self.conv_encoder = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "            nn.Conv2d(in_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((64,64)),\n",
        "            nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "            nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "            # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "    def forward(self, x): # in [4, 3, 224, 224]\n",
        "        # print(\"conv forward shape\",x.shape)\n",
        "        # x=x.squeeze()\n",
        "        # x = torch.transpose(x, 1,2)\n",
        "        # x = torch.transpose(x, 0,1)\n",
        "        x = torch.transpose(x, -2,-1)\n",
        "        x = torch.transpose(x, -3,-2)\n",
        "        # print(\"conv forward\",x.shape)\n",
        "        x = self.conv_encoder(x)\n",
        "        x=x.flatten(start_dim=1)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        return x # out [batch, 256]\n",
        "\n",
        "conv=Conv_Encoder(3)\n",
        "# x=torch.rand(1, 210, 160, 3)#.squeeze()\n",
        "x=torch.rand(2, 210, 160, 3)#.squeeze()\n",
        "# x = torch.transpose(x, 1,2)\n",
        "# x = torch.transpose(x, 0,1)\n",
        "print(x.shape)\n",
        "out=conv(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "PMzj00fxGYdJ",
        "outputId": "afa68546-d14a-49be-fd7b-1898e6309bcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 210, 160, 3])\n",
            "torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "-0j7HgLc4n5y"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, n_actions, space_dims, hidden_dims): #cartpole 2 4 32\n",
        "        super(Actor, self).__init__()\n",
        "        # print(\"init actor\",n_actions, space_dims, hidden_dims)\n",
        "        space_dims=256\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(space_dims, hidden_dims),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hidden_dims, n_actions),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # print(\"forward1\",x.shape) # cp [1, 4] mon [1, 210, 160, 3]\n",
        "        x = self.conv_encoder(x)\n",
        "        # print(\"forward2\",x.shape) # cp [1, 4]\n",
        "        features = self.feature_extractor(x)\n",
        "        policy = self.actor(features)\n",
        "        return policy\n",
        "    \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, space_dims, hidden_dims):\n",
        "        super(Critic, self).__init__()\n",
        "        space_dims=256\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(space_dims, hidden_dims),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.critic = nn.Linear(hidden_dims, 1)\n",
        "        self.conv_encoder = Conv_Encoder(3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_encoder(x)\n",
        "        features = self.feature_extractor(x)\n",
        "        est_reward = self.critic(features)\n",
        "        return est_reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InvModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(InvModel, self).__init__()\n",
        "        self.inv_lstm = nn.LSTMCell(phist_size, hidden_size)\n",
        "        self.fc = nn.Linear(phist_size+hidden_size, n_actions)\n",
        "        self.inv_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.inv_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "        \n",
        "    def reset_hidden(self):\n",
        "        self.inv_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.inv_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "        \n",
        "    def forward(self, phist1): # [1, 512]\n",
        "        features = torch.cat([self.inv_latent[0],phist1], dim=1)\n",
        "        features = features.view(1, -1) # (1, hidden_dims)\n",
        "        # print(\"InverseModel\", features.shape)\n",
        "        athat = self.fc(features) # (1, n_actions)\n",
        "        self.inv_latent = self.inv_lstm(phist,self.inv_latent)\n",
        "        return athat\n",
        "\n",
        "class FwdModel(nn.Module):\n",
        "    def __init__(self, n_actions, hidden_dims, phist_size):\n",
        "        super(FwdModel, self).__init__()\n",
        "        self.fwd_lstm = nn.LSTMCell(phist_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_dims+n_actions, phist_size)\n",
        "        self.eye = torch.eye(n_actions, device=device)\n",
        "        self.fwd_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.fwd_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "        \n",
        "    def reset_hidden(self):\n",
        "        self.fwd_latent = (torch.zeros(1, 512).to(device), torch.zeros(1, 512).to(device))\n",
        "        # self.fwd_latent = (torch.zeros(1, 512), torch.zeros(1, 512))\n",
        "\n",
        "    def forward(self, action, phist):\n",
        "        self.fwd_latent = self.fwd_lstm(phist, self.fwd_latent)\n",
        "        # print(\"ForwardModel\",action.shape, fwd_latent[0].shape)\n",
        "        # print(\"ForwardModel2\",self.eye[action], fwd_latent[0].shape)\n",
        "        x = torch.cat([self.eye[action], self.fwd_latent[0]], dim=-1) # (1, n_actions+hidden_dims)\n",
        "        phihat1 = self.fc(x) # (1, hidden_dims)\n",
        "        return phihat1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #     forward_pred_err = 1/2 * self.forward_loss(forward_pred, enc_state2.detach()).sum(dim=1).unsqueeze(dim=1)\n",
        "    #     # calc prediction error\n",
        "    #     pred_action = self.inverse_model(enc_state1, enc_state2) \n",
        "    #     inverse_pred_err = self.inverse_loss(pred_action, action.flatten().long()).unsqueeze(dim=1)    \n",
        "    #     return forward_pred_err, inverse_pred_err\n",
        "\n",
        "    # def update_ICM(self, forward_err, inverse_err):\n",
        "    #     self.optimizer.zero_grad()\n",
        "    #     loss = ((1. - self.beta)*inverse_err + self.beta*forward_err).mean()\n",
        "    #     #print(loss)\n",
        "    #     loss.backward(retain_graph=True)\n",
        "    #     clip_grad_norm_(self.inverse_model.parameters(),1)\n",
        "    #     clip_grad_norm_(self.forward_model.parameters(),1)\n",
        "    #     self.optimizer.step()\n",
        "    #     return loss.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=None):\n",
        "    # return torch.tensor(x, dtype=dtype).unsqueeze(0)\n",
        "    return torch.tensor(x.copy(), dtype=dtype).unsqueeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwwwwwwwwww"
      ],
      "metadata": {
        "id": "_ePcqu9yEL8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "zzY629Gx4n57",
        "outputId": "d90582de-9beb-44c8-d81e-b6de1f6e5c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        }
      ],
      "source": [
        "beta = 0.2\n",
        "lamda = 0.1\n",
        "eta = 100.0 # scale factor for intrinsic reward\n",
        "gamma = 0.99\n",
        "lr_critic = 0.005\n",
        "lr_actor = 0.001\n",
        "lr_icm = 0.001\n",
        "# max_eps = 1000\n",
        "sparse_mode = True\n",
        "\n",
        "\n",
        "# env = gym.make('CartPole-v1')\n",
        "# env = gym.make('PongDeterministic-v4')\n",
        "# env = gym.make('LunarLander-v2')\n",
        "# env = gym.make('MontezumaRevengeDeterministic-v4')\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7mcTd9Al4n58"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Actor Critic\n",
        "space_dims = env.observation_space.shape[0]\n",
        "# space_dims = env.observation_space.shape\n",
        "# cp 2 (4,) 32 mon 18 (210, 160, 3) 32\n",
        "actor = Actor(n_actions=env.action_space.n, space_dims=space_dims, hidden_dims=32).to(device)\n",
        "critic = Critic(space_dims=space_dims, hidden_dims=32).to(device)\n",
        "\n",
        "conv_encode = Conv_Encoder(3).to(device)\n",
        "phist_size=256\n",
        "hidden_size=512\n",
        "\n",
        "fwd_model = FwdModel(env.action_space.n, hidden_size, phist_size).to(device) #256 phist_size(no lstm) hidden_size(lstm)\n",
        "inv_model = InvModel(env.action_space.n, hidden_size, phist_size).to(device) #32\n",
        "\n",
        "# Actor Critic\n",
        "a_optim = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
        "c_optim = torch.optim.Adam(critic.parameters(), lr=lr_critic)\n",
        "\n",
        "# ICM\n",
        "# icm_params = list(feature_extractor.parameters()) + list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "# icm_params = list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "icm_params = list(fwd_model.parameters()) + list(inv_model.parameters())\n",
        "icm_optim = torch.optim.Adam(icm_params, lr=lr_icm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "id": "CKFwyTLe4n59",
        "outputId": "bb7c8e82-d853-4ef9-8e85-c9d9b0d55eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MarioEarlyStop: early stop  45\n",
            "Episodes: 0, AVG Score: 0.000, Score: 0, AVG reward i: 4956.118231\n",
            "MarioSparse: died\n",
            "Episodes: 1, AVG Score: 0.000, Score: 0, AVG reward i: 4786.830893\n",
            "MarioEarlyStop: early stop  435\n",
            "Episodes: 2, AVG Score: 0.000, Score: 0, AVG reward i: 4682.334605\n",
            "MarioEarlyStop: early stop  82\n",
            "Episodes: 3, AVG Score: 0.000, Score: 0, AVG reward i: 4886.280052\n",
            "MarioSparse: died\n",
            "Episodes: 4, AVG Score: 0.000, Score: 0, AVG reward i: 4864.733166\n",
            "MarioEarlyStop: early stop  88\n",
            "Episodes: 5, AVG Score: 0.000, Score: 0, AVG reward i: 4874.130335\n",
            "MarioEarlyStop: early stop  434\n",
            "Episodes: 6, AVG Score: 3580.000, Score: 71600, AVG reward i: 4748.948863\n",
            "MarioEarlyStop: early stop  48\n",
            "Episodes: 7, AVG Score: 3401.000, Score: 0, AVG reward i: 4951.741298\n",
            "MarioEarlyStop: early stop  53\n",
            "Episodes: 8, AVG Score: 3230.950, Score: 0, AVG reward i: 4951.472426\n",
            "MarioEarlyStop: early stop  40\n",
            "Episodes: 9, AVG Score: 3069.402, Score: 0, AVG reward i: 4955.545267\n",
            "MarioSparse: died\n",
            "Episodes: 10, AVG Score: 2915.932, Score: 0, AVG reward i: 4889.523964\n",
            "MarioEarlyStop: early stop  594\n",
            "Episodes: 11, AVG Score: 2770.136, Score: 0, AVG reward i: 4774.075607\n",
            "MarioEarlyStop: early stop  434\n",
            "Episodes: 12, AVG Score: 2751.629, Score: 2400, AVG reward i: 4698.082299\n",
            "MarioEarlyStop: early stop  49\n",
            "Episodes: 13, AVG Score: 2614.048, Score: 0, AVG reward i: 4952.119423\n",
            "MarioEarlyStop: early stop  40\n",
            "Episodes: 14, AVG Score: 2483.345, Score: 0, AVG reward i: 4955.208494\n",
            "MarioEarlyStop: early stop  435\n",
            "Episodes: 15, AVG Score: 5404.178, Score: 60900, AVG reward i: 4739.712467\n",
            "MarioEarlyStop: early stop  45\n",
            "Episodes: 16, AVG Score: 5133.969, Score: 0, AVG reward i: 4955.157906\n",
            "MarioEarlyStop: early stop  42\n",
            "Episodes: 17, AVG Score: 4877.271, Score: 0, AVG reward i: 4953.080736\n",
            "MarioEarlyStop: early stop  40\n",
            "Episodes: 18, AVG Score: 4633.407, Score: 0, AVG reward i: 4955.251230\n",
            "MarioSparse: died\n",
            "Episodes: 19, AVG Score: 4401.737, Score: 0, AVG reward i: 4838.742230\n",
            "MarioEarlyStop: early stop  46\n",
            "Episodes: 20, AVG Score: 4181.650, Score: 0, AVG reward i: 4952.926460\n",
            "MarioEarlyStop: early stop  80\n",
            "Episodes: 21, AVG Score: 3972.567, Score: 0, AVG reward i: 4950.085255\n",
            "MarioEarlyStop: early stop  54\n",
            "Episodes: 22, AVG Score: 3773.939, Score: 0, AVG reward i: 4952.702310\n",
            "MarioEarlyStop: early stop  88\n",
            "Episodes: 23, AVG Score: 3585.242, Score: 0, AVG reward i: 4855.089775\n",
            "MarioEarlyStop: early stop  41\n",
            "Episodes: 24, AVG Score: 3405.980, Score: 0, AVG reward i: 4956.427877\n",
            "MarioEarlyStop: early stop  45\n",
            "Episodes: 25, AVG Score: 3235.681, Score: 0, AVG reward i: 4951.867723\n",
            "MarioEarlyStop: early stop  40\n",
            "Episodes: 26, AVG Score: 3073.897, Score: 0, AVG reward i: 4957.174267\n",
            "MarioEarlyStop: early stop  44\n",
            "Episodes: 27, AVG Score: 2920.202, Score: 0, AVG reward i: 4956.520947\n",
            "MarioEarlyStop: early stop  594\n",
            "Episodes: 28, AVG Score: 2774.192, Score: 0, AVG reward i: 4820.802102\n",
            "MarioEarlyStop: early stop  97\n",
            "Episodes: 29, AVG Score: 2635.482, Score: 0, AVG reward i: 4857.108965\n",
            "MarioEarlyStop: early stop  49\n",
            "Episodes: 30, AVG Score: 2503.708, Score: 0, AVG reward i: 4953.726883\n",
            "MarioSparse: died\n",
            "Episodes: 31, AVG Score: 2378.523, Score: 0, AVG reward i: 4798.197383\n",
            "MarioEarlyStop: early stop  123\n",
            "Episodes: 32, AVG Score: 2259.597, Score: 0, AVG reward i: 4768.214407\n",
            "MarioEarlyStop: early stop  434\n",
            "Episodes: 33, AVG Score: 5146.617, Score: 60000, AVG reward i: 4690.338373\n",
            "MarioEarlyStop: early stop  57\n",
            "Episodes: 34, AVG Score: 4889.286, Score: 0, AVG reward i: 4950.438944\n",
            "MarioEarlyStop: early stop  52\n",
            "Episodes: 35, AVG Score: 4644.822, Score: 0, AVG reward i: 4953.723033\n",
            "MarioEarlyStop: early stop  56\n",
            "Episodes: 36, AVG Score: 4412.581, Score: 0, AVG reward i: 4952.568711\n",
            "MarioEarlyStop: early stop  228\n",
            "Episodes: 37, AVG Score: 6246.952, Score: 41100, AVG reward i: 4801.410663\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50af055995ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mval1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mphist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mphist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# athat, inv_latent = inv_model(phist1, inv_latent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-010539d63a9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print(\"conv forward\",x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# x = x.view(-1, 16 * 5 * 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.30 GiB already allocated; 3.75 MiB free; 13.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def pg_loss(action_prob, reward): return -torch.mean(torch.log(action_prob+1e-6)*reward)\n",
        "mse_loss = nn.MSELoss()\n",
        "xe_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "global_step = 0\n",
        "reward_lst = []\n",
        "mva_lst = []\n",
        "mva = 0.\n",
        "avg_ireward_lst = []\n",
        "\n",
        "actor.train()\n",
        "\n",
        "for n_eps in range(100):\n",
        "    st1 = to_tensor(env.reset(), dtype=torch.float).to(device).detach()\n",
        "    done = False\n",
        "    score = 0\n",
        "    ireward_lst = []\n",
        "    loss=0\n",
        "    while not done:\n",
        "        st = st1\n",
        "        a_optim.zero_grad()\n",
        "        c_optim.zero_grad()\n",
        "        icm_optim.zero_grad()\n",
        "        \n",
        "        # estimate action with policy network\n",
        "        policy = actor(st) # [18]\n",
        "        # action = select_action(policy.detach().numpy()[0]) #only for lame\n",
        "        # print(policy.shape)\n",
        "        prob = nn.functional.softmax(policy, dim=-1) #1\n",
        "        action = prob.multinomial(1).data\n",
        "        \n",
        "        # interaction with environment\n",
        "        # st1, reward, done, info = env.step(action)\n",
        "        st1, reward, done, info = env.step(action.item())\n",
        "        st1 = to_tensor(st1, dtype=torch.float).to(device).detach()\n",
        "        advantages = torch.zeros_like(policy)\n",
        "        # extrinsic_reward = to_tensor([0.], dtype=torch.float) if sparse_mode else to_tensor([reward], dtype=torch.float)\n",
        "        extrinsic_reward = torch.tensor(reward, dtype=torch.float, device=device).view(1,1)\n",
        "        \n",
        "        val = critic(st).squeeze(0) #[1,1]\n",
        "        val1 = critic(st1).squeeze(0)\n",
        "\n",
        "        phist = conv_encode(st)\n",
        "        phist1 = conv_encode(st1)\n",
        "        # athat, inv_latent = inv_model(phist1, inv_latent)\n",
        "        athat = inv_model(phist1)\n",
        "        # phihat, fwd_latent = fwd_model(action.squeeze(0), phist, fwd_latent)\n",
        "        phihat = fwd_model(action.squeeze(0), phist)\n",
        "        # print(phihat.shape, phist1.shape)\n",
        "        # forward_loss = mse_loss(phihat, phist1)\n",
        "        forward_loss = mse_loss(phihat.detach(), phist1.detach())\n",
        "\n",
        "\n",
        "        # print(\"2\",athat.shape, action.view(-1).shape)\n",
        "        # inverse_loss = xe_loss(athat, t_action.view(-1))\n",
        "        # inverse_loss = xe_loss(athat.squeeze(1), t_action.view(-1))\n",
        "        inverse_loss = xe_loss(athat.squeeze(1), action.view(-1))\n",
        "        icm_loss = (1-beta)*inverse_loss + beta*forward_loss\n",
        "        # icm_loss = beta*forward_loss #problem\n",
        "        # icm_loss = (1-beta)*inverse_loss #prob;em\n",
        "        \n",
        "        # Reward\n",
        "        intrinsic_reward = eta*forward_loss.detach()\n",
        "        # if done:\n",
        "        #     total_reward = -100 + intrinsic_reward if score < 499 else intrinsic_reward\n",
        "        #     advantages[0, action] = total_reward - val\n",
        "        #     c_target = total_reward\n",
        "        # else:\n",
        "        # print(extrinsic_reward.dtype , intrinsic_reward.dtype)\n",
        "        total_reward = extrinsic_reward + intrinsic_reward\n",
        "        advantages[0, action] = total_reward + gamma*val1 - val\n",
        "        c_target = total_reward + gamma*val1\n",
        "        \n",
        "        # Loss - Actor Critic\n",
        "        # print(\"3\",policy.shape, advantages.detach().shape)\n",
        "        actor_loss = pg_loss(policy, advantages.detach())\n",
        "        # print(\"4\",val.shape, c_target.detach().squeeze(0).shape)\n",
        "        critic_loss = mse_loss(val, c_target.detach().squeeze(0))\n",
        "        ac_loss = actor_loss + critic_loss\n",
        "        # Update\n",
        "        # loss = lamda*ac_loss + icm_loss\n",
        "        loss += lamda*ac_loss + icm_loss\n",
        "        # loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        # # print(\"passed\")\n",
        "        # icm_optim.step()\n",
        "        # a_optim.step()\n",
        "        # c_optim.step()\n",
        "        if not done:\n",
        "            score += reward\n",
        "        ireward_lst.append(intrinsic_reward.item())\n",
        "        global_step += 1\n",
        "    loss.backward()\n",
        "    icm_optim.step()\n",
        "    a_optim.step()\n",
        "    c_optim.step()\n",
        "    inv_model.reset_hidden() # i added\n",
        "    fwd_model.reset_hidden()\n",
        "    avg_intrinsic_reward = sum(ireward_lst) / len(ireward_lst)\n",
        "    mva = 0.95*mva + 0.05*score\n",
        "    reward_lst.append(score)\n",
        "    avg_ireward_lst.append(avg_intrinsic_reward)\n",
        "    mva_lst.append(mva)\n",
        "    print('Episodes: {}, AVG Score: {:.3f}, Score: {}, AVG reward i: {:.6f}'.format(n_eps, mva, score, avg_intrinsic_reward))\n",
        "    del val,val1, st1, reward, done, info, st, action, prob,athat, phihat\n",
        "    del intrinsic_reward, total_reward ,extrinsic_reward\n",
        "    del ac_loss , actor_loss , critic_loss, icm_loss,inverse_loss ,forward_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSQ3EoiY4n5_"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szv_Z9-s4n5_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title plot\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(reward_lst)\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mva_lst)\n",
        "plt.ylabel('Moving Average Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV2MvSK-aL_"
      },
      "source": [
        "#### save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-XW-LvZ8Xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "name='curiousity_mario.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "model=actor\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# actor=model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6PNxVu-W6K"
      },
      "source": [
        "#### video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fkhEcBB3tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "# model=actor\n",
        "\n",
        "model.eval()\n",
        "while True:\n",
        "    # state = to_tensor(state, dtype=torch.float).to(device)\n",
        "    state = torch.tensor(state.copy()).type(torch.float).to(device)\n",
        "    policy = model(state) # [18]\n",
        "    # action = select_action(policy.detach().numpy()[0])\n",
        "    # print(policy.shape)\n",
        "    prob = nn.functional.softmax(policy, dim=-1) #1\n",
        "    action = prob.multinomial(1).data\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    # state, reward, done, info = env.step(action)\n",
        "    state, reward, done, info = env.step(action.item())\n",
        "    if done: break\n",
        "env.play()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Curiosity_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tI88ewSgEPXQ"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
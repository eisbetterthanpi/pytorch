{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/AIM2_simplify_strip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yWeBVzxkn0az"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/tree/main/grid_world_experiments\n",
        "# python main.py --reward aim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8YZstFpi7O0"
      },
      "source": [
        "#### buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MEOVQhzJi8wa"
      },
      "outputs": [],
      "source": [
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/buffers.py\n",
        "import random\n",
        "from typing import List, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size: int):\n",
        "        \"\"\"Implements a ring buffer (FIFO).\n",
        "        :param size: (int)  Max number of transitions to store in the buffer. When the buffer overflows the old memories are dropped.\"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._storage)\n",
        "\n",
        "    @property\n",
        "    def storage(self):\n",
        "        \"\"\"[(Union[np.ndarray, int], Union[np.ndarray, int], float, Union[np.ndarray, int], bool)]:\n",
        "         content of the replay buffer\"\"\"\n",
        "        return self._storage\n",
        "\n",
        "    @property\n",
        "    def buffer_size(self) -> int:\n",
        "        \"\"\"float: Max capacity of the buffer\"\"\"\n",
        "        return self._maxsize\n",
        "\n",
        "    def can_sample(self, n_samples: int) -> bool:\n",
        "        return len(self) >= n_samples\n",
        "\n",
        "    def is_full(self) -> int:\n",
        "        return len(self) == self.buffer_size\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def extend(self, obs_t, action, reward, obs_tp1, done):\n",
        "        for data in zip(obs_t, action, reward, obs_tp1, done):\n",
        "            if self._next_idx >= len(self._storage):\n",
        "                self._storage.append(data)\n",
        "            else:\n",
        "                self._storage[self._next_idx] = data\n",
        "            self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes: Union[List[int], np.ndarray]):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "\n",
        "        obses_t = np.array(obses_t)\n",
        "        actions = np.array(actions)\n",
        "        obses_tp1 = np.array(obses_tp1)\n",
        "        return (torch.tensor(obses_t).type(torch.float),\n",
        "                torch.tensor(actions).type(torch.float),\n",
        "                torch.tensor(rewards).type(torch.float),\n",
        "                torch.tensor(obses_tp1).type(torch.float),\n",
        "                torch.tensor(dones).type(torch.float))\n",
        "\n",
        "    def sample(self, batch_size: int, **_kwargs):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        :param batch_size: (int) How many transitions to sample.\n",
        "        :return:\n",
        "            - obs_batch: (np.ndarray) batch of observations\n",
        "            - act_batch: (numpy float) batch of actions executed given obs_batch\n",
        "            - rew_batch: (numpy float) rewards received as results of executing act_batch\n",
        "            - next_obs_batch: (np.ndarray) next set of observations seen after executing act_batch\n",
        "            - done_mask: (numpy bool) done_mask[i] = 1 if executing act_batch[i] resulted in the end of an episodeand 0 otherwise.\"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu5AXAfBpexp"
      },
      "source": [
        "#### grid_mdp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "NbXRzYNkpbnM"
      },
      "outputs": [],
      "source": [
        "# @title GridMDP\n",
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/grid_mdp.py\n",
        "import numpy as np\n",
        "\n",
        "class GridMDP:\n",
        "    def __init__(self):\n",
        "        self.x_dim = 10\n",
        "        self.y_dim = 10\n",
        "        self.num_states = self.x_dim * self.y_dim\n",
        "        self.start_state = np.asarray([0, 0])\n",
        "        self._state = self.start_state\n",
        "        self.action_space = [0, 1, 2, 3]\n",
        "        self.target_state = [np.asarray([9, 9])]\n",
        "        self.time_limit = 15\n",
        "        self.t = 0\n",
        "        self._rewards = np.zeros(shape=(self.x_dim, self.y_dim), dtype=np.float)\n",
        "        for t in self.target_state:\n",
        "            self._rewards[t[0], t[1]] = 1.\n",
        "\n",
        "    def target_distribution(self, weight=3.) -> np.ndarray:\n",
        "        # dist = np.exp(self._rewards * weight)\n",
        "        dist = self._rewards * weight\n",
        "        dist /= np.sum(dist)\n",
        "        return dist\n",
        "\n",
        "    def reward(self):\n",
        "        return self._rewards[self._state[0], self._state[1]]\n",
        "        # if self._state == self.target_state:\n",
        "        #     return 1.\n",
        "        # return 0.\n",
        "\n",
        "\n",
        "class MazeWorld(GridMDP):\n",
        "    \"\"\"Maze World is the same grid world as above But it has multiple goals, one of which is unreachable\"\"\"\n",
        "    def __init__(self):\n",
        "        super(MazeWorld, self).__init__()\n",
        "        self.start_state = np.asarray([1, 1])\n",
        "        self._state = self.start_state\n",
        "        self.target_state = [np.asarray([1, 4])]  # , np.asarray([8, 1]), np.asarray([6, 6]), np.asarray([4, 9])]\n",
        "        self.time_limit = 100\n",
        "        self._rewards = np.zeros(shape=(self.x_dim, self.y_dim), dtype=np.float)\n",
        "        for c, t in enumerate(self.target_state):\n",
        "            self._rewards[t[0], t[1]] = 1. / (c + 1.)\n",
        "\n",
        "    def sample_transitions(self, num=50):\n",
        "        \"\"\"sample valid transitions from the mdp\"\"\"\n",
        "        transitions = []\n",
        "        temp_state = self._state\n",
        "        temp_time = self.t\n",
        "        temp_done = self._done\n",
        "        self._done = False\n",
        "        self.t = 0\n",
        "        while len(transitions) < num:\n",
        "            state = np.random.randint(self.min_state, self.max_state)\n",
        "            act = np.random.choice(self.action_space)\n",
        "            self._state = np.copy(state)\n",
        "            next_state, _, _, _ = self.step(act)\n",
        "            if not np.equal(state, next_state).all():\n",
        "                transitions.append([state, next_state])\n",
        "            self.t = 0\n",
        "            self._done = False\n",
        "        self._state = temp_state\n",
        "        self._done = temp_done\n",
        "        self.t = temp_time\n",
        "        states, next_states = (np.asarray(x) for x in zip(*transitions))\n",
        "        return states, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPN3_5JrpAd7"
      },
      "source": [
        "#### policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-pAnASeqo8xA"
      },
      "outputs": [],
      "source": [
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/policy.py\n",
        "from abc import ABC\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# class Mepol(nn.Module):\n",
        "#     def __init__(self, s_size, a_size, h_size=32):\n",
        "#         super(Mepol, self).__init__()\n",
        "#         self.model=nn.Sequential(\n",
        "#             nn.Linear(s_size, h_size), nn.ReLU(),\n",
        "#             nn.Linear(h_size, a_size),\n",
        "#             nn.Softmax(dim=0),\n",
        "#         )\n",
        "    \n",
        "#     def forward(self, state): # og discrete\n",
        "#         # state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "#         probs = self.model(state).cpu()\n",
        "#         m = torch.distributions.Categorical(probs)\n",
        "#         action = m.sample() # can't use action = np.argmax(m) use  m.sample(), sample an action with prob dist P(.|s)\n",
        "#         return action.item()\n",
        "\n",
        "class MlpNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=1, activ=f.relu, n_units=64):\n",
        "        super(MlpNetwork, self).__init__()\n",
        "        # n_units = 512\n",
        "        self.h1 = nn.Linear(input_dim, n_units)\n",
        "        self.h2 = nn.Linear(n_units, n_units)\n",
        "        # self.h3 = nn.Linear(n_units, n_units)\n",
        "        self.out = nn.Linear(n_units, output_dim)\n",
        "        self.activ = activ\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activ(self.h1(x))\n",
        "        x = self.activ(self.h2(x))\n",
        "        # x = self.activ(self.h3(x))\n",
        "        x = self.out(x)\n",
        "        x = f.log_softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SoftQLearning(nn.Module, ABC):\n",
        "    \"\"\"Learns a soft Q-function. Samples from softmax distribution of Q-values for policy\"\"\"\n",
        "    def __init__(self, x_dim=1, out_dim=2, max_state=9., min_state=0, ent_coef=0.01, target_update=1e-1):\n",
        "        super(SoftQLearning, self).__init__()\n",
        "        self.diff_state = np.array(max_state - min_state).astype(np.float32)\n",
        "        self.mean_state = np.asarray(self.diff_state / 2 + min_state).astype(np.float32)\n",
        "        self.input_dim = x_dim\n",
        "        self.num_actions = out_dim\n",
        "        self.alpha = ent_coef\n",
        "        self.q = MlpNetwork(self.input_dim, output_dim=out_dim, n_units=64)\n",
        "        self.q_target = MlpNetwork(self.input_dim, output_dim=out_dim, n_units=64)\n",
        "        self.target_params = self.q_target.parameters()\n",
        "        self.q_params = self.q.parameters()\n",
        "        self.target_update_rate = target_update\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.q_params\n",
        "\n",
        "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.type(torch.float32)\n",
        "        x = (x - self.mean_state) / self.diff_state\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.normalize(x)\n",
        "        q = self.q(x)\n",
        "        v = self.alpha * torch.logsumexp(q / self.alpha, dim=-1)\n",
        "        # self.alpha = max(0.01, 0.99 * self.alpha + 0.01 * (torch.mean(torch.abs(q)).detach().numpy() / 10.))\n",
        "        qt = self.q_target(x)\n",
        "        vt = self.alpha * torch.logsumexp(qt / self.alpha, dim=-1)\n",
        "        return q, v, qt, vt\n",
        "        # return q\n",
        "\n",
        "    # def pi_loss(self, x: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
        "    #     \"\"\"Return log_pi for the policy gradient\"\"\"\n",
        "    #     x = self.normalize(x)\n",
        "    #     logits = self.pi(x)\n",
        "    #     actions = actions.type(torch.long)\n",
        "    #     log_pi = logits.gather(dim=-1, index=actions)\n",
        "    #     return log_pi\n",
        "\n",
        "    def sample_action(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Sample from policy\"\"\"\n",
        "        x = self.normalize(x)\n",
        "        q = self.q(x)\n",
        "        v = self.alpha * torch.logsumexp(q / self.alpha, dim=-1)\n",
        "        logits = 1. / self.alpha * (q - v)\n",
        "        pi = torch.exp(logits)\n",
        "        action = pi.multinomial(1)\n",
        "        return action\n",
        "\n",
        "    def entropy(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.normalize(x)\n",
        "        q = self.q(x)\n",
        "        v = self.alpha * torch.logsumexp(q / self.alpha, dim=-1)\n",
        "        logits = 1. / self.alpha * (q - torch.unsqueeze(v, dim=-1))\n",
        "        entropy_kl = torch.sum(torch.log(torch.ones_like(logits) / self.num_actions) - logits, dim=-1)\n",
        "        # pi = torch.exp(logits)\n",
        "        # pisum = torch.sum(pi, dim=-1)\n",
        "        # entropy = -torch.sum(pi * logits, dim=-1)\n",
        "        return entropy_kl\n",
        "\n",
        "    def update_target(self):\n",
        "        \"\"\"update the target network using polyak averaging\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for c, t in zip(self.q.parameters(), self.q_target.parameters()):\n",
        "                t.data.copy_((1. - self.target_update_rate) * t.data + self.target_update_rate * c.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGZXEC_dovUB"
      },
      "source": [
        "#### mian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKO63-lBot97",
        "outputId": "cddffd33-ab9a-4927-c6cf-a270b0f1a41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47166\n",
            "aim\n",
            "self.s_size 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/main.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "# from torch.nn import utils\n",
        "import torch.nn.functional as f\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "seed=1123\n",
        "reward='aim' # ['gail', 'airl', 'fairl', 'aim', 'none']\n",
        "dir='/content'\n",
        "\n",
        "torch.set_default_dtype(torch.float32)\n",
        "# Set random seeds\n",
        "seed = 42 * seed\n",
        "print(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed = seed\n",
        "np.random.seed = seed\n",
        "reward_to_use = reward  # use one of ['gail', 'airl', 'fairl', 'none']\n",
        "print(reward_to_use)\n",
        "\n",
        "def wasserstein_reward(d):\n",
        "    return d\n",
        "reward_dict = {'aim': wasserstein_reward}\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"The discriminator used to learn the potentials or the reward functions\"\"\"\n",
        "    def __init__(self, x_dim=1, max_state=10., min_state=0):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.mean_state = torch.tensor((max_state - min_state) / 2 + min_state, dtype=torch.float32)\n",
        "        self.diff_state = torch.tensor(max_state - min_state, dtype=torch.float32)\n",
        "        self.input_dim = x_dim\n",
        "        self.d = MlpNetwork(self.input_dim, n_units=64)  # , activ=f.tanh)\n",
        "\n",
        "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.type(torch.float32)\n",
        "        x = (x - self.mean_state) / self.diff_state\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalize(x)\n",
        "        output = self.d(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "import gym\n",
        "# !pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "# import gym_pygame\n",
        "\n",
        "\n",
        "class GAIL:\n",
        "    \"\"\"Class to take the continuous MDP and use gail to match given target distribution\"\"\"\n",
        "    def __init__(self):\n",
        "        # self.env = gym.make(\"CartPole-v1\")\n",
        "        # self.env = gym.make(\"Pendulum-v0\") #continuous\n",
        "        self.env = gym.make(\"MountainCar-v0\") #discrete\n",
        "        self.s_size = self.env.observation_space.shape[0]\n",
        "        self.a_size = self.env.action_space.n\n",
        "        max_state = 10\n",
        "        min_state = 0\n",
        "        # self.policy = SoftQLearning(x_dim=self.env.dims, out_dim=len(self.env.action_space),\n",
        "        #     max_state=self.env.max_state, min_state=self.env.min_state, ent_coef=.3, target_update=3e-2)\n",
        "        self.policy = SoftQLearning(x_dim=self.s_size, out_dim=self.a_size, max_state=max_state, min_state=min_state, ent_coef=.3, target_update=3e-2)\n",
        "        # self.policy = Mepol(self.s_size, self.a_size)\n",
        "\n",
        "        # self.discriminator = Discriminator(x_dim=self.env.dims, max_state=self.env.max_state,\n",
        "        #     min_state=self.env.min_state)\n",
        "        print(\"self.s_size\",self.s_size)\n",
        "        self.discriminator = Discriminator(x_dim=self.s_size, max_state=max_state, min_state=min_state)\n",
        "        self.discount = 0.99\n",
        "        self.check_state = set()\n",
        "        self.agent_buffer = ReplayBuffer(size=5000)\n",
        "        self.target_buffer = ReplayBuffer(size=5000)\n",
        "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters())  # , lr=3e-4)\n",
        "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters())  # , lr=1e-4)\n",
        "        self.max_r = 0.\n",
        "        self.min_r = -1.\n",
        "\n",
        "    def gather_data(self, num_trans=100): # line 4-\n",
        "        t = 0\n",
        "        while t < num_trans:\n",
        "            s = self.env.reset()\n",
        "            s = torch.tensor(s).type(torch.float32)\n",
        "            done = False\n",
        "            while not done:\n",
        "                # self.states.append(deepcopy(s))\n",
        "                # print(\"sssssssss\",s)\n",
        "                action = self.policy.sample_action(s)\n",
        "                # self.actions.append(a)\n",
        "                a = np.squeeze(action.data.detach().numpy())\n",
        "                s_p, r, done, _ = self.env.step(a)\n",
        "                s_p = torch.tensor(s_p).type(torch.float32)\n",
        "                # d = self.discriminator(sp)\n",
        "                # i_r = gail_reward(d)\n",
        "                # self.next_states.append(deepcopy(s))\n",
        "                # self.rewards.append(i_r)  # deepcopy(r))\n",
        "                # self.dones.append(deepcopy(done))\n",
        "                # print(s.squeeze(), action.reshape([-1]).detach(), r, s_p.squeeze(), done)\n",
        "                # tensor([ 0.0144, -0.0008, -0.0287,  0.0008]) tensor([0]) 1.0 tensor([ 0.0144, -0.1955, -0.0286,  0.2843]) False\n",
        "                self.agent_buffer.add(s.squeeze(), action.reshape([-1]).detach(), r, s_p.squeeze(), done)\n",
        "                if s_p not in self.check_state:\n",
        "                    self.check_state.add(s_p)\n",
        "                    self.target_buffer.add(s, a, r, s_p, done)\n",
        "                s = s_p\n",
        "                t += 1\n",
        "            # self.states.append(s)\n",
        "\n",
        "\n",
        "    def compute_td_targets(self, states, next_states, dones, rewards=None):\n",
        "        \"\"\"Compute the value of the current states and the TD target based on one step reward and value of next states\n",
        "        :return: value of current states v, TD target targets\"\"\"\n",
        "        # states = states.reshape([-1, self.env.dims])\n",
        "        states = states.reshape([-1, self.s_size])\n",
        "        # next_states = next_states.reshape([-1, self.env.dims])\n",
        "        next_states = next_states.reshape([-1, self.s_size])\n",
        "        v = self.policy(states)[0]\n",
        "        v_prime = self.policy(next_states)[-1]\n",
        "        if rewards is not None:\n",
        "            dones = rewards.type(torch.float32).reshape([-1, 1])\n",
        "        else:\n",
        "            dones = dones.type(torch.float32).reshape([-1, 1])\n",
        "        reward_func = reward_dict[reward_to_use]\n",
        "        if reward_func is not None:\n",
        "            # d0 = self.discriminator(states)\n",
        "            d1 = self.discriminator(next_states)\n",
        "            # Compute rewards\n",
        "            # r0 = reward_func(d0)\n",
        "            r1 = reward_func(d1)\n",
        "            rewards = rewards.type(torch.float32).reshape([-1, 1]) + ((r1 - self.max_r) / (self.max_r - self.min_r))\n",
        "        targets = rewards.type(torch.float32).reshape([-1, 1])\n",
        "        # print(\"in compute_td_targets\",dones, self.discount, v_prime) #100?*[1.] 0.99 tensor([-0.6525, -0.7356]\n",
        "        targets += (1. - dones) * self.discount * v_prime.reshape([-1, 1])\n",
        "        return v, targets.detach()\n",
        "\n",
        "    def fit_v_func(self):\n",
        "        \"\"\"This function will train the value function using the collected data\"\"\"\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        s, a, r, s_p, dones = self.agent_buffer.sample(100)\n",
        "        q, targets = self.compute_td_targets(s, s_p, dones, rewards=r)\n",
        "        actions = torch.tensor(a, dtype=torch.long)\n",
        "        v = q.gather(dim=-1, index=actions)\n",
        "        loss = torch.mean(0.5 * (targets - v) ** 2)\n",
        "        loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        self.policy.update_target()\n",
        "        return\n",
        "\n",
        "    def optimize_policy(self): #line 27-29\n",
        "        \"\"\"This function will optimize the policy to maximize returns Based on collected data\"\"\"\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        s, a, r, s_p, dones = self.agent_buffer.sample(100)\n",
        "        v, targets = self.compute_td_targets(s, s_p, dones, rewards=r)\n",
        "        advantages = (targets - v).detach()\n",
        "        a = a.reshape([-1, 1]).detach()\n",
        "        # print(\"s, a\",s.shape, a.shape) #[100, 4]float -1 1 [100, 1] 1./0.\n",
        "        # neg_log_pi = -1. * self.policy.pi_loss(s.reshape([-1, self.env.dims]), a)\n",
        "        # neg_log_pi = -1. * self.policy.pi_loss(s, a)\n",
        "        # entropy_kl = self.policy.entropy(s.reshape([-1, self.env.dims]))\n",
        "        entropy_kl = self.policy.entropy(s)\n",
        "        loss = torch.mean(advantages * neg_log_pi) + 1e-1 * torch.mean(entropy_kl)\n",
        "        loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        return\n",
        "\n",
        "    def compute_aim_pen(self, target_state, prev_state, next_state_state, lambda_=10.):\n",
        "        \"\"\"Computes values of the discriminator at different points and constraints the difference to be 0.1\"\"\"\n",
        "        prev_out = self.discriminator(prev_state)\n",
        "        next_out = self.discriminator(next_state_state)\n",
        "        penalty = lambda_ * torch.max(torch.abs(next_out - prev_out) - 0.1, torch.tensor(0.)).pow(2).mean()\n",
        "        return penalty\n",
        "\n",
        "\n",
        "    def optimize_discriminator(self): # line 32-33\n",
        "        \"\"\"Optimize the discriminator based on the memory and target_distribution\"\"\"\n",
        "        num_samples = 100\n",
        "        self.discriminator_optimizer.zero_grad()\n",
        "        # _, _, _, target_distribution, _ = self.target_buffer.sample(100)\n",
        "        _, _, _, target_distribution, _ = self.target_buffer.sample(1)\n",
        "        # target_dist = np.reshape(self.env.target_distribution(), (-1,))\n",
        "        target_dist=target_distribution.squeeze()\n",
        "        # import torch.nn.functional as f\n",
        "        # f.normalize(input, p=2, dim=2)\n",
        "        # print(\"target_dist\",target_dist) # [4,0,0,0]\n",
        "        p=target_dist/sum(target_dist) # [4,0,0,0]\n",
        "        # print(\"p\",p)\n",
        "        # target_distribution = np.random.choice(target_dist.shape[0], num_samples, p=target_dist)\n",
        "        # target_distribution = np.random.choice(target_dist.shape[0], num_samples, p=p)\n",
        "        target_distribution = np.random.choice(target_dist.shape[0], num_samples)\n",
        "        # print(\"target_distribution\",target_distribution) #[100* 0]\n",
        "        states, _, _, next_states, _ = self.agent_buffer.sample(num_samples)\n",
        "        # target_distribution = sample_target_distribution(mean=self.env.target_mean, std=self.env.target_std, num=100)\n",
        "        target_distribution = target_distribution.reshape([-1, 1])\n",
        "\n",
        "        target_distribution = np.tile(target_distribution,(1,self.s_size))\n",
        "        # print(\"target_distribution\",target_distribution)\n",
        "        # next_states = next_states.reshape([-1, self.env.dims])\n",
        "        # print(\"next_states\",next_states.shape) #[100, 4]\n",
        "        \n",
        "        ones = torch.tensor(target_distribution).type(torch.float32)\n",
        "        zeros = torch.tensor(next_states).type(torch.float32)\n",
        "        zeros_prev = torch.tensor(states).type(torch.float32)\n",
        "        # print(ones.shape, zeros.shape, zeros_prev.shape) #[100, 4] [100, 4] [100, 4]\n",
        "\n",
        "        # ####### WGAN loss\n",
        "        pred_ones = self.discriminator(ones)\n",
        "        pred_zeros = self.discriminator(zeros)\n",
        "        preds = torch.cat([pred_zeros, pred_ones], dim=0)\n",
        "        self.max_r = torch.max(preds).detach().cpu().numpy() + 0.1\n",
        "        self.min_r = torch.min(preds).detach().cpu().numpy() - 0.1\n",
        "        wgan_loss = torch.mean(pred_zeros) + torch.mean(pred_ones * (-1.))\n",
        "        aim_penalty = self.compute_aim_pen(ones, zeros_prev, zeros)\n",
        "        # grad_penalty = self.compute_grad_pen(ones, zeros)\n",
        "        loss = wgan_loss + aim_penalty  # + grad_penalty\n",
        "\n",
        "        # loss = torch.mean(- labels * pred.log() - (1 - labels) * (1. - pred).log())\n",
        "        loss.backward()\n",
        "        # utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=0.5)\n",
        "        self.discriminator_optimizer.step()\n",
        "\n",
        "    def act(self, state):\n",
        "        # state = torch.tensor(state).type(torch.float32)\n",
        "        action = self.policy.sample_action(state)\n",
        "        # a = np.squeeze(action.data.detach().numpy())\n",
        "        # s_p, r, done, _ = self.env.step(a)\n",
        "        return action\n",
        "\n",
        "\n",
        "gail = GAIL()\n",
        "# .to(device)\n",
        "gail.gather_data(num_trans=50) #500\n",
        "print('')\n",
        "for i in range(100): #500\n",
        "    if reward_to_use != 'none':\n",
        "        for _ in range(5):\n",
        "            # gail.gather_data(num_trans=500)\n",
        "            gail.optimize_discriminator() #line 31-34\n",
        "            # gail.optimize_discriminator(target_states, policy_states, policy_next_states)\n",
        "    for _ in range(10):\n",
        "        gail.gather_data(num_trans=50)\n",
        "        gail.fit_v_func()\n",
        "\n",
        "        # Useful only if using a separate policy\n",
        "        # gail.gather_data(num_trans=500)\n",
        "        # gail.optimize_policy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gail.gather_data(num_trans=50)\n",
        "print('')\n",
        "for i in range(100): #500\n",
        "    if reward_to_use != 'none':\n",
        "        for _ in range(5):\n",
        "            # gail.gather_data(num_trans=500)\n",
        "            gail.optimize_discriminator()\n",
        "            # gail.optimize_discriminator(target_states, policy_states, policy_next_states)\n",
        "    for _ in range(10):\n",
        "        gail.gather_data(num_trans=500)\n",
        "        gail.fit_v_func()\n"
      ],
      "metadata": {
        "id": "UiZwRlFA0uy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p4cPQiuoiwAH",
        "outputId": "ac521190-5da4-47f0-e9c4-692efaea26b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sssssssss tensor([[[104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         ...,\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.]],\n",
            "\n",
            "        [[104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         ...,\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.]],\n",
            "\n",
            "        [[104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         ...,\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.],\n",
            "         [104., 136., 252.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         ...,\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.]],\n",
            "\n",
            "        [[228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         ...,\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.]],\n",
            "\n",
            "        [[228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         ...,\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.],\n",
            "         [228.,  92.,  16.]]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-501d4b9b54cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sssssssss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# def act(self, state):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#     state = torch.tensor(state).type(torch.float32).reshape([-1, self.env.dims])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-982f0ba5ebb9>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# state = torch.tensor(state).type(torch.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;31m# a = np.squeeze(action.data.detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# s_p, r, done, _ = self.env.step(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d9a59830efac>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;34m\"\"\"Sample from policy\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d9a59830efac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# x = self.activ(self.h3(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (61440x3 and 2x64)"
          ]
        }
      ],
      "source": [
        "\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "model=gail\n",
        "rewards=0\n",
        "s = env.reset()\n",
        "s = torch.tensor(s.copy()).type(torch.float32)\n",
        "done = False\n",
        "while not done:\n",
        "    print(\"sssssssss\",s)\n",
        "    action = model.act(s)\n",
        "    # def act(self, state):\n",
        "    #     state = torch.tensor(state).type(torch.float32).reshape([-1, self.env.dims])\n",
        "    #     action = self.policy.sample_action(state)\n",
        "    #     # a = np.squeeze(action.data.detach().numpy())\n",
        "    #     # s_p, r, done, _ = self.env.step(a)\n",
        "    #     return action\n",
        "    # action = self.policy.sample_action(s)\n",
        "    # self.actions.append(a)\n",
        "    a = np.squeeze(action.data.detach().numpy())\n",
        "    s_p, r, done, _ = env.step(a)\n",
        "    s_p = torch.tensor(s_p).type(torch.float32)\n",
        "    s = s_p\n",
        "    rewards+=r\n",
        "print(rewards)\n",
        "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### mario"
      ],
      "metadata": {
        "id": "L6YGV3WH4VnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v3') #og v0 pixel v3\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "# env = MarioSparse(env)\n",
        "# env = MarioEarlyStop(env)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZwMMlbO14VJc",
        "outputId": "f0e34f72-f333-4a74-c24d-5261ec97397a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting nes-py\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.7.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp37-cp37m-linux_x86_64.whl size=438136 sha256=653f1432ffbb3589dd03a07c90a3bcb4cbd9b89bab1ba2cdecdaae91e7263d4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/96/0e/22a8c7dbdf412d8e988286f223b223baf0f4ad90c9e699c56d\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "AIM2_simplify_strip.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
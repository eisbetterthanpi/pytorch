{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/AIM2_simplify_strip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yWeBVzxkn0az"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/tree/main/grid_world_experiments\n",
        "# python main.py --reward aim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8YZstFpi7O0"
      },
      "source": [
        "#### buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MEOVQhzJi8wa"
      },
      "outputs": [],
      "source": [
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/buffers.py\n",
        "import random\n",
        "from typing import List, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size: int):\n",
        "        \"\"\"Implements a ring buffer (FIFO).\n",
        "        :param size: (int)  Max number of transitions to store in the buffer. When the buffer overflows the old memories are dropped.\"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._storage)\n",
        "\n",
        "    @property\n",
        "    def storage(self):\n",
        "        \"\"\"[(Union[np.ndarray, int], Union[np.ndarray, int], float, Union[np.ndarray, int], bool)]:\n",
        "         content of the replay buffer\"\"\"\n",
        "        return self._storage\n",
        "\n",
        "    @property\n",
        "    def buffer_size(self) -> int:\n",
        "        \"\"\"float: Max capacity of the buffer\"\"\"\n",
        "        return self._maxsize\n",
        "\n",
        "    def can_sample(self, n_samples: int) -> bool:\n",
        "        return len(self) >= n_samples\n",
        "\n",
        "    def is_full(self) -> int:\n",
        "        return len(self) == self.buffer_size\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def extend(self, obs_t, action, reward, obs_tp1, done):\n",
        "        for data in zip(obs_t, action, reward, obs_tp1, done):\n",
        "            if self._next_idx >= len(self._storage):\n",
        "                self._storage.append(data)\n",
        "            else:\n",
        "                self._storage[self._next_idx] = data\n",
        "            self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    # at staticmethod\n",
        "    # def _normalize_obs(obs: np.ndarray,\n",
        "    #                    env: Optional[VecNormalize] = None) -> np.ndarray:\n",
        "    #     \"\"\"\n",
        "    #     Helper for normalizing the observation.\n",
        "    #     \"\"\"\n",
        "    #     if env is not None:\n",
        "    #         return env.normalize_obs(obs)\n",
        "    #     return obs\n",
        "    #\n",
        "    # at staticmethod\n",
        "    # def _normalize_reward(reward: np.ndarray,\n",
        "    #                       env: Optional[VecNormalize] = None) -> np.ndarray:\n",
        "    #     \"\"\"\n",
        "    #     Helper for normalizing the reward.\n",
        "    #     \"\"\"\n",
        "    #     if env is not None:\n",
        "    #         return env.normalize_reward(reward)\n",
        "    #     return reward\n",
        "\n",
        "    def _encode_sample(self, idxes: Union[List[int], np.ndarray]):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "\n",
        "        obses_t = np.array(obses_t)\n",
        "        actions = np.array(actions)\n",
        "        obses_tp1 = np.array(obses_tp1)\n",
        "        return (torch.tensor(obses_t).type(torch.float),\n",
        "                torch.tensor(actions).type(torch.float),\n",
        "                torch.tensor(rewards).type(torch.float),\n",
        "                torch.tensor(obses_tp1).type(torch.float),\n",
        "                torch.tensor(dones).type(torch.float))\n",
        "\n",
        "    def sample(self, batch_size: int, **_kwargs):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        :param batch_size: (int) How many transitions to sample.\n",
        "        :return:\n",
        "            - obs_batch: (np.ndarray) batch of observations\n",
        "            - act_batch: (numpy float) batch of actions executed given obs_batch\n",
        "            - rew_batch: (numpy float) rewards received as results of executing act_batch\n",
        "            - next_obs_batch: (np.ndarray) next set of observations seen after executing act_batch\n",
        "            - done_mask: (numpy bool) done_mask[i] = 1 if executing act_batch[i] resulted in the end of an episodeand 0 otherwise.\"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu5AXAfBpexp"
      },
      "source": [
        "#### grid_mdp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "NbXRzYNkpbnM"
      },
      "outputs": [],
      "source": [
        "# @title GridMDP\n",
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/grid_mdp.py\n",
        "import numpy as np\n",
        "\n",
        "class GridMDP:\n",
        "    def __init__(self):\n",
        "        self.x_dim = 10\n",
        "        self.y_dim = 10\n",
        "        self.num_states = self.x_dim * self.y_dim\n",
        "        self.start_state = np.asarray([0, 0])\n",
        "        self._state = self.start_state\n",
        "        self.action_space = [0, 1, 2, 3]\n",
        "        self.target_state = [np.asarray([9, 9])]\n",
        "        self.time_limit = 15\n",
        "        self.t = 0\n",
        "        self._rewards = np.zeros(shape=(self.x_dim, self.y_dim), dtype=np.float)\n",
        "        for t in self.target_state:\n",
        "            self._rewards[t[0], t[1]] = 1.\n",
        "\n",
        "    def target_distribution(self, weight=3.) -> np.ndarray:\n",
        "        # dist = np.exp(self._rewards * weight)\n",
        "        dist = self._rewards * weight\n",
        "        dist /= np.sum(dist)\n",
        "        return dist\n",
        "\n",
        "    def reward(self):\n",
        "        return self._rewards[self._state[0], self._state[1]]\n",
        "        # if self._state == self.target_state:\n",
        "        #     return 1.\n",
        "        # return 0.\n",
        "\n",
        "\n",
        "class MazeWorld(GridMDP):\n",
        "    \"\"\"Maze World is the same grid world as above But it has multiple goals, one of which is unreachable\"\"\"\n",
        "    def __init__(self):\n",
        "        super(MazeWorld, self).__init__()\n",
        "        self.start_state = np.asarray([1, 1])\n",
        "        self._state = self.start_state\n",
        "        self.target_state = [np.asarray([1, 4])]  # , np.asarray([8, 1]), np.asarray([6, 6]), np.asarray([4, 9])]\n",
        "        self.time_limit = 100\n",
        "        self._rewards = np.zeros(shape=(self.x_dim, self.y_dim), dtype=np.float)\n",
        "        for c, t in enumerate(self.target_state):\n",
        "            self._rewards[t[0], t[1]] = 1. / (c + 1.)\n",
        "\n",
        "    def sample_transitions(self, num=50):\n",
        "        \"\"\"sample valid transitions from the mdp\"\"\"\n",
        "        transitions = []\n",
        "        temp_state = self._state\n",
        "        temp_time = self.t\n",
        "        temp_done = self._done\n",
        "        self._done = False\n",
        "        self.t = 0\n",
        "        while len(transitions) < num:\n",
        "            state = np.random.randint(self.min_state, self.max_state)\n",
        "            act = np.random.choice(self.action_space)\n",
        "            self._state = np.copy(state)\n",
        "            next_state, _, _, _ = self.step(act)\n",
        "            if not np.equal(state, next_state).all():\n",
        "                transitions.append([state, next_state])\n",
        "            self.t = 0\n",
        "            self._done = False\n",
        "        self._state = temp_state\n",
        "        self._done = temp_done\n",
        "        self.t = temp_time\n",
        "        states, next_states = (np.asarray(x) for x in zip(*transitions))\n",
        "        return states, next_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPN3_5JrpAd7"
      },
      "source": [
        "#### policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-pAnASeqo8xA"
      },
      "outputs": [],
      "source": [
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/policy.py\n",
        "\"\"\"\n",
        "Implements a basic MLP network and the soft Q learning agent\n",
        "\"\"\"\n",
        "from abc import ABC\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class Mepol(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size=32):\n",
        "        super(Mepol, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(s_size, h_size), nn.ReLU(),\n",
        "            nn.Linear(h_size, a_size),\n",
        "            # nn.Linear(h_size, h_size*2), nn.ReLU(),\n",
        "            # nn.Linear(h_size*2, a_size),\n",
        "            # nn.Softmax(dim=1),\n",
        "            nn.Softmax(dim=0),\n",
        "        )\n",
        "    \n",
        "    def forward(self, state): # og discrete\n",
        "        # state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.model(state).cpu()\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample() # can't use action = np.argmax(m) use  m.sample(), sample an action with prob dist P(.|s)\n",
        "        return action.item()\n",
        "\n",
        "class MlpNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=1, activ=f.relu, n_units=64):\n",
        "        super(MlpNetwork, self).__init__()\n",
        "        # n_units = 512\n",
        "        self.h1 = nn.Linear(input_dim, n_units)\n",
        "        self.h2 = nn.Linear(n_units, n_units)\n",
        "        # self.h3 = nn.Linear(n_units, n_units)\n",
        "        self.out = nn.Linear(n_units, output_dim)\n",
        "        self.activ = activ\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.activ(self.h1(x))\n",
        "        x = self.activ(self.h2(x))\n",
        "        # x = self.activ(self.h3(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        x = f.log_softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SoftQLearning(nn.Module, ABC):\n",
        "    \"\"\"Learns a soft Q-function. Samples from softmax distribution of Q-values for policy\"\"\"\n",
        "    def __init__(self, x_dim=1, out_dim=2, max_state=9., min_state=0, ent_coef=0.01, target_update=1e-1):\n",
        "        super(SoftQLearning, self).__init__()\n",
        "        self.diff_state = np.array(max_state - min_state).astype(np.float32)\n",
        "        self.mean_state = np.asarray(self.diff_state / 2 + min_state).astype(np.float32)\n",
        "        self.input_dim = x_dim\n",
        "        self.num_actions = out_dim\n",
        "        self.alpha = ent_coef\n",
        "        self.q = MlpNetwork(self.input_dim, output_dim=out_dim, n_units=64)\n",
        "        self.q_target = MlpNetwork(self.input_dim, output_dim=out_dim, n_units=64)\n",
        "        self.target_params = self.q_target.parameters()\n",
        "        self.q_params = self.q.parameters()\n",
        "        self.target_update_rate = target_update\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.q_params\n",
        "\n",
        "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.type(torch.float32)\n",
        "        x = (x - self.mean_state) / self.diff_state\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.normalize(x)\n",
        "        q = self.q(x)\n",
        "        v = self.alpha * torch.logsumexp(q / self.alpha, dim=-1)\n",
        "        # self.alpha = max(0.01, 0.99 * self.alpha + 0.01 * (torch.mean(torch.abs(q)).detach().numpy() / 10.))\n",
        "        qt = self.q_target(x)\n",
        "        vt = self.alpha * torch.logsumexp(qt / self.alpha, dim=-1)\n",
        "        return q, v, qt, vt\n",
        "        # return q\n",
        "\n",
        "    # def pi_loss(self, x: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
        "    #     \"\"\"Return log_pi for the policy gradient\"\"\"\n",
        "    #     x = self.normalize(x)\n",
        "    #     logits = self.pi(x)\n",
        "    #     actions = actions.type(torch.long)\n",
        "    #     log_pi = logits.gather(dim=-1, index=actions)\n",
        "    #     return log_pi\n",
        "\n",
        "    def sample_action(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Sample from policy\"\"\"\n",
        "        x = self.normalize(x)\n",
        "        q = self.q(x)\n",
        "        v = self.alpha * torch.logsumexp(q / self.alpha, dim=-1)\n",
        "        logits = 1. / self.alpha * (q - v)\n",
        "        pi = torch.exp(logits)\n",
        "        action = pi.multinomial(1)\n",
        "        return action\n",
        "\n",
        "    def entropy(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.normalize(x)\n",
        "        q = self.q(x)\n",
        "        v = self.alpha * torch.logsumexp(q / self.alpha, dim=-1)\n",
        "        logits = 1. / self.alpha * (q - torch.unsqueeze(v, dim=-1))\n",
        "        entropy_kl = torch.sum(torch.log(torch.ones_like(logits) / self.num_actions) - logits, dim=-1)\n",
        "        # pi = torch.exp(logits)\n",
        "        # pisum = torch.sum(pi, dim=-1)\n",
        "        # entropy = -torch.sum(pi * logits, dim=-1)\n",
        "        return entropy_kl\n",
        "\n",
        "    def update_target(self):\n",
        "        \"\"\"update the target network using polyak averaging\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for c, t in zip(self.q.parameters(), self.q_target.parameters()):\n",
        "                t.data.copy_((1. - self.target_update_rate) * t.data + self.target_update_rate * c.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGZXEC_dovUB"
      },
      "source": [
        "#### mian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKO63-lBot97",
        "outputId": "255cfd90-3dc9-4fdf-8c8d-2a98b0a10514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47166\n",
            "aim\n",
            "self.s_size 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/iDurugkar/adversarial-intrinsic-motivation/blob/main/grid_world_experiments/main.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "# from torch.nn import utils\n",
        "import torch.nn.functional as f\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "seed=1123\n",
        "reward='aim' # ['gail', 'airl', 'fairl', 'aim', 'none']\n",
        "dir='/content'\n",
        "\n",
        "torch.set_default_dtype(torch.float32)\n",
        "# Set random seeds\n",
        "seed = 42 * seed\n",
        "print(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed = seed\n",
        "np.random.seed = seed\n",
        "reward_to_use = reward  # use one of ['gail', 'airl', 'fairl', 'none']\n",
        "print(reward_to_use)\n",
        "\n",
        "def wasserstein_reward(d):\n",
        "    return d\n",
        "reward_dict = {'aim': wasserstein_reward}\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"The discriminator used to learn the potentials or the reward functions\"\"\"\n",
        "    def __init__(self, x_dim=1, max_state=10., min_state=0):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.mean_state = torch.tensor((max_state - min_state) / 2 + min_state, dtype=torch.float32)\n",
        "        self.diff_state = torch.tensor(max_state - min_state, dtype=torch.float32)\n",
        "        self.input_dim = x_dim\n",
        "        self.d = MlpNetwork(self.input_dim, n_units=64)  # , activ=f.tanh)\n",
        "\n",
        "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.type(torch.float32)\n",
        "        x = (x - self.mean_state) / self.diff_state\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalize(x)\n",
        "        output = self.d(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "def to_one_hot(x: torch.Tensor, num_vals) -> torch.Tensor:\n",
        "    \"\"\"Convert tensor to one-hot encoding\"\"\"\n",
        "    if type(x) is not torch.Tensor:\n",
        "        # x = torch.tensor(x)\n",
        "        x = x.clone().detach()\n",
        "    x = x.type(torch.long)\n",
        "    x_one_hot = torch.zeros((x.shape[0], num_vals), dtype=torch.float32)\n",
        "    x_one_hot = x_one_hot.scatter(1, x, 1.)\n",
        "    return x_one_hot\n",
        "\n",
        "import gym\n",
        "# !pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "# import gym_pygame\n",
        "\n",
        "\n",
        "class GAIL:\n",
        "    \"\"\"Class to take the continuous MDP and use gail to match given target distribution\"\"\"\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(\"CartPole-v1\")\n",
        "        # self.env = gym.make(\"Pendulum-v0\") #continuous\n",
        "        # self.env = gym.make(\"MountainCar-v0\") #discrete\n",
        "        self.s_size = self.env.observation_space.shape[0]\n",
        "        self.a_size = self.env.action_space.n\n",
        "        max_state = 10\n",
        "        min_state = 0\n",
        "        # self.policy = SoftQLearning(x_dim=self.env.dims, out_dim=len(self.env.action_space),\n",
        "        #     max_state=self.env.max_state, min_state=self.env.min_state, ent_coef=.3, target_update=3e-2)\n",
        "        self.policy = SoftQLearning(x_dim=self.s_size, out_dim=self.a_size, max_state=max_state, min_state=min_state, ent_coef=.3, target_update=3e-2)\n",
        "        # self.policy = Mepol(self.s_size, self.a_size)\n",
        "\n",
        "        # self.discriminator = Discriminator(x_dim=self.env.dims, max_state=self.env.max_state,\n",
        "        #     min_state=self.env.min_state)\n",
        "        print(\"self.s_size\",self.s_size)\n",
        "        self.discriminator = Discriminator(x_dim=self.s_size, max_state=max_state, min_state=min_state)\n",
        "        self.discount = 0.99\n",
        "        self.check_state = set()\n",
        "        self.agent_buffer = ReplayBuffer(size=5000)\n",
        "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters())  # , lr=3e-4)\n",
        "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters())  # , lr=1e-4)\n",
        "        self.max_r = 0.\n",
        "        self.min_r = -1.\n",
        "\n",
        "    def gather_data(self, num_trans=100) -> None:\n",
        "        t = 0\n",
        "        while t < num_trans:\n",
        "            s = self.env.reset()\n",
        "            s = torch.tensor(s).type(torch.float32)\n",
        "            done = False\n",
        "            while not done:\n",
        "                # self.states.append(deepcopy(s))\n",
        "                # print(\"sssssssss\",s)\n",
        "                action = self.policy.sample_action(s)\n",
        "                # self.actions.append(a)\n",
        "                a = np.squeeze(action.data.detach().numpy())\n",
        "                s_p, r, done, _ = self.env.step(a)\n",
        "                s_p = torch.tensor(s_p).type(torch.float32)\n",
        "                # d = self.discriminator(sp)\n",
        "                # i_r = gail_reward(d)\n",
        "                # self.next_states.append(deepcopy(s))\n",
        "                # self.rewards.append(i_r)  # deepcopy(r))\n",
        "                # self.dones.append(deepcopy(done))\n",
        "                # print(s.squeeze(), action.reshape([-1]).detach(), r, s_p.squeeze(), done)\n",
        "                # tensor([ 0.0144, -0.0008, -0.0287,  0.0008]) tensor([0]) 1.0 tensor([ 0.0144, -0.1955, -0.0286,  0.2843]) False\n",
        "                self.agent_buffer.add(s.squeeze(), action.reshape([-1]).detach(), r, s_p.squeeze(), done)\n",
        "                if s_p not in self.check_state:\n",
        "                    self.check_state.add(s_p)\n",
        "                    # self.target_buffer.add(s, a, r, s_p, done)\n",
        "                s = s_p\n",
        "                t += 1\n",
        "            # self.states.append(s)\n",
        "\n",
        "\n",
        "    def compute_td_targets(self, states, next_states, dones, rewards=None):\n",
        "        \"\"\"Compute the value of the current states and the TD target based on one step reward and value of next states\n",
        "        :return: value of current states v, TD target targets\"\"\"\n",
        "        # states = states.reshape([-1, self.env.dims])\n",
        "        states = states.reshape([-1, self.s_size])\n",
        "        # next_states = next_states.reshape([-1, self.env.dims])\n",
        "        next_states = next_states.reshape([-1, self.s_size])\n",
        "        v = self.policy(states)[0]\n",
        "        v_prime = self.policy(next_states)[-1]\n",
        "        if rewards is not None:\n",
        "            dones = rewards.type(torch.float32).reshape([-1, 1])\n",
        "        else:\n",
        "            dones = dones.type(torch.float32).reshape([-1, 1])\n",
        "        reward_func = reward_dict[reward_to_use]\n",
        "        if reward_func is not None:\n",
        "            # d0 = self.discriminator(states)\n",
        "            d1 = self.discriminator(next_states)\n",
        "            # Compute rewards\n",
        "            # r0 = reward_func(d0)\n",
        "            r1 = reward_func(d1)\n",
        "            rewards = rewards.type(torch.float32).reshape([-1, 1]) + ((r1 - self.max_r) / (self.max_r - self.min_r))\n",
        "        targets = rewards.type(torch.float32).reshape([-1, 1])\n",
        "        # print(\"in compute_td_targets\",dones, self.discount, v_prime) #100?*[1.] 0.99 tensor([-0.6525, -0.7356]\n",
        "        targets += (1. - dones) * self.discount * v_prime.reshape([-1, 1])\n",
        "        return v, targets.detach()\n",
        "\n",
        "    def fit_v_func(self):\n",
        "        \"\"\"This function will train the value function using the collected data\"\"\"\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        s, a, r, s_p, dones = self.agent_buffer.sample(100)\n",
        "        q, targets = self.compute_td_targets(s, s_p, dones, rewards=r)\n",
        "        actions = torch.tensor(a, dtype=torch.long)\n",
        "        v = q.gather(dim=-1, index=actions)\n",
        "        loss = torch.mean(0.5 * (targets - v) ** 2)\n",
        "        loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        self.policy.update_target()\n",
        "        return\n",
        "\n",
        "    def optimize_policy(self):\n",
        "        \"\"\"This function will optimize the policy to maximize returns Based on collected data\"\"\"\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        s, a, r, s_p, dones = self.agent_buffer.sample(100)\n",
        "        v, targets = self.compute_td_targets(s, s_p, dones, rewards=r)\n",
        "        advantages = (targets - v).detach()\n",
        "        a = a.reshape([-1, 1]).detach()\n",
        "        neg_log_pi = -1. * self.policy.pi_loss(s.reshape([-1, self.env.dims]), a)\n",
        "        entropy_kl = self.policy.entropy(s.reshape([-1, self.env.dims]))\n",
        "        loss = torch.mean(advantages * neg_log_pi) + 1e-1 * torch.mean(entropy_kl)\n",
        "        loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        return\n",
        "\n",
        "    def compute_aim_pen(self, target_state, prev_state, next_state_state, lambda_=10.):\n",
        "        \"\"\"Computes values of the discriminator at different points and constraints the difference to be 0.1\"\"\"\n",
        "        prev_out = self.discriminator(prev_state)\n",
        "        next_out = self.discriminator(next_state_state)\n",
        "        penalty = lambda_ * torch.max(torch.abs(next_out - prev_out) - 0.1, torch.tensor(0.)).pow(2).mean()\n",
        "        return penalty\n",
        "\n",
        "\n",
        "    def optimize_discriminator(self):\n",
        "        \"\"\"Optimize the discriminator based on the memory and target_distribution\"\"\"\n",
        "        num_samples = 100\n",
        "        self.discriminator_optimizer.zero_grad()\n",
        "        # _, _, _, target_distribution, _ = self.target_buffer.sample(100)\n",
        "        # target_dist = np.reshape(self.env.target_distribution(), (-1,))\n",
        "        target_dist=np.array([4,0,0,0])\n",
        "        # import torch.nn.functional as f\n",
        "        # f.normalize(input, p=2, dim=2)\n",
        "        p=target_dist/sum(target_dist) # [4,0,0,0]\n",
        "        # print(\"p\",p)\n",
        "        # target_distribution = np.random.choice(target_dist.shape[0], num_samples, p=target_dist)\n",
        "        target_distribution = np.random.choice(target_dist.shape[0], num_samples, p=p)\n",
        "        # print(\"target_distribution\",target_distribution) #[100* 0]\n",
        "        states, _, _, next_states, _ = self.agent_buffer.sample(num_samples)\n",
        "        # target_distribution = sample_target_distribution(mean=self.env.target_mean, std=self.env.target_std, num=100)\n",
        "        target_distribution = target_distribution.reshape([-1, 1])\n",
        "        # if self.env.dims > 1:\n",
        "        if True:\n",
        "            # target_distribution = np.concatenate([target_distribution, target_distribution], axis=-1)\n",
        "            target_distribution = np.tile(target_distribution,(1,self.s_size))\n",
        "            # print(\"target_distribution\",target_distribution)\n",
        "            # target_distribution[:, 0] = target_distribution[:, 0] // self.env.y_dim\n",
        "            # target_distribution[:, 1] = target_distribution[:, 1] % self.env.y_dim\n",
        "        # next_states = next_states.reshape([-1, self.env.dims])\n",
        "        # print(\"next_states\",next_states.shape) #[100, 4]\n",
        "        # ones = torch.tensor(target_distribution).type(torch.float32).reshape([-1, self.env.dims])\n",
        "        # zeros = torch.tensor(next_states).type(torch.float32).reshape([-1, self.env.dims])\n",
        "        # zeros_prev = torch.tensor(states).type(torch.float32).reshape([-1, self.env.dims])\n",
        "        ones = torch.tensor(target_distribution).type(torch.float32)\n",
        "        zeros = torch.tensor(next_states).type(torch.float32)\n",
        "        zeros_prev = torch.tensor(states).type(torch.float32)\n",
        "        # print(ones.shape, zeros.shape, zeros_prev.shape) #[100, 2] [100, 4] [100, 4]\n",
        "\n",
        "        if reward_to_use == 'aim':\n",
        "            # ####### WGAN loss\n",
        "            pred_ones = self.discriminator(ones)\n",
        "            pred_zeros = self.discriminator(zeros)\n",
        "            preds = torch.cat([pred_zeros, pred_ones], dim=0)\n",
        "            self.max_r = torch.max(preds).detach().cpu().numpy() + 0.1\n",
        "            self.min_r = torch.min(preds).detach().cpu().numpy() - 0.1\n",
        "            wgan_loss = torch.mean(pred_zeros) + torch.mean(pred_ones * (-1.))\n",
        "            aim_penalty = self.compute_aim_pen(ones, zeros_prev, zeros)\n",
        "            # grad_penalty = self.compute_grad_pen(ones, zeros)\n",
        "            loss = wgan_loss + aim_penalty  # + grad_penalty\n",
        "\n",
        "        # loss = torch.mean(- labels * pred.log() - (1 - labels) * (1. - pred).log())\n",
        "        loss.backward()\n",
        "        # utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=0.5)\n",
        "        self.discriminator_optimizer.step()\n",
        "\n",
        "    def act(self, state):\n",
        "        # state = torch.tensor(state).type(torch.float32)\n",
        "        action = self.policy.sample_action(state)\n",
        "        # a = np.squeeze(action.data.detach().numpy())\n",
        "        # s_p, r, done, _ = self.env.step(a)\n",
        "        return action\n",
        "\n",
        "\n",
        "gail = GAIL()\n",
        "# .to(device)\n",
        "gail.gather_data(num_trans=50) #500\n",
        "print('')\n",
        "for i in range(100): #500\n",
        "    if reward_to_use != 'none':\n",
        "        for _ in range(5):\n",
        "            # gail.gather_data(num_trans=500)\n",
        "            gail.optimize_discriminator()\n",
        "            # gail.optimize_discriminator(target_states, policy_states, policy_next_states)\n",
        "    for _ in range(10):\n",
        "        gail.gather_data(num_trans=50)\n",
        "        gail.fit_v_func()\n",
        "\n",
        "        # Useful only if using a separate policy\n",
        "        # gail.gather_data(num_trans=500)\n",
        "        # gail.optimize_policy()\n",
        "    # if i % 10 == 0:\n",
        "    # if (i + 1) % 10 == 0:\n",
        "    #     # gather more data if you want to see exactly what the agent's policy is\n",
        "    #     # gail.gather_data()\n",
        "    #     print(i + 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gail.gather_data(num_trans=50)\n",
        "print('')\n",
        "for i in range(100): #500\n",
        "    if reward_to_use != 'none':\n",
        "        for _ in range(5):\n",
        "            # gail.gather_data(num_trans=500)\n",
        "            gail.optimize_discriminator()\n",
        "            # gail.optimize_discriminator(target_states, policy_states, policy_next_states)\n",
        "    for _ in range(10):\n",
        "        gail.gather_data(num_trans=500)\n",
        "        gail.fit_v_func()\n"
      ],
      "metadata": {
        "id": "UiZwRlFA0uy5",
        "outputId": "9590d936-3cf7-49dd-ad89-b678b534f54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4cPQiuoiwAH",
        "outputId": "73805aee-eeda-42a0-d9d1-a97371f5da11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sssssssss tensor([-0.0424,  0.0032, -0.0492, -0.0249])\n",
            "sssssssss tensor([-0.0423, -0.1912, -0.0497,  0.2519])\n",
            "sssssssss tensor([-0.0461, -0.3855, -0.0447,  0.5285])\n",
            "sssssssss tensor([-0.0539, -0.5800, -0.0341,  0.8068])\n",
            "sssssssss tensor([-0.0655, -0.7747, -0.0180,  1.0885])\n",
            "sssssssss tensor([-0.0810, -0.9695,  0.0038,  1.3755])\n",
            "sssssssss tensor([-0.1003, -1.1647,  0.0313,  1.6694])\n",
            "sssssssss tensor([-0.1236, -1.3602,  0.0647,  1.9716])\n",
            "sssssssss tensor([-0.1508, -1.5559,  0.1041,  2.2836])\n",
            "sssssssss tensor([-0.1820, -1.7518,  0.1498,  2.6064])\n",
            "sssssssss tensor([-0.2170, -1.9478,  0.2019,  2.9409])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "s = env.reset()\n",
        "s = torch.tensor(s).type(torch.float32)\n",
        "done = False\n",
        "model=gail\n",
        "while not done:\n",
        "    print(\"sssssssss\",s)\n",
        "    action = model.act(s)\n",
        "    # def act(self, state):\n",
        "    #     state = torch.tensor(state).type(torch.float32).reshape([-1, self.env.dims])\n",
        "    #     action = self.policy.sample_action(state)\n",
        "    #     # a = np.squeeze(action.data.detach().numpy())\n",
        "    #     # s_p, r, done, _ = self.env.step(a)\n",
        "    #     return action\n",
        "    # action = self.policy.sample_action(s)\n",
        "    # self.actions.append(a)\n",
        "    a = np.squeeze(action.data.detach().numpy())\n",
        "    s_p, r, done, _ = env.step(a)\n",
        "    s_p = torch.tensor(s_p).type(torch.float32)\n",
        "    s = s_p\n",
        "\n",
        "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "AIM2_simplify_strip.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
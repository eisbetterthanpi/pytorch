{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/6_Attention_is_All_You_Need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAyqkMjq6T2C"
      },
      "outputs": [],
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://www.mihaileric.com/posts/transformers-attention-in-disguise/\n",
        "# https://jalammar.github.io/illustrated-transformer/\n",
        "# http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "# position embedding has a \"vocabulary\" size of 100, model can accept sentences up to 100 tokens long\n",
        "# we use a learned positional encoding instead of a static one\n",
        "# we use the standard Adam optimizer with a static learning rate instead of one with warm-up and cool-down steps\n",
        "# we do not use label smoothing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V1teyZuwff9_"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64c91cf87c13c0e83586b8e66e4d74e/translation_transformer.ipynb\n",
        "\n",
        "# https://github.com/pytorch/data\n",
        "%pip install portalocker\n",
        "%pip install torchdata\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -U torchdata\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BFat7RgKSwR",
        "outputId": "0f2c1bca-d05e-4690-8597-2a431395f43e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "# modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "de_vocab.set_default_index(UNK_IDX)\n",
        "en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(en_transform(tgt_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# vocab_transform = {SRC_LANGUAGE:de_vocab, TGT_LANGUAGE:en_vocab}\n",
        "# text_transform = {SRC_LANGUAGE:de_transform, TGT_LANGUAGE:en_transform}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYW9bOrcEubq"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import math\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq_length=512, dropout=0.1):\n",
        "        super(PositionalEncoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_seq_length, embedding_dim)\n",
        "        position = torch.arange(0, max_seq_length).unsqueeze(1) # https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x*math.sqrt(self.embedding_dim) # ?\n",
        "        # return self.dropout(x + Variable(self.pe[:, :x.size(1)], requires_grad=False).to(x.device))\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        batch_size = query.shape[0]\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "        # print(\"mha fwd1\",query.shape, key.shape, value.shape)\n",
        "        Q = self.fc_q(query) #Q = [batch size, query len, hid dim]\n",
        "        K = self.fc_k(key) #K = [batch size, key len, hid dim]\n",
        "        V = self.fc_v(value) #V = [batch size, value len, hid dim]\n",
        "        # print(\"mha fwd2\",Q.shape,K.shape,V.shape)\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) #Q = [batch size, n heads, query len, head dim]\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) #K = [batch size, n heads, key len, head dim]\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) #V = [batch size, n heads, value len, head dim]\n",
        "        # scaled dot-product attention\n",
        "        # print(\"mha fwd3\",Q.shape,K.shape,V.shape)\n",
        "        energy = torch.matmul(Q, K.transpose(2,3)) / self.scale #energy = [batch size, n heads, query len, key len]\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy, dim = -1) #attention = [batch size, n heads, query len, key len]\n",
        "        # why dropout applied directly to the attn?\n",
        "        # print(\"mha fwd\",self.dropout(attention).shape, V.shape) # [11, 8, 11, 11], [11, 8, 1, 32]\n",
        "        x = torch.matmul(self.dropout(attention), V) #x = [batch size, n heads, query len, head dim]\n",
        "        x = x.transpose(1, 2).contiguous() #x = [batch size, query len, n heads, head dim]\n",
        "        x = x.view(batch_size, -1, self.hid_dim) #x = [batch size, query len, hid dim]\n",
        "        x = self.fc_o(x) #x = [batch size, query len, hid dim]\n",
        "        return x, attention\n",
        "\n",
        "# maybe noneed?\n",
        "class PositionwiseFeedforwardLayer(nn.Module): # input is transformed from hid_dim to pf_dim,  pf_dim>>hid_dim. then ReLU, dropout then transform back into a hid_dim representation\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x): #x = [batch size, seq len, hid dim]\n",
        "        x = self.dropout(torch.relu(self.fc_1(x))) #x = [batch size, seq len, pf dim]\n",
        "        # x = self.dropout(torch.gelu(self.fc_1(x))) #bert use gelu\n",
        "        x = self.fc_2(x) #x = [batch size, seq len, hid dim]\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src)) #src = [batch size, src len, hid dim]\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src)) #src = [batch size, src len, hid dim]\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg)) #trg = [batch size, trg len, hid dim]\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg)) #trg = [batch size, trg len, hid dim]\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        return trg, attention\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim) # learnt positional embeddings\n",
        "        self.pos_enc = PositionalEncoder(hid_dim, max_length, dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) # [batch size, src len]\n",
        "        # src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)) #src = [batch size, src len, hid dim]\n",
        "        src = self.dropout(self.pos_enc(self.tok_embedding(src) * self.scale)) # sine pos enc\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask) #src = [batch size, src len, hid dim]\n",
        "        return src\n",
        "\n",
        "# src_mask, same shape as the source sentence,\n",
        "# value of 1 when the token in the source sentence is not <pad> token,\n",
        "# for encoder layers to mask the multi-head attention mechanisms,\n",
        "# calculate and apply attention over the source sentence, so dun pay attention to <pad> tokens\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.pos_enc = PositionalEncoder(hid_dim, max_length, dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) #pos = [batch size, trg len]\n",
        "        # trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)) #trg = [batch size, trg len, hid dim]\n",
        "        trg = self.dropout(self.pos_enc(self.tok_embedding(trg))) * self.scale # sine pos enc\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        output = self.fc_out(trg) #output = [batch size, trg len, output dim]\n",
        "        return output, attention\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src): #src = [batch size, src len]\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2).to(self.device) #src_mask = [batch size, 1, 1, src len]\n",
        "        return src_mask\n",
        "\n",
        "# subsequent mask, tril, nth in tgt can only see up to nth in\n",
        "# bitwise & with trg_pad_mask: dun pay attn to <pad>\n",
        "    def make_trg_mask(self, trg): #trg = [batch size, trg len]\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2).to(self.device) #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool() #trg_sub_mask = [trg len, trg len]\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "        src_mask = self.make_src_mask(src) #src_mask = [batch size, 1, 1, src len]\n",
        "        trg_mask = self.make_trg_mask(trg) #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        enc_src = self.encoder(src, src_mask) #enc_src = [batch size, src len, hid dim]\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "HID_DIM = 512 #256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# emb_size = 512\n",
        "# nhead = 8\n",
        "# dim_feedforward = 512\n",
        "# num_encoder_layers = 3\n",
        "# num_decoder_layers = 3\n",
        "# transformer = Transformer(num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward).to(device)\n",
        "\n",
        "\n",
        "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "model = Seq2Seq(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
        "\n",
        "# def initialize_weights(m):\n",
        "#     if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "#         nn.init.xavier_uniform_(m.weight.data)\n",
        "# model.apply(initialize_weights);\n",
        "\n",
        "for p in model.parameters():\n",
        "# for p in self.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "    src_emb = self.pos_enc(self.src_tok_emb(src))\n",
        "    tgt_emb = self.pos_enc(self.tgt_tok_emb(tgt))\n",
        "    outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "    return self.generator(outs)\n",
        "\n",
        "def encode(self, src, src_mask):\n",
        "    return self.transformer.encoder(self.pos_enc(self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "def decode(self, tgt, memory, tgt_mask):\n",
        "    return self.transformer.decoder(self.pos_enc(self.tgt_tok_emb(tgt)), memory, tgt_mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ug0_nK0xKQxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP0IVy1kADYz"
      },
      "source": [
        "##text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiB5qU-oEubr"
      },
      "source": [
        "### Mutli Head Attention Layer\n",
        "\n",
        "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-attention.png?raw=1)\n",
        "\n",
        "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
        "\n",
        "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$\n",
        "\n",
        "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
        "\n",
        "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
        "\n",
        "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
        "\n",
        "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
        "\n",
        "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`.\n",
        "\n",
        "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
        "\n",
        "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyRWxCL4Eubs"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The objective of the decoder is to take the encoded representation of the source sentence, $Z$, and convert it into predicted tokens in the target sentence, $\\hat{Y}$. We then compare $\\hat{Y}$ with the actual tokens in the target sentence, $Y$, to calculate our loss, which will be used to calculate the gradients of our parameters and then use our optimizer to update our weights in order to improve our predictions.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-decoder.png?raw=1)\n",
        "\n",
        "The decoder is similar to encoder, however it now has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
        "\n",
        "The decoder uses positional embeddings and combines - via an elementwise sum - them with the scaled embedded target tokens, followed by dropout. Again, our positional encodings have a \"vocabulary\" of 100, which means they can accept sequences up to 100 tokens long. This can be increased if desired.\n",
        "\n",
        "The combined embeddings are then passed through the $N$ decoder layers, along with the encoded source, `enc_src`, and the source and target masks. Note that the number of layers in the encoder does not have to be equal to the number of layers in the decoder, even though they are both denoted by $N$.\n",
        "\n",
        "The decoder representation after the $N^{th}$ layer is then passed through a linear layer, `fc_out`. In PyTorch, the softmax operation is contained within our loss function, so we do not explicitly need to use a softmax layer here.\n",
        "\n",
        "As well as using the source mask, as we did in the encoder to prevent our model attending to `<pad>` tokens, we also use a target mask. This will be explained further in the `Seq2Seq` model which encapsulates both the encoder and decoder, but the gist of it is that it performs a similar operation as the decoder padding in the convolutional sequence-to-sequence model. As we are processing all of the target tokens at once in parallel we need a method of stopping the decoder from \"cheating\" by simply \"looking\" at what the next token in the target sequence is and outputting it.\n",
        "\n",
        "Our decoder layer also outputs the normalized attention values so we can later plot them to see what our model is actually paying attention to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TmhO4XzEubt"
      },
      "source": [
        "### Decoder Layer\n",
        "decoder layer has two multi-head attention layers, `self_attention` and `encoder_attention`.\n",
        "\n",
        "The first performs self-attention, as in the encoder, by using the decoder representation so far as the query, key and value. This is followed by dropout, residual connection and layer normalization. This `self_attention` layer uses the target sequence mask, `trg_mask`, in order to prevent the decoder from \"cheating\" by paying attention to tokens that are \"ahead\" of the one it is currently processing as it processes all tokens in the target sentence in parallel.\n",
        "\n",
        "The second is how we actually feed the encoded source sentence, `enc_src`, into our decoder. In this multi-head attention layer the queries are the decoder representations and the keys and values are the encoder representations. Here, the source mask, `src_mask` is used to prevent the multi-head attention layer from attending to `<pad>` tokens within the source sentence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg6a--EGEubt"
      },
      "source": [
        "### Seq2Seq\n",
        "\n",
        "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
        "\n",
        "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape **_[batch size, n heads, seq len, seq len]_**.\n",
        "\n",
        "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
        "\n",
        "$$\\begin{matrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 1\\\\\n",
        "\\end{matrix}$$\n",
        "\n",
        "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of **_[1, 0, 0, 0, 0]_** which means it can only look at the first target token. The second target token has a mask of **_[1, 1, 0, 0, 0]_** which it means it can look at both the first and second target tokens.\n",
        "\n",
        "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
        "\n",
        "$$\\begin{matrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}$$\n",
        "\n",
        "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1w_llnzEub5"
      },
      "source": [
        "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
        "\n",
        "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
        "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
        "\\end{align*}$$\n",
        "\n",
        "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{output} &= [y_1, y_2, y_3, eos]\n",
        "\\end{align*}$$\n",
        "\n",
        "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
        "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
        "\\end{align*}$$\n",
        "\n",
        "We then calculate our losses and update our parameters as is standard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMP0g1CO_-LX"
      },
      "source": [
        "##end text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TaVIwZgMEub6"
      },
      "outputs": [],
      "source": [
        "# @title train eval\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg in dataloader:\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device) #trg = [batch size, trg len]\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(src, trg[:,:-1]) #output = [batch size, trg len - 1, output dim]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim) #output = [batch size * trg len - 1, output dim]\n",
        "        trg = trg[:,1:].contiguous().view(-1) #trg = [batch size * trg len - 1]\n",
        "        loss = loss_fn(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(list(dataloader))\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device) #trg = [batch size, trg len]\n",
        "            output, _ = model(src, trg[:,:-1]) #output = [batch size, trg len - 1, output dim]\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim) #output = [batch size * trg len - 1, output dim]\n",
        "            trg = trg[:,1:].contiguous().view(-1) #trg = [batch size * trg len - 1]\n",
        "            loss = loss_fn(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(list(dataloader))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UCgV-efIfzne"
      },
      "outputs": [],
      "source": [
        "# @title translate\n",
        "\n",
        "def translate(model, src_sentence):\n",
        "    model.eval()\n",
        "    src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = torch.zeros((num_tokens, num_tokens), dtype=bool, device=device)\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src, src_mask)\n",
        "    trg_indexes = [BOS_IDX]\n",
        "    max_len = src.shape[1]+5\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == EOS_IDX: break\n",
        "    tgt_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "    return \" \".join(en_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy())))\n",
        "\n",
        "# UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymQe6xb6Eub7",
        "outputId": "91c59e9e-b242-481a-d648-d43ca49959f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 5.685, Val loss: 4.895, Epoch time = 44.724s\n",
            "A woman in a a a a a a .\n",
            "Epoch: 2, Train loss: 4.724, Val loss: 4.455, Epoch time = 43.922s\n",
            "A group of of of of of of of of .\n",
            "Epoch: 3, Train loss: 4.254, Val loss: 3.989, Epoch time = 44.762s\n",
            "A group of of of of of of of of of .\n",
            "Epoch: 4, Train loss: 3.874, Val loss: 3.693, Epoch time = 43.549s\n",
            "A group of of of of of of of of of .\n",
            "Epoch: 5, Train loss: 3.571, Val loss: 3.408, Epoch time = 43.872s\n",
            "A group of people of front of of of of of of of .\n",
            "Epoch: 6, Train loss: 3.238, Val loss: 3.126, Epoch time = 44.194s\n",
            "A crowd of a a building of a a a .\n",
            "Epoch: 7, Train loss: 2.981, Val loss: 2.935, Epoch time = 44.248s\n",
            "A crowd of a a crowd of a a building .\n",
            "Epoch: 8, Train loss: 2.773, Val loss: 2.803, Epoch time = 44.176s\n",
            "A crowd of a front of a store .\n",
            "Epoch: 9, Train loss: 2.589, Val loss: 2.661, Epoch time = 43.593s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 10, Train loss: 2.429, Val loss: 2.562, Epoch time = 44.491s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 11, Train loss: 2.293, Val loss: 2.465, Epoch time = 43.628s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 12, Train loss: 2.173, Val loss: 2.416, Epoch time = 43.635s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 13, Train loss: 2.050, Val loss: 2.327, Epoch time = 44.271s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 14, Train loss: 1.952, Val loss: 2.285, Epoch time = 44.567s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 15, Train loss: 1.861, Val loss: 2.285, Epoch time = 43.584s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 16, Train loss: 1.770, Val loss: 2.206, Epoch time = 43.573s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 17, Train loss: 1.699, Val loss: 2.217, Epoch time = 44.364s\n",
            "A crowd of a crowd of a crowd .\n",
            "Epoch: 18, Train loss: 1.635, Val loss: 2.198, Epoch time = 44.176s\n",
            "A crowd of people of a crowd .\n",
            "Epoch: 19, Train loss: 1.564, Val loss: 2.153, Epoch time = 43.617s\n",
            "A crowd of people of a crowd .\n",
            "Epoch: 20, Train loss: 1.506, Val loss: 2.149, Epoch time = 43.637s\n",
            "A crowd of people of a crowd .\n"
          ]
        }
      ],
      "source": [
        "# @title run\n",
        "# import math\n",
        "import time\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001) # lr=0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) # lr=0.0001\n",
        "\n",
        "for epoch in range(20):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = evaluate(model, val_loader, loss_fn)\n",
        "    end_time = time.time()\n",
        "    print((f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "#  A group of people standing in front of an igloo\n",
        "\n",
        "# sine pos enc, scale after pos\n",
        "# Epoch: 20, Train loss: 3.238, Val loss: 3.504, Epoch time = 44.329s\n",
        "# A person is playing a trick in a race .\n",
        "\n",
        "# sine pos enc, token,scale,pos\n",
        "# Epoch: 20, Train loss: 1.506, Val loss: 2.149, Epoch time = 43.637s\n",
        "# A crowd of people of a crowd .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7NvAns6qEub9"
      },
      "outputs": [],
      "source": [
        "# @title translate_sentence, display_attention\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    model.eval()\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]: break\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    return trg_tokens[1:], attention\n",
        "\n",
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    assert n_rows * n_cols == n_heads\n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    for i in range(n_heads):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# print(train_iter[0])\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "print(f'predicted trg = {translation}')\n",
        "display_attention(src, translation, attention)\n",
        "\n",
        "for src,trg in valid_loader: break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3wjVTOsoEucI"
      },
      "outputs": [],
      "source": [
        "# @title bleu\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    for datum in data:\n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "    return bleu_score(pred_trgs, trgs)\n",
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')\n",
        "# 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model.\n",
        "\n",
        "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "\n",
        "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    translations_done = [0] * len(src_tensor)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_tokens = output.argmax(2)[:,-1]\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\n",
        "            trg_indexes[i].append(pred_token_i)\n",
        "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                translations_done[i] = 1\n",
        "        if all(translations_done):\n",
        "            break\n",
        "\n",
        "    # Iterate through each predicted example one by one;\n",
        "    # Cut-off the portion including the after the <eos> token\n",
        "    pred_sentences = []\n",
        "    for trg_sentence in trg_indexes:\n",
        "        pred_sentence = []\n",
        "        for i in range(1, len(trg_sentence)):\n",
        "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                break\n",
        "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
        "        pred_sentences.append(pred_sentence)\n",
        "    return pred_sentences, attention\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            _trgs = []\n",
        "            for sentence in trg:\n",
        "                tmp = []\n",
        "                # Start from the first token which skips the <start> token\n",
        "                for i in sentence[1:]:\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
        "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
        "                        break\n",
        "                    tmp.append(trg_field.vocab.itos[i])\n",
        "                _trgs.append([tmp])\n",
        "            trgs += _trgs\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
        "            pred_trgs += pred_trg\n",
        "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hP0IVy1kADYz"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
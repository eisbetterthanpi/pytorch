{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/fast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "bc22ef60-9041-4c55-f9d5-5f60a3328789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://github.com/python-engineer/pytorchTutorial/blob/master/14_cnn.py\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader) # get some random training images\n",
        "images, labels = dataiter.next()\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title simplifi\n",
        "# https://github.com/JayPatwardhan/ResNet-PyTorch/blob/master/ResNet/ResNet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        self.i_downsample = i_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels*self.expansion),\n",
        "        )\n",
        "        self.i_downsample = i_downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        if self.i_downsample is not None: #downsample if needed\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity #add identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        # https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py\n",
        "        # num_blocks=[3,3,3] aka layer_list\n",
        "        # plane_list=[64,128,256,512]\n",
        "        plane_list=[64,128,256]\n",
        "        # plane_list=[16,32,64]\n",
        "        self.in_channels = plane_list[0]\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, plane_list[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(plane_list[0]), nn.ReLU(),\n",
        "            self._make_layer(ResBlock, layer_list[0], plane_list[0], stride=1),\n",
        "            self._make_layer(ResBlock, layer_list[1], plane_list[1], stride=2),\n",
        "            self._make_layer(ResBlock, layer_list[2], plane_list[2], stride=2),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Linear(plane_list[2]*ResBlock.expansion, num_classes)\n",
        "        self.cc = nn.Conv2d(plane_list[2]*ResBlock.expansion, num_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # print(\"forward x\",x.shape)\n",
        "        # x = x.reshape(x.shape[0], -1)\n",
        "        # x = self.fc(x)\n",
        "        x = self.cc(x)\n",
        "        x = torch.squeeze(x)\n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = ResNet(Bottleneck, [3,4,6,3], num_classes=10, num_channels=3).to(device)\n",
        "model = ResNet(Bottleneck, [3,3,3], num_classes=10, num_channels=3).to(device)\n",
        "# print(model)\n",
        "\n",
        "loss_list=[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h7iCNId6oXUK"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "\n",
        "# class ConvNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(ConvNet, self).__init__()\n",
        "#         plane_list=[64,128,256,512]\n",
        "#         self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "#             nn.Conv2d(3, plane_list[0], 3, 1, 1), nn.BatchNorm2d(plane_list[0]), nn.ReLU(),# nn.MaxPool2d(2, 2),\n",
        "#             nn.Conv2d(plane_list[0], plane_list[1], 5, 1, 2), nn.BatchNorm2d(plane_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "#             nn.Conv2d(plane_list[1], plane_list[2], 7, 1, 3), nn.BatchNorm2d(plane_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "#         )\n",
        "#         self.linear = nn.Sequential(\n",
        "#             # nn.Linear(16 * 8 * 8, 256), nn.ReLU(),\n",
        "#             nn.Linear(plane_list[2] * 8 * 8, 256), nn.ReLU(),\n",
        "#             # nn.Linear(plane_list[2]//16, 256), nn.ReLU(),\n",
        "#             nn.Linear(256, 64), nn.ReLU(),\n",
        "#             nn.Linear(64, 10),\n",
        "#         )\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        plane_list=[64,128,256,512]\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Conv2d(3, plane_list[0], 3, 1, 1), nn.BatchNorm2d(plane_list[0]), nn.ReLU(), #nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(plane_list[0], plane_list[1], 3, 1, 1), nn.BatchNorm2d(plane_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(plane_list[1], plane_list[2], 3, 1, 1), nn.BatchNorm2d(plane_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Conv2d(plane_list[2],64, 1, 1, 0), #nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 10, 1, 1, 0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # print(\"forward x\",x.shape)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        # x = nn.Flatten()(x)\n",
        "        # x = self.linear(x)\n",
        "        x = torch.squeeze(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "# print(model)\n",
        "\n",
        "loss_list=[]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEZCFg5YSS9J",
        "outputId": "130f96bd-6fa7-45f4-d728-077959b8c1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(64, 3, 32, 32, device=device)\n",
        "logits = model(X)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# for data, label in data_iter:\n",
        "#    optimizer.zero_grad()\n",
        "#    # Casts operations to mixed precision\n",
        "#    with torch.cuda.amp.autocast():\n",
        "#       loss = model(data)\n",
        "#    scaler.scale(loss).backward()\n",
        "#    scaler.step(optimizer)\n",
        "#    scaler.update()\n",
        "\n",
        "\n",
        "# def strain(dataloader, model, loss_fn, optimizer):\n",
        "# def strain(dataloader, model, loss_fn, optimizer, sstep=False, verbose=True):\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(sx)\n",
        "            loss = loss_fn(pred, sy)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if sstep: scheduler.step()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            loss_list.append(loss)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "# def train(dataloader, model, loss_fn, optimizer):\n",
        "def train(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "    # for batch, ((x,y), labels) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        # print(\"sx sy\",sx.shape,sy.shape)\n",
        "        pred = model(sx)\n",
        "        loss = loss_fn(pred, sy)\n",
        "        # loss = model.loss(sx,sy)\n",
        "        optimizer.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "        loss.backward() # Backpropagate gradients\n",
        "        optimizer.step() # adjust the parameters by the gradients\n",
        "        # scheduler.step()\n",
        "        # if batch % 200 == 0: # 2000\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            loss_list.append(loss)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# def test(dataloader, model, loss_fn):\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            x, y = X.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    if verbose: print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title lrfinder\n",
        "\n",
        "# https://towardsdatascience.com/super-convergence-with-just-pytorch-c223c0fc1e51\n",
        "# https://github.com/davidtvs/pytorch-lr-finder\n",
        "# # https://arxiv.org/pdf/1506.01186.pdf\n",
        "# LR range test\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "# _/\\\n",
        "\n",
        "# cyclic\n",
        "# 0.0001-0.001\n",
        "\n",
        "num_iter=20\n",
        "start_lr=0.00005\n",
        "end_lr= 0.01\n",
        "# train_batches=100\n",
        "# test_batches=20\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "\n",
        "# def lrfinder(model, optimizer, loss_fn, train_loader, start_lr=0.0001, end_lr=0.1, num_iter=20, train_batches=100, test_batches=20):\n",
        "def lrfinder(model, optimizer, loss_fn, train_loader, test_loader, start_lr=0.0001, end_lr=0.1, num_iter=20):\n",
        "    batch_size=64\n",
        "    train_batches, test_batches = 100, 20\n",
        "    train_lengths=[batch_size*train_batches,len(train_dataset)-batch_size*train_batches]\n",
        "    test_lengths=[batch_size*test_batches,len(test_dataset)-batch_size*test_batches]\n",
        "    # print(train_size,test_size) #50000 10000\n",
        "\n",
        "    for g in optimizer.param_groups: g['lr'] = start_lr\n",
        "\n",
        "    acc_list=[]\n",
        "    lr_list=[]\n",
        "    num_batches=int(np.ceil(len(train_dataset)/batch_size))\n",
        "    gamma = np.exp(np.log(end_lr/start_lr)/num_iter)\n",
        "    # gamma = np.exp(np.log(1e-4/1e-2)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "    import copy\n",
        "    model1=copy.deepcopy(model)\n",
        "    optimizer = torch.optim.AdamW(model1.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "    \n",
        "    for t in range(num_iter):\n",
        "        train_sets = torch.utils.data.random_split(train_dataset, train_lengths)[0] # https://stackoverflow.com/a/55760170\n",
        "        test_sets = torch.utils.data.random_split(test_dataset, test_lengths)[0]\n",
        "        train_loaders = torch.utils.data.DataLoader(train_sets, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "        test_loaders = torch.utils.data.DataLoader(test_sets, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        \n",
        "        print(\"iter: \",t)\n",
        "        lr=optimizer.param_groups[0][\"lr\"]\n",
        "        # lr=scheduler.get_last_lr()[0]\n",
        "        lr_list.append(lr)\n",
        "        print(\"lr: \",lr)\n",
        "        # train(train_loader, model, loss_fn, optimizer)\n",
        "        # strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "        strain(train_loader, model1, loss_fn, optimizer, verbose=False)\n",
        "        test(test_loader, model1, loss_fn)\n",
        "\n",
        "        accuracy = test(test_loader, model1, loss_fn, verbose=False)\n",
        "        print(\"accuracy: \",accuracy)\n",
        "        acc_list.append(accuracy)\n",
        "        scheduler.step()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(lr_list,acc_list)\n",
        "    plt.xscale('log')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return lr_list,acc_list\n",
        "    # # base_lr ,max_lr\n",
        "\n",
        "\n",
        "# lr_list, acc_list = lrfinder(model, optimizer, loss_fn, train_loader, start_lr, end_lr, num_iter, train_batches, test_batches)\n",
        "lr_list, acc_list = lrfinder(model, optimizer, loss_fn, train_loader, test_loader, start_lr, end_lr, num_iter)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "keNSPmbziSMa",
        "outputId": "467dc6f5-026d-4dce-9f0e-dfc95666fe41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter:  0\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 56.0%, Avg loss: 1.229621 \n",
            "\n",
            "accuracy:  0.5605\n",
            "iter:  1\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 65.8%, Avg loss: 0.964394 \n",
            "\n",
            "accuracy:  0.6578\n",
            "iter:  2\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 66.6%, Avg loss: 0.940948 \n",
            "\n",
            "accuracy:  0.6665\n",
            "iter:  3\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 71.8%, Avg loss: 0.821122 \n",
            "\n",
            "accuracy:  0.7175\n",
            "iter:  4\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 69.7%, Avg loss: 0.928130 \n",
            "\n",
            "accuracy:  0.6971\n",
            "iter:  5\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.938467 \n",
            "\n",
            "accuracy:  0.7089\n",
            "iter:  6\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 1.116704 \n",
            "\n",
            "accuracy:  0.6833\n",
            "iter:  7\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 1.101765 \n",
            "\n",
            "accuracy:  0.7019\n",
            "iter:  8\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 71.5%, Avg loss: 1.039989 \n",
            "\n",
            "accuracy:  0.7146\n",
            "iter:  9\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 73.7%, Avg loss: 0.983248 \n",
            "\n",
            "accuracy:  0.7367\n",
            "iter:  10\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 1.226495 \n",
            "\n",
            "accuracy:  0.6997\n",
            "iter:  11\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 1.154684 \n",
            "\n",
            "accuracy:  0.7218\n",
            "iter:  12\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 1.104864 \n",
            "\n",
            "accuracy:  0.7404\n",
            "iter:  13\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 1.207607 \n",
            "\n",
            "accuracy:  0.72\n",
            "iter:  14\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 1.583425 \n",
            "\n",
            "accuracy:  0.6918\n",
            "iter:  15\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 1.157473 \n",
            "\n",
            "accuracy:  0.7321\n",
            "iter:  16\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 73.5%, Avg loss: 1.129032 \n",
            "\n",
            "accuracy:  0.7351\n",
            "iter:  17\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 1.180262 \n",
            "\n",
            "accuracy:  0.7412\n",
            "iter:  18\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 1.244419 \n",
            "\n",
            "accuracy:  0.7331\n",
            "iter:  19\n",
            "lr:  5e-05\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 1.329654 \n",
            "\n",
            "accuracy:  0.7138\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATnUlEQVR4nO3df4xd5X3n8fdnjUwqViy/JitiE+Kqpm4adkGZuJWQIooE8Uq7wdqNqK1GgbaLFXWd1RYJAYpEKldRu/8sUrVWKjd1CE2Cg7z5Md06cpFIm90sIA+Sk2BHmKnpinGRGMCkqtICJt/94x5vTy9jzxnPNcZ53i/pyud8z3Oe+zyydT73/Li+qSokSe35Z+d6AJKkc8MAkKRGGQCS1CgDQJIaZQBIUqMMAElq1KAASLIpyTNJ5pLcu8j2B5Ic7F5Hkrza2/Zmb9tMr74uyZNdn19NsnoyU5IkDZGlvgeQZBVwBLgZmAcOAFur6vAp2n8KuL6qfqNb/7uq+ueLtHsE+FpV7Unyh8D3qupzK5qNJGmwIWcAG4G5qjpaVa8De4BbT9N+K/Dw6TpMEuAmYG9X+iKwecBYJEkTMiQA1gDP99bnu9pbJLkaWAc81iu/K8lskieSnDzIXw68WlUnlupTknR2XDDh/rYAe6vqzV7t6qo6luRngceS/AD40dAOk2wDtgFcdNFFH9ywYcNEByxJP+2eeuqpl6pqarw+JACOAVf11td2tcVsAf5Tv1BVx7o/jyb5C+B64H8AlyS5oDsLOGWfVbUL2AUwPT1ds7OzA4YsSTopyf9drD7kEtABYH331M5qRgf5mfFGSTYAlwKP92qXJrmwW74CuAE4XKM7z98GPtY1vR345vDpSJJWaskA6D6hbwf2Az8EHqmqQ0l2JPlor+kWYE/908eKfgGYTfI9Rgf83+89PXQPcFeSOUb3BP545dORJA215GOg7yReApKk5UvyVFVNj9f9JrAkNcoAkKRGGQCS1CgDQNKyffzzT/Lxzz95roehFZr0F8EkNeB/z710roegCfAMQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1alAAJNmU5Jkkc0nuXWT7A0kOdq8jSV7t6tcleTzJoSTfT/KrvX0eTPJcb7/rJjctSdJSlvw9gCSrgJ3AzcA8cCDJTFUdPtmmqn671/5TwPXd6o+BT1TVs0neAzyVZH9Vvdptv7uq9k5oLpKkZRhyBrARmKuqo1X1OrAHuPU07bcCDwNU1ZGqerZb/hvgRWBqZUOWJE3CkABYAzzfW5/vam+R5GpgHfDYIts2AquBv+qVP9tdGnogyYWn6HNbktkkswsLCwOGK0kaYtI3gbcAe6vqzX4xyZXAnwC/XlU/6cr3ARuADwGXAfcs1mFV7aqq6aqanpry5EGSJmVIABwDruqtr+1qi9lCd/nnpCQXA38GfLqqnjhZr6oXauQ14AuMLjVJkt4mQwLgALA+ybokqxkd5GfGGyXZAFwKPN6rrQa+Djw0frO3OysgSYDNwNNnOglJ0vIt+RRQVZ1Ish3YD6wCdlfVoSQ7gNmqOhkGW4A9VVW93W8DPgxcnuSOrnZHVR0EvpxkCghwEPjkRGYkSRpkyQAAqKp9wL6x2v1j67+zyH5fAr50ij5vGjxKSdLE+U1gSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNGhQASTYleSbJXJJ7F9n+QJKD3etIkld7225P8mz3ur1X/2CSH3R9/kH34/CSpLfJkr8JnGQVsBO4GZgHDiSZqarDJ9tU1W/32n8KuL5bvgz4DDANFPBUt+9x4HPAncCTjH5veBPwrQnNS5K0hCFnABuBuao6WlWvA3uAW0/TfivwcLf8EeDRqnqlO+g/CmxKciVwcVU9UVUFPARsPuNZSJKWbUgArAGe763Pd7W3SHI1sA54bIl913TLS/YpSTo7Jn0TeAuwt6renFSHSbYlmU0yu7CwMKluJal5QwLgGHBVb31tV1vMFv7x8s/p9j3WLS/ZZ1XtqqrpqpqempoaMFxJ0hBDAuAAsD7JuiSrGR3kZ8YbJdkAXAo83ivvB25JcmmSS4FbgP1V9QLwt0l+uXv65xPAN1c4F0nSMiz5FFBVnUiyndHBfBWwu6oOJdkBzFbVyTDYAuzpbuqe3PeVJL/LKEQAdlTVK93ybwEPAj/D6OkfnwCSpLfRkgEAUFX7GD2q2a/dP7b+O6fYdzewe5H6LPCBoQOVJE2W3wSWpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0a9N9BS1LfL6277FwPQRPgGYAkNcoAkKRGGQCS1KhBAZBkU5JnkswlufcUbW5LcjjJoSRf6Wq/kuRg7/UPSTZ32x5M8lxv23WTm5YkaSlL3gROsgrYCdwMzAMHksxU1eFem/XAfcANVXU8ybsBqurbwHVdm8uAOeDPe93fXVV7JzUZSdJwQ84ANgJzVXW0ql4H9gC3jrW5E9hZVccBqurFRfr5GPCtqvrxSgYsSZqMIQGwBni+tz7f1fquAa5J8t0kTyTZtEg/W4CHx2qfTfL9JA8kuXDwqCVJKzapm8AXAOuBG4GtwB8lueTkxiRXAtcC+3v73AdsAD4EXAbcs1jHSbYlmU0yu7CwMKHhSpKGBMAx4Kre+tqu1jcPzFTVG1X1HHCEUSCcdBvw9ap642Shql6okdeALzC61PQWVbWrqqaranpqamrAcCVJQwwJgAPA+iTrkqxmdClnZqzNNxh9+ifJFYwuCR3tbd/K2OWf7qyAJAE2A0+fwfglSWdoyaeAqupEku2MLt+sAnZX1aEkO4DZqprptt2S5DDwJqOne14GSPI+RmcQfznW9ZeTTAEBDgKfnMyUJElDDPq/gKpqH7BvrHZ/b7mAu7rX+L5/zVtvGlNVNy1zrJKkCfKbwJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjBgVAkk1Jnkkyl+TeU7S5LcnhJIeSfKVXfzPJwe4106uvS/Jk1+dXk6xe+XQkSUMtGQBJVgE7gX8DvB/YmuT9Y23WA/cBN1TVLwL/pbf576vquu710V79vwIPVNXPAceB31zZVCRJyzHkDGAjMFdVR6vqdWAPcOtYmzuBnVV1HKCqXjxdh0kC3ATs7UpfBDYvZ+CSpJUZEgBrgOd76/Ndre8a4Jok303yRJJNvW3vSjLb1U8e5C8HXq2qE6fpE4Ak27r9ZxcWFgYMV5I0xAUT7Gc9cCOwFvhOkmur6lXg6qo6luRngceS/AD40dCOq2oXsAtgenq6JjReSWrekDOAY8BVvfW1Xa1vHpipqjeq6jngCKNAoKqOdX8eBf4CuB54GbgkyQWn6VOSdBYNCYADwPruqZ3VwBZgZqzNNxh9+ifJFYwuCR1NcmmSC3v1G4DDVVXAt4GPdfvfDnxzhXORJC3DkgHQXaffDuwHfgg8UlWHkuxIcvKpnv3Ay0kOMzqw311VLwO/AMwm+V5X//2qOtztcw9wV5I5RvcE/niSE5Mknd6gewBVtQ/YN1a7v7dcwF3dq9/m/wDXnqLPo4yeMJIknQN+E1iSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMGBUCSTUmeSTKX5N5TtLktyeEkh5J8patdl+Txrvb9JL/aa/9gkueSHOxe101mSpKkIZb8TeAkq4CdwM3APHAgyUzvx91Jsh64D7ihqo4neXe36cfAJ6rq2STvAZ5Ksr+qXu22311Veyc5IUnSMEPOADYCc1V1tKpeB/YAt461uRPYWVXHAarqxe7PI1X1bLf8N8CLwNSkBi9JOnNDAmAN8Hxvfb6r9V0DXJPku0meSLJpvJMkG4HVwF/1yp/tLg09kOTCZY5dkrQCk7oJfAGwHrgR2Ar8UZJLTm5MciXwJ8CvV9VPuvJ9wAbgQ8BlwD2LdZxkW5LZJLMLCwsTGq4kaUgAHAOu6q2v7Wp988BMVb1RVc8BRxgFAkkuBv4M+HRVPXFyh6p6oUZeA77A6FLTW1TVrqqarqrpqSmvHknSpAwJgAPA+iTrkqwGtgAzY22+wejTP0muYHRJ6GjX/uvAQ+M3e7uzApIE2Aw8vYJ5SJKWacmngKrqRJLtwH5gFbC7qg4l2QHMVtVMt+2WJIeBNxk93fNyko8DHwYuT3JH1+UdVXUQ+HKSKSDAQeCTk56cJOnUlgwAgKraB+wbq93fWy7gru7Vb/Ml4Eun6POm5Q5WkjQ5fhNYkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjBgVAkk1Jnkkyl+TeU7S5LcnhJIeSfKVXvz3Js93r9l79g0l+0PX5B92Pw0uS3iZL/iZwklXATuBmYB44kGSmqg732qwH7gNuqKrjSd7d1S8DPgNMAwU81e17HPgccCfwJKPfG94EfGuSk5MkndqQM4CNwFxVHa2q14E9wK1jbe4EdnYHdqrqxa7+EeDRqnql2/YosCnJlcDFVfVE94PyDwGbJzAfSdJAQwJgDfB8b32+q/VdA1yT5LtJnkiyaYl913TLp+tTknQWLXkJaBn9rAduBNYC30ly7SQ6TrIN2Abw3ve+dxJdSpIYdgZwDLiqt762q/XNAzNV9UZVPQccYRQIp9r3WLd8uj4BqKpdVTVdVdNTU1MDhitJGmJIABwA1idZl2Q1sAWYGWvzDUaf/klyBaNLQkeB/cAtSS5NcilwC7C/ql4A/jbJL3dP/3wC+OYkJiRJGmbJS0BVdSLJdkYH81XA7qo6lGQHMFtVM/zjgf4w8CZwd1W9DJDkdxmFCMCOqnqlW/4t4EHgZxg9/eMTQJL0Nhp0D6Cq9jF6VLNfu7+3XMBd3Wt8393A7kXqs8AHljleSdKE+E1gSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVGDAiDJpiTPJJlLcu8i2+9IspDkYPf6j139V3q1g0n+IcnmbtuDSZ7rbbtuslOTJJ3Okj8Kn2QVsBO4GZgHDiSZqarDY02/WlXb+4Wq+jZwXdfPZcAc8Oe9JndX1d4VjF+SdIaGnAFsBOaq6mhVvQ7sAW49g/f6GPCtqvrxGewrSZqwIQGwBni+tz7f1cb9hyTfT7I3yVWLbN8CPDxW+2y3zwNJLlzszZNsSzKbZHZhYWHAcCVJQ0zqJvCfAu+rqn8FPAp8sb8xyZXAtcD+Xvk+YAPwIeAy4J7FOq6qXVU1XVXTU1NTExquJGlIABwD+p/o13a1/6+qXq6q17rVzwMfHOvjNuDrVfVGb58XauQ14AuMLjVJkt4mQwLgALA+ybokqxldypnpN+g+4Z/0UeCHY31sZezyz8l9kgTYDDy9vKFLklZiyaeAqupEku2MLt+sAnZX1aEkO4DZqpoB/nOSjwIngFeAO07un+R9jM4g/nKs6y8nmQICHAQ+ueLZSJIGWzIAAKpqH7BvrHZ/b/k+Rtf0F9v3r1nkpnFV3bScgUqSJstvAktSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElq1KD/CkKS+t7/novP9RA0AQaApGX7zL/7xXM9BE2Al4AkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjUpVnesxDJZkAXgV+NEZ7H4F8NJkR6TT+Bec2d/TO9k7dU7nalxn+30n3f+k+ltJP2e670qPX1dX1dR48bwKAIAku6pq2xnsN1tV02djTHqrM/17eid7p87pXI3rbL/vpPufVH8r6eeddvw6Hy8B/em5HoAG+Wn8e3qnzulcjetsv++k+59Ufyvp5x31b+i8OwM4U54BSDpfeQawcrvO9QAk6QydleNXM2cAkqR/qqUzAElSjwEgSY0yACSpUQYAkOTGJP8ryR8mufFcj0eSliPJRUlmk/zb5ex33gdAkt1JXkzy9Fh9U5JnkswluXeJbgr4O+BdwPzZGqsk9U3o+AVwD/DIst//fH8KKMmHGR28H6qqD3S1VcAR4GZGB/QDwFZgFfB7Y138BvBSVf0kyb8E/ltV/drbNX5J7ZrQ8etfA5cz+gD7UlX9z6Hvf97/KHxVfSfJ+8bKG4G5qjoKkGQPcGtV/R5wulOk48CFZ2OckjRuEsev7rL1RcD7gb9Psq+qfjLk/c/7ADiFNcDzvfV54JdO1TjJvwc+AlwC/PezOzRJOq1lHb+q6tMASe6gu5ox9I1+WgNgWarqa8DXzvU4JOlMVdWDy93nvL8JfArHgKt662u7miS9071tx6+f1gA4AKxPsi7JamALMHOOxyRJQ7xtx6/zPgCSPAw8Dvx8kvkkv1lVJ4DtwH7gh8AjVXXoXI5Tksad6+PXef8YqCTpzJz3ZwCSpDNjAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIa9f8AHpgw/3UI6DkAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(acc_list)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "WUDgi0PQpSto",
        "outputId": "bb67cfa0-3333-4f2f-d9b0-5a5b5ec144cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e36f565aab23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'acc_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "strain(train_loader, model, loss_fn, optimizer, verbose=False)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)"
      ],
      "metadata": {
        "id": "8kjZ2lMGpwNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v7jNuLqr4hB"
      },
      "outputs": [],
      "source": [
        "# @title plot lr\n",
        "# https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide\n",
        "\n",
        "\n",
        "num_batches=int(np.ceil(len(train_dataset)/batch_size))\n",
        "print(num_batches)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=total_steps, anneal_strategy='cos')\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=100, anneal_strategy='linear')\n",
        "epochs=10\n",
        "steps_per_epoch=10#num_batches\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, epochs=epochs, steps_per_epoch=steps_per_epoch, anneal_strategy='cos')\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999) # 0.75(20)-0.9(100)\n",
        "# # gamma^step\n",
        "\n",
        "num_batches=int(np.ceil(len(train_dataset)/batch_size))\n",
        "# print(num_batches)\n",
        "epochs = 10 #5 20\n",
        "\n",
        "# (1e-5/1e-1)=gamma**(num_batches*epochs)\n",
        "# print(\"1e-4?\",0.99993**(num_batches*epochs))\n",
        "gamma = np.exp(np.log(1e-4/1e-1)/(num_batches*epochs))\n",
        "print(\"gamma\",gamma)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-6)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 30)\n",
        "\n",
        "\n",
        "num_iter=20\n",
        "start_lr=0.0001\n",
        "end_lr= 0.1\n",
        "gamma = np.exp(np.log(end_lr/start_lr)/num_iter)\n",
        "print(\"gamma\",gamma)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=start_lr, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "\n",
        "lr_list=[]\n",
        "# # for batch, (x, y) in enumerate(train_loader):\n",
        "# print(\"in range \",num_batches*epochs)\n",
        "# for x in range(num_batches*epochs):\n",
        "for x in range(num_iter):\n",
        "    scheduler.step()\n",
        "    # lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_list.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lr_list)\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDBEk-l-Oxjn"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwww\n",
        "import time\n",
        "start = time.time()\n",
        "lr_list=[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_batches=int(np.ceil(len(train_dataset)/batch_size))\n",
        "# print(num_batches)\n",
        "epochs = 10 #5 20\n",
        "# (1e-5/1e-1)=gamma**(num_batches*epochs)\n",
        "# gamma = np.exp(np.log(1e-3/1e-1)/epochs) # for scheduler step every epoch\n",
        "gamma = np.exp(np.log(1e-4/1e-2)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# print(gamma)\n",
        "\n",
        "# # optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=10, anneal_strategy='cos')\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=num_batches*epochs, anneal_strategy='cos')\n",
        "# scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 2e-3, epochs=50, steps_per_epoch=len(train_loader))\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, epochs=epochs, steps_per_epoch=num_batches, anneal_strategy='cos')\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_list.append(lr)\n",
        "    print(lr)\n",
        "    # train(train_loader, model, loss_fn, optimizer)\n",
        "    # scheduler.step()\n",
        "    # strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    strain(train_loader, model, loss_fn, optimizer)\n",
        "\n",
        "    test(test_loader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# model = NeuralNetwork().to(device)\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrqgHR_zOpeD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_list)\n",
        "# plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lr_list)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
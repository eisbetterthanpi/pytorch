{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/fast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "34d0f9566aaa46939a8cccc12e6e8019",
            "0a0bafb50aa0472380c9a23f4d15b014",
            "13ca0902626041aeb8c62c65b0dccbff",
            "40defc6eecba4a6bada27b94c4b2c66c",
            "44a8e48862a84baf8134dc1117ee118e",
            "d8bdc85ee13b49e18cd0510093c54f65",
            "eba8eafe35814e2b9d218f5ec209af0f",
            "7cab1655d4cb4357b1d3d23a222222ea",
            "e261e006652f4465b629ae43d727a956",
            "f98493a6d08148fca219c42fbb52d35b",
            "0efda9b5c46a4a97913ce6621169bd99"
          ]
        },
        "id": "LxACli7GdyGq",
        "outputId": "841c30c5-7a54-4eac-a990-121b2e5a41b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34d0f9566aaa46939a8cccc12e6e8019"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://github.com/python-engineer/pytorchTutorial/blob/master/14_cnn.py\n",
        "\n",
        "# dataset has PILImage images of range [0, 1], transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose(transforms.ToTensor())\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader) # get some random training images\n",
        "images, labels = dataiter.next()\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0mXVAUnVYX-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title simplifi\n",
        "# https://github.com/JayPatwardhan/ResNet-PyTorch/blob/master/ResNet/ResNet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        self.i_downsample = i_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(out_channels), nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels*self.expansion),\n",
        "        )\n",
        "        self.i_downsample = i_downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        if self.i_downsample is not None: #downsample if needed\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity #add identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        # https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py\n",
        "        # num_blocks=[3,3,3] aka layer_list\n",
        "        plane_list=[64,128,256]\n",
        "        # plane_list=[16,32,64] #og\n",
        "        self.in_channels = plane_list[0]\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, plane_list[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(plane_list[0]), nn.ReLU(),\n",
        "            self._make_layer(ResBlock, layer_list[0], plane_list[0], stride=1),\n",
        "            self._make_layer(ResBlock, layer_list[1], plane_list[1], stride=2),\n",
        "            self._make_layer(ResBlock, layer_list[2], plane_list[2], stride=2),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        # self.fc = nn.Linear(plane_list[2]*ResBlock.expansion, num_classes)\n",
        "        self.cc = nn.Conv2d(plane_list[2]*ResBlock.expansion, num_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # print(\"forward x\",x.shape)\n",
        "        # x = x.reshape(x.shape[0], -1)\n",
        "        # x = self.fc(x)\n",
        "        x = self.cc(x)\n",
        "        x = torch.squeeze(x)\n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = ResNet(Bottleneck, [3,4,6,3], num_classes=10, num_channels=3).to(device)\n",
        "model = ResNet(Bottleneck, [3,3,3], num_classes=10, num_channels=3).to(device)\n",
        "# print(model)\n",
        "\n",
        "loss_list=[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h7iCNId6oXUK"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "\n",
        "# class ConvNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(ConvNet, self).__init__()\n",
        "#         plane_list=[64,128,256,512]\n",
        "#         self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "#             nn.Conv2d(3, plane_list[0], 3, 1, 1), nn.BatchNorm2d(plane_list[0]), nn.ReLU(),# nn.MaxPool2d(2, 2),\n",
        "#             nn.Conv2d(plane_list[0], plane_list[1], 5, 1, 2), nn.BatchNorm2d(plane_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "#             nn.Conv2d(plane_list[1], plane_list[2], 7, 1, 3), nn.BatchNorm2d(plane_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "#         )\n",
        "#         self.linear = nn.Sequential(\n",
        "#             # nn.Linear(16 * 8 * 8, 256), nn.ReLU(),\n",
        "#             nn.Linear(plane_list[2] * 8 * 8, 256), nn.ReLU(),\n",
        "#             # nn.Linear(plane_list[2]//16, 256), nn.ReLU(),\n",
        "#             nn.Linear(256, 64), nn.ReLU(),\n",
        "#             nn.Linear(64, 10),\n",
        "#         )\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        plane_list=[64,128,256,512]\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Conv2d(3, plane_list[0], 3, 1, 1), nn.BatchNorm2d(plane_list[0]), nn.ReLU(), #nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(plane_list[0], plane_list[1], 3, 1, 1), nn.BatchNorm2d(plane_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(plane_list[1], plane_list[2], 3, 1, 1), nn.BatchNorm2d(plane_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Conv2d(plane_list[2],64, 1, 1, 0), #nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 10, 1, 1, 0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # print(\"forward x\",x.shape)\n",
        "        # x = x.view(-1, 16 * 5 * 5)\n",
        "        # x = nn.Flatten()(x)\n",
        "        # x = self.linear(x)\n",
        "        x = torch.squeeze(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "# print(model)\n",
        "\n",
        "loss_list=[]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEZCFg5YSS9J",
        "outputId": "4915f1e4-cff2-40b9-deaa-6a10db11a4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward x torch.Size([64, 256, 8, 8])\n",
            "torch.Size([64, 10, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(64, 3, 32, 32, device=device)\n",
        "logits = model(X)\n",
        "print(logits.shape)\n",
        "# print(logits[0])\n",
        "# print(logits[0].argmax(1))\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# for data, label in data_iter:\n",
        "#    optimizer.zero_grad()\n",
        "#    # Casts operations to mixed precision\n",
        "#    with torch.cuda.amp.autocast():\n",
        "#       loss = model(data)\n",
        "#    scaler.scale(loss).backward()\n",
        "#    scaler.step(optimizer)\n",
        "#    scaler.update()\n",
        "\n",
        "# def strain(dataloader, model, loss_fn, optimizer):\n",
        "def strain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    lr_list, loss_list = [], []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(sx)\n",
        "            loss = loss_fn(pred, sy)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if scheduler is not None:\n",
        "            lr_list.append(optimizer.param_groups[0][\"lr\"])\n",
        "            scheduler.step()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return lr_list, loss_list\n",
        "\n",
        "# def train(dataloader, model, loss_fn, optimizer):\n",
        "def train(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "    # for batch, ((x,y), labels) in enumerate(dataloader):\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        # print(\"sx sy\",sx.shape,sy.shape)\n",
        "        pred = model(sx)\n",
        "        loss = loss_fn(pred, sy)\n",
        "        # loss = model.loss(sx,sy)\n",
        "        optimizer.zero_grad() # reset gradients of model parameters, to prevent double-counting\n",
        "        loss.backward() # Backpropagate gradients\n",
        "        optimizer.step() # adjust the parameters by the gradients\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        if (batch) % (size//(10* len(x))) == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            loss_list.append(loss)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# def test(dataloader, model, loss_fn):\n",
        "def test(dataloader, model, loss_fn, verbose=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            x, y = X.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    # if verbose: print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    if verbose: print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return correct\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title lrfinder\n",
        "\n",
        "# https://towardsdatascience.com/super-convergence-with-just-pytorch-c223c0fc1e51\n",
        "# https://github.com/davidtvs/pytorch-lr-finder\n",
        "# # https://arxiv.org/pdf/1506.01186.pdf\n",
        "# LR range test\n",
        "# Note the learning rate value when the accuracy starts to\n",
        "# increase and when the accuracy slows, becomes ragged, or starts to fall\n",
        "# _/\\\n",
        "\n",
        "# cyclic\n",
        "# 0.0001-0.001\n",
        "\n",
        "# num_iter=20\n",
        "# start_lr=3e-5\n",
        "# end_lr= 3e-0\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#     model1=copy.deepcopy(model)\n",
        "#     # for g in optimizer.param_groups: g['lr'] = start_lr\n",
        "# https://stackoverflow.com/a/55760170\n",
        "#         lr=optimizer.param_groups[0][\"lr\"]\n",
        "#         # lr=scheduler.get_last_lr()[0]\n",
        "\n",
        "def lrfinder(model1, dataloader, start_lr=1e-5, end_lr=1e1, num_iter=200):\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    acc_list, lr_list, loss_list = [], [], []\n",
        "    optimizer = torch.optim.AdamW(model1.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6) #3e-6\n",
        "    # optimizer = torch.optim.SGD(model1.parameters(), lr=start_lr, momentum=0.9)\n",
        "    gamma = np.exp(np.log(end_lr/start_lr)/num_iter)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "    \n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loader_iter = iter(dataloader)\n",
        "    for t in range(num_iter):\n",
        "    # for batch, (x, y) in enumerate(dataloader):\n",
        "        # if \n",
        "            # loader_iter = iter(dataloader)\n",
        "        x, y = next(loader_iter)\n",
        "        sx, sy = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(sx)\n",
        "            loss = loss_fn(pred, sy)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        lr=optimizer.param_groups[0][\"lr\"] # lr=scheduler.get_last_lr()[0]\n",
        "        lr_list.append(lr); loss_list.append(loss.item())#; acc_list.append(accuracy)\n",
        "        print(\"###### iter: \",t,\", lr: \",lr,\", loss: \",loss.item()) # print(\"accuracy: \",accuracy)\n",
        "        scheduler.step()\n",
        "    return lr_list, loss_list #,acc_list\n",
        "    # # base_lr ,max_lr\n",
        "\n",
        "\n",
        "batch_size=512\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "model = ResNet(Bottleneck, [3,3,3], num_classes=10, num_channels=3).to(device)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "lr_list, loss_list = lrfinder(model, train_loader, start_lr=1e-8, end_lr=1e0, num_iter=97) #195(256) 97(512)\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n"
      ],
      "metadata": {
        "id": "keNSPmbziSMa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plt.plot(lr_list,acc_list)\n",
        "# plt.plot(acc_list)\n",
        "plt.plot(lr_list, loss_list)\n",
        "# plt.plot(lr_list[:-40], loss_list[:-40])\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "WUDgi0PQpSto",
        "outputId": "7d094a66-da5b-43ff-f454-76c280009ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  time epoch\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "strain(train_loader, model, loss_fn, optimizer, verbose=False)\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n"
      ],
      "metadata": {
        "id": "8kjZ2lMGpwNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title scheduler\n",
        "\n",
        "# https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
        "\n",
        "\n",
        "class _LRScheduler(object):\n",
        "\n",
        "    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n",
        "\n",
        "        # Attach optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initialize epoch and base learning rates\n",
        "        if last_epoch == -1:\n",
        "            for group in optimizer.param_groups:\n",
        "                group.setdefault('initial_lr', group['lr'])\n",
        "        else:\n",
        "            for i, group in enumerate(optimizer.param_groups):\n",
        "                if 'initial_lr' not in group:\n",
        "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
        "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
        "        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n",
        "        self.last_epoch = last_epoch\n",
        "\n",
        "        # Following https://github.com/pytorch/pytorch/issues/20124\n",
        "        # We would like to ensure that `lr_scheduler.step()` is called after\n",
        "        # `optimizer.step()`\n",
        "        def with_counter(method):\n",
        "            if getattr(method, '_with_counter', False):\n",
        "                # `optimizer.step()` has already been replaced, return.\n",
        "                return method\n",
        "\n",
        "            # Keep a weak reference to the optimizer instance to prevent\n",
        "            # cyclic references.\n",
        "            instance_ref = weakref.ref(method.__self__)\n",
        "            # Get the unbound method for the same purpose.\n",
        "            func = method.__func__\n",
        "            cls = instance_ref().__class__\n",
        "            del method\n",
        "\n",
        "            @wraps(func)\n",
        "            def wrapper(*args, **kwargs):\n",
        "                instance = instance_ref()\n",
        "                instance._step_count += 1\n",
        "                wrapped = func.__get__(instance, cls)\n",
        "                return wrapped(*args, **kwargs)\n",
        "\n",
        "            # Note that the returned function here is no longer a bound method,\n",
        "            # so attributes like `__func__` and `__self__` no longer exist.\n",
        "            wrapper._with_counter = True\n",
        "            return wrapper\n",
        "\n",
        "        self.optimizer.step = with_counter(self.optimizer.step)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self._initial_step()\n",
        "\n",
        "    def _initial_step(self):\n",
        "        \"\"\"Initialize step counts and performs a step\"\"\"\n",
        "        self.optimizer._step_count = 0\n",
        "        self._step_count = 0\n",
        "        self.step()\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
        "\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the schedulers state.\n",
        "\n",
        "        Args:\n",
        "            state_dict (dict): scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict)\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        \"\"\" Return last computed learning rate by current scheduler.\n",
        "        \"\"\"\n",
        "        return self._last_lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        # Compute learning rate using chainable form of the scheduler\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def print_lr(self, is_verbose, group, lr, epoch=None):\n",
        "        \"\"\"Display the current learning rate.\n",
        "        \"\"\"\n",
        "        if is_verbose:\n",
        "            if epoch is None:\n",
        "                print('Adjusting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(group, lr))\n",
        "            else:\n",
        "                epoch_str = (\"%.2f\" if isinstance(epoch, float) else\n",
        "                             \"%.5d\") % epoch\n",
        "                print('Epoch {}: adjusting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(epoch_str, group, lr))\n",
        "\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        # Raise a warning if old pattern is detected\n",
        "        # https://github.com/pytorch/pytorch/issues/20124\n",
        "        if self._step_count == 1:\n",
        "            if not hasattr(self.optimizer.step, \"_with_counter\"):\n",
        "                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n",
        "                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n",
        "                              \"`lr_scheduler.step()`. See more details at \"\n",
        "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
        "\n",
        "            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n",
        "            elif self.optimizer._step_count < 1:\n",
        "                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
        "                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n",
        "                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n",
        "                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n",
        "                              \"See more details at \"\n",
        "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
        "        self._step_count += 1\n",
        "\n",
        "        class _enable_get_lr_call:\n",
        "\n",
        "            def __init__(self, o):\n",
        "                self.o = o\n",
        "\n",
        "            def __enter__(self):\n",
        "                self.o._get_lr_called_within_step = True\n",
        "                return self\n",
        "\n",
        "            def __exit__(self, type, value, traceback):\n",
        "                self.o._get_lr_called_within_step = False\n",
        "\n",
        "        with _enable_get_lr_call(self):\n",
        "            if epoch is None:\n",
        "                self.last_epoch += 1\n",
        "                values = self.get_lr()\n",
        "            else:\n",
        "                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
        "                self.last_epoch = epoch\n",
        "                if hasattr(self, \"_get_closed_form_lr\"):\n",
        "                    values = self._get_closed_form_lr()\n",
        "                else:\n",
        "                    values = self.get_lr()\n",
        "\n",
        "        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n",
        "            param_group, lr = data\n",
        "            param_group['lr'] = lr\n",
        "            self.print_lr(self.verbose, i, lr, epoch)\n",
        "\n",
        "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
        "\n",
        "\n",
        "class OneCycleLRr(_LRScheduler):\n",
        "    def __init__(self,\n",
        "                 optimizer,\n",
        "                 max_lr,\n",
        "                 total_steps=None,\n",
        "                 epochs=None,\n",
        "                 steps_per_epoch=None,\n",
        "                 pct_start=0.3,\n",
        "                 anneal_strategy='cos',\n",
        "                 cycle_momentum=True,\n",
        "                 base_momentum=0.85,\n",
        "                 max_momentum=0.95,\n",
        "                 div_factor=25.,\n",
        "                 final_div_factor=1e4,\n",
        "                 three_phase=False,\n",
        "                 last_epoch=-1,\n",
        "                 verbose=False):\n",
        "\n",
        "        # Validate optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Validate total_steps\n",
        "        if total_steps is None and epochs is None and steps_per_epoch is None:\n",
        "            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n",
        "        elif total_steps is not None:\n",
        "            if total_steps <= 0 or not isinstance(total_steps, int):\n",
        "                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n",
        "            self.total_steps = total_steps\n",
        "        else:\n",
        "            if epochs <= 0 or not isinstance(epochs, int):\n",
        "                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n",
        "            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n",
        "                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n",
        "            self.total_steps = epochs * steps_per_epoch\n",
        "\n",
        "        if three_phase:\n",
        "            self._schedule_phases = [\n",
        "                {\n",
        "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'max_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'base_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': float(2 * pct_start * self.total_steps) - 2,\n",
        "                    'start_lr': 'max_lr',\n",
        "                    'end_lr': 'initial_lr',\n",
        "                    'start_momentum': 'base_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': self.total_steps - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'min_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "            ]\n",
        "        else:\n",
        "            self._schedule_phases = [\n",
        "                {\n",
        "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'max_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'base_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': self.total_steps - 1,\n",
        "                    'start_lr': 'max_lr',\n",
        "                    'end_lr': 'min_lr',\n",
        "                    'start_momentum': 'base_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "            ]\n",
        "\n",
        "        # Validate pct_start\n",
        "        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n",
        "            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n",
        "\n",
        "        # Validate anneal_strategy\n",
        "        if anneal_strategy not in ['cos', 'linear']:\n",
        "            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n",
        "        elif anneal_strategy == 'cos':\n",
        "            self.anneal_func = self._annealing_cos\n",
        "        elif anneal_strategy == 'linear':\n",
        "            self.anneal_func = self._annealing_linear\n",
        "\n",
        "        # Initialize learning rate variables\n",
        "        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n",
        "        if last_epoch == -1:\n",
        "            for idx, group in enumerate(self.optimizer.param_groups):\n",
        "                group['initial_lr'] = max_lrs[idx] / div_factor\n",
        "                group['max_lr'] = max_lrs[idx]\n",
        "                group['min_lr'] = group['initial_lr'] / final_div_factor\n",
        "\n",
        "        # Initialize momentum variables\n",
        "        self.cycle_momentum = cycle_momentum\n",
        "        if self.cycle_momentum:\n",
        "            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n",
        "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
        "            self.use_beta1 = 'betas' in self.optimizer.defaults\n",
        "            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
        "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
        "            if last_epoch == -1:\n",
        "                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n",
        "                    if self.use_beta1:\n",
        "                        group['betas'] = (m_momentum, *group['betas'][1:])\n",
        "                    else:\n",
        "                        group['momentum'] = m_momentum\n",
        "                    group['max_momentum'] = m_momentum\n",
        "                    group['base_momentum'] = b_momentum\n",
        "\n",
        "        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)\n",
        "\n",
        "    def _format_param(self, name, optimizer, param):\n",
        "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
        "        if isinstance(param, (list, tuple)):\n",
        "            if len(param) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
        "                    len(optimizer.param_groups), name, len(param)))\n",
        "            return param\n",
        "        else:\n",
        "            return [param] * len(optimizer.param_groups)\n",
        "\n",
        "    def _annealing_cos(self, start, end, pct):\n",
        "        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
        "        cos_out = math.cos(math.pi * pct) + 1\n",
        "        return end + (start - end) / 2.0 * cos_out\n",
        "\n",
        "    def _annealing_linear(self, start, end, pct):\n",
        "        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
        "        return (end - start) * pct + start\n",
        "\n",
        "    def get_lr(self):\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "                          \"please use `get_last_lr()`.\", UserWarning)\n",
        "\n",
        "        lrs = []\n",
        "        step_num = self.last_epoch\n",
        "\n",
        "        if step_num > self.total_steps:\n",
        "            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n",
        "                             .format(step_num + 1, self.total_steps))\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            start_step = 0\n",
        "            for i, phase in enumerate(self._schedule_phases):\n",
        "                end_step = phase['end_step']\n",
        "                if step_num <= end_step or i == len(self._schedule_phases) - 1:\n",
        "                    pct = (step_num - start_step) / (end_step - start_step)\n",
        "                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)\n",
        "                    if self.cycle_momentum:\n",
        "                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)\n",
        "                    break\n",
        "                start_step = phase['end_step']\n",
        "\n",
        "            lrs.append(computed_lr)\n",
        "            if self.cycle_momentum:\n",
        "                if self.use_beta1:\n",
        "                    group['betas'] = (computed_momentum, *group['betas'][1:])\n",
        "                else:\n",
        "                    group['momentum'] = computed_momentum\n",
        "\n",
        "        return lrs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ng20e75zNZkK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v7jNuLqr4hB",
        "outputId": "78780fb9-f10f-46b5-9d44-e9d7d94dae8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "{'lr': 3e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 3e-06, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False}\n"
          ]
        }
      ],
      "source": [
        "# @title plot lr\n",
        "# https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide\n",
        "# https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "# num_batches=int(np.ceil(len(train_dataset)/batch_size))\n",
        "num_batches=10\n",
        "print(num_batches)\n",
        "epochs = 10 #5 20\n",
        "num_iter= epochs*num_batches #20\n",
        "\n",
        "base_lr ,max_lr = 3e-5, 3e-4\n",
        "\n",
        "# (1e-5/1e-1)=gamma**(num_batches*epochs)\n",
        "# gamma = np.exp(np.log(1e-4/1e-1)/(num_batches*epochs))\n",
        "# gamma = np.exp(np.log(end_lr/start_lr)/num_iter)\n",
        "# print(\"gamma\",gamma)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=num_iter, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=True,)\n",
        "\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=max_lr, momentum=0.9, weight_decay=1e-6)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=base_lr)\n",
        "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=10, step_size_down=5, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=base_lr)\n",
        "# torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# print(optimizer.param_groups[0].keys())\n",
        "print(optimizer.defaults)\n",
        "# https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
        "\n",
        "# lr_list=[]\n",
        "# for x in range(num_iter):\n",
        "#     scheduler.step()\n",
        "#     # lr=optimizer.param_groups[0][\"lr\"]\n",
        "#     lr_list.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(lr_list)\n",
        "# # plt.yscale('log')\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDBEk-l-Oxjn",
        "outputId": "6bc51926-6f45-42fc-8dd1-c99a9be4ceea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "3.9999999999999996e-05\n",
            "loss: 2.486335  [    0/50000]\n",
            "loss: 2.165511  [ 4608/50000]\n",
            "loss: 1.952967  [ 9216/50000]\n",
            "loss: 1.861454  [13824/50000]\n",
            "loss: 1.805730  [18432/50000]\n",
            "loss: 1.652891  [23040/50000]\n",
            "loss: 1.636802  [27648/50000]\n",
            "loss: 1.519005  [32256/50000]\n",
            "loss: 1.472714  [36864/50000]\n",
            "loss: 1.443112  [41472/50000]\n",
            "loss: 1.402772  [46080/50000]\n",
            "Accuracy: 44.7%, Avg loss: 1.598970\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "0.0001527886758400849\n",
            "loss: 1.374696  [    0/50000]\n",
            "loss: 1.353732  [ 4608/50000]\n",
            "loss: 1.213977  [ 9216/50000]\n",
            "loss: 1.286799  [13824/50000]\n",
            "loss: 1.192356  [18432/50000]\n",
            "loss: 1.171222  [23040/50000]\n",
            "loss: 1.145673  [27648/50000]\n",
            "loss: 1.177003  [32256/50000]\n",
            "loss: 1.112185  [36864/50000]\n",
            "loss: 1.088130  [41472/50000]\n",
            "loss: 1.100788  [46080/50000]\n",
            "Accuracy: 57.6%, Avg loss: 1.193986\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "0.0004381493475363405\n",
            "loss: 1.043305  [    0/50000]\n",
            "loss: 0.966300  [ 4608/50000]\n",
            "loss: 0.856182  [ 9216/50000]\n",
            "loss: 0.849606  [13824/50000]\n",
            "loss: 1.031654  [18432/50000]\n",
            "loss: 0.968498  [23040/50000]\n",
            "loss: 0.914612  [27648/50000]\n",
            "loss: 0.891220  [32256/50000]\n",
            "loss: 0.789871  [36864/50000]\n",
            "loss: 0.860163  [41472/50000]\n",
            "loss: 0.791828  [46080/50000]\n",
            "Accuracy: 53.3%, Avg loss: 1.671414\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "0.0007619759638493593\n",
            "loss: 0.690052  [    0/50000]\n",
            "loss: 0.732222  [ 4608/50000]\n",
            "loss: 0.696679  [ 9216/50000]\n",
            "loss: 0.757528  [13824/50000]\n",
            "loss: 0.648958  [18432/50000]\n",
            "loss: 0.601028  [23040/50000]\n",
            "loss: 0.691234  [27648/50000]\n",
            "loss: 0.783883  [32256/50000]\n",
            "loss: 0.725365  [36864/50000]\n",
            "loss: 0.693946  [41472/50000]\n",
            "loss: 0.662001  [46080/50000]\n",
            "Accuracy: 60.5%, Avg loss: 1.355137\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "0.0009720852945469718\n",
            "loss: 0.524787  [    0/50000]\n",
            "loss: 0.529045  [ 4608/50000]\n",
            "loss: 0.528655  [ 9216/50000]\n",
            "loss: 0.531895  [13824/50000]\n",
            "loss: 0.492867  [18432/50000]\n",
            "loss: 0.582208  [23040/50000]\n",
            "loss: 0.562479  [27648/50000]\n",
            "loss: 0.535556  [32256/50000]\n",
            "loss: 0.563877  [36864/50000]\n",
            "loss: 0.526452  [41472/50000]\n",
            "loss: 0.511770  [46080/50000]\n",
            "Accuracy: 56.5%, Avg loss: 1.539080\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "0.0009697358679998857\n",
            "loss: 0.532175  [    0/50000]\n",
            "loss: 0.358211  [ 4608/50000]\n",
            "loss: 0.466223  [ 9216/50000]\n",
            "loss: 0.354248  [13824/50000]\n",
            "loss: 0.457783  [18432/50000]\n",
            "loss: 0.385431  [23040/50000]\n",
            "loss: 0.351576  [27648/50000]\n",
            "loss: 0.405550  [32256/50000]\n",
            "loss: 0.383896  [36864/50000]\n",
            "loss: 0.406786  [41472/50000]\n",
            "loss: 0.426457  [46080/50000]\n",
            "Accuracy: 78.0%, Avg loss: 0.666646\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "0.0007560318038298899\n",
            "loss: 0.256388  [    0/50000]\n",
            "loss: 0.236469  [ 4608/50000]\n",
            "loss: 0.217867  [ 9216/50000]\n",
            "loss: 0.211560  [13824/50000]\n",
            "loss: 0.211612  [18432/50000]\n",
            "loss: 0.226487  [23040/50000]\n",
            "loss: 0.211175  [27648/50000]\n",
            "loss: 0.222757  [32256/50000]\n",
            "loss: 0.231844  [36864/50000]\n",
            "loss: 0.223962  [41472/50000]\n",
            "loss: 0.206626  [46080/50000]\n",
            "Accuracy: 80.1%, Avg loss: 0.651269\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "0.00043140392878439425\n",
            "loss: 0.128780  [    0/50000]\n",
            "loss: 0.115302  [ 4608/50000]\n",
            "loss: 0.091708  [ 9216/50000]\n",
            "loss: 0.082960  [13824/50000]\n",
            "loss: 0.063229  [18432/50000]\n",
            "loss: 0.070866  [23040/50000]\n",
            "loss: 0.065967  [27648/50000]\n",
            "loss: 0.074312  [32256/50000]\n",
            "loss: 0.065920  [36864/50000]\n",
            "loss: 0.066317  [41472/50000]\n",
            "loss: 0.061536  [46080/50000]\n",
            "Accuracy: 84.0%, Avg loss: 0.520149\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "0.0001484120268932404\n",
            "loss: 0.033173  [    0/50000]\n",
            "loss: 0.026104  [ 4608/50000]\n",
            "loss: 0.024196  [ 9216/50000]\n",
            "loss: 0.019819  [13824/50000]\n",
            "loss: 0.015911  [18432/50000]\n",
            "loss: 0.017082  [23040/50000]\n",
            "loss: 0.015216  [27648/50000]\n",
            "loss: 0.015847  [32256/50000]\n",
            "loss: 0.028677  [36864/50000]\n",
            "loss: 0.025590  [41472/50000]\n",
            "loss: 0.019188  [46080/50000]\n",
            "Accuracy: 85.0%, Avg loss: 0.506115\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "3.9959737556084744e-05\n",
            "loss: 0.011903  [    0/50000]\n",
            "loss: 0.012871  [ 4608/50000]\n",
            "loss: 0.022595  [ 9216/50000]\n",
            "loss: 0.014750  [13824/50000]\n",
            "loss: 0.012292  [18432/50000]\n",
            "loss: 0.010156  [23040/50000]\n",
            "loss: 0.017090  [27648/50000]\n",
            "loss: 0.009767  [32256/50000]\n",
            "loss: 0.011305  [36864/50000]\n",
            "loss: 0.011692  [41472/50000]\n",
            "loss: 0.011485  [46080/50000]\n",
            "Accuracy: 85.0%, Avg loss: 0.509592\n",
            "Done!\n",
            "time:  575.9331557750702\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwww\n",
        "import time\n",
        "start = time.time()\n",
        "lr_lst, loss_lst=[],[]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model = ResNet(Bottleneck, [3,3,3], num_classes=10, num_channels=3).to(device)\n",
        "batch_size = 512 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "base_lr, max_lr = 1e-5, 1e-3\n",
        "end_lr, start_lr = 0.0001,0.1\n",
        "\n",
        "num_batches=int(np.ceil(len(train_dataset)/batch_size))\n",
        "# print(num_batches)\n",
        "epochs = 10 #5 20\n",
        "# (1e-5/1e-1)=gamma**(num_batches*epochs)\n",
        "# gamma = np.exp(np.log(1e-3/1e-1)/epochs) # for scheduler step every epoch\n",
        "gamma = np.exp(np.log(end_lr/start_lr)/(num_batches*epochs)) # for scheduler step every optimizer step\n",
        "# print(gamma)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=num_iter, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=True,)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=num_batches, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=True,)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # 0.75(20)-0.9(100)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lr=optimizer.param_groups[0][\"lr\"]\n",
        "    lr_list.append(lr)\n",
        "    print(lr)\n",
        "    # train(train_loader, model, loss_fn, optimizer)\n",
        "    lr_ls, loss_ls = strain(train_loader, model, loss_fn, optimizer, scheduler)\n",
        "    # strain(train_loader, model, loss_fn, optimizer)\n",
        "    test(test_loader, model, loss_fn)\n",
        "    # scheduler.step()\n",
        "    lr_lst.append(lr_ls); loss_lst.append(loss_ls)\n",
        "print(\"Done!\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# model = NeuralNetwork().to(device)\n",
        "# model.load_state_dict(torch.load(\"model.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrqgHR_zOpeD",
        "outputId": "12d45ff0-5aeb-4e9a-f3d8-466943d80f5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_lst)\n",
        "# plt.yscale('log')\n",
        "plt.show()\n",
        "# plt.plot(lr_lst)\n",
        "plt.show()\n",
        "\n",
        "# print(len(lr_lst[0]))\n",
        "# print(len(loss_lst[0])) 10,98\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "34d0f9566aaa46939a8cccc12e6e8019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a0bafb50aa0472380c9a23f4d15b014",
              "IPY_MODEL_13ca0902626041aeb8c62c65b0dccbff",
              "IPY_MODEL_40defc6eecba4a6bada27b94c4b2c66c"
            ],
            "layout": "IPY_MODEL_44a8e48862a84baf8134dc1117ee118e"
          }
        },
        "0a0bafb50aa0472380c9a23f4d15b014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8bdc85ee13b49e18cd0510093c54f65",
            "placeholder": "​",
            "style": "IPY_MODEL_eba8eafe35814e2b9d218f5ec209af0f",
            "value": "100%"
          }
        },
        "13ca0902626041aeb8c62c65b0dccbff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cab1655d4cb4357b1d3d23a222222ea",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e261e006652f4465b629ae43d727a956",
            "value": 170498071
          }
        },
        "40defc6eecba4a6bada27b94c4b2c66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f98493a6d08148fca219c42fbb52d35b",
            "placeholder": "​",
            "style": "IPY_MODEL_0efda9b5c46a4a97913ce6621169bd99",
            "value": " 170498071/170498071 [00:14&lt;00:00, 13047363.51it/s]"
          }
        },
        "44a8e48862a84baf8134dc1117ee118e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8bdc85ee13b49e18cd0510093c54f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eba8eafe35814e2b9d218f5ec209af0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cab1655d4cb4357b1d3d23a222222ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e261e006652f4465b629ae43d727a956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f98493a6d08148fca219c42fbb52d35b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0efda9b5c46a4a97913ce6621169bd99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "haron1100 Upside-Down wandb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsIla9Fl4qfPcN8KGFZ9+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/haron1100_Upside_Down_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://github.com/haron1100/Upside-Down-Reinforcement-Learning/blob/master/Upside%20Down%20Reinforcement%20Learning.py\n",
        "# https://arxiv.org/pdf/1912.02877.pdf\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import torch.nn.functional as F\n",
        "!pip install gym[box2d]\n",
        "import gym\n",
        "# env = gym.make('CartPole-v1')\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # \n",
        "wandb.init(project=\"haron1100\", entity=\"bobdole\")\n"
      ],
      "metadata": {
        "id": "EO9jdbhnGOBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6VzoQvI5YT3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_policy(obs, command):\n",
        "    return np.random.randint(env.action_space.n)\n",
        "\n",
        "import time\n",
        "#Visualise agent function\n",
        "def visualise_agent(policy, command, n=5):\n",
        "    try:\n",
        "        for trial_i in range(n):\n",
        "            current_command = deepcopy(command)\n",
        "            observation = env.reset()\n",
        "            done=False\n",
        "            t=0\n",
        "            episode_return=0\n",
        "            while not done:\n",
        "                env.render()\n",
        "                action = policy(torch.tensor([observation]).double(), torch.tensor([command]).double())\n",
        "                observation, reward, done, info = env.step(action)\n",
        "                episode_return+=reward\n",
        "                current_command[0]-= reward\n",
        "                current_command[1] = max(1, current_command[1]-1)\n",
        "                t+=1\n",
        "            env.render()\n",
        "            time.sleep(1.5)\n",
        "            print(\"Episode {} finished after {} timesteps. Return = {}\".format(trial_i, t, episode_return))\n",
        "        env.close()\n",
        "    except KeyboardInterrupt:\n",
        "        env.close()\n",
        "        \n",
        "#Behaviour function - Neural Network\n",
        "class FCNN_AGENT(torch.nn.Module):\n",
        "    def __init__(self, command_scale):\n",
        "        super().__init__()\n",
        "        hidden_size=64\n",
        "        self.command_scale=command_scale\n",
        "        self.observation_embedding = torch.nn.Sequential(\n",
        "            torch.nn.Linear(np.prod(env.observation_space.shape), hidden_size),\n",
        "            torch.nn.Tanh()\n",
        "        ).to(device)\n",
        "        self.command_embedding = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2, hidden_size),\n",
        "            torch.nn.Sigmoid()\n",
        "        ).to(device)\n",
        "        self.to_output = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, hidden_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_size, env.action_space.n)\n",
        "        ).to(device)\n",
        "    \n",
        "    def forward(self, observation, command):\n",
        "        obs_emebdding = self.observation_embedding(observation)\n",
        "        cmd_embedding = self.command_embedding(command*self.command_scale)\n",
        "        embedding = torch.mul(obs_emebdding, cmd_embedding)\n",
        "        action_prob_logits = self.to_output(embedding)\n",
        "        return action_prob_logits\n",
        "    \n",
        "    def create_optimizer(self, lr):\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "#Fill the replay buffer with more experience\n",
        "def collect_experience(policy, replay_buffer, replay_size, last_few, n_episodes=100, log_to_tensorboard=True):\n",
        "    global i_episode\n",
        "    init_replay_buffer = deepcopy(replay_buffer)\n",
        "    try:\n",
        "        for _ in range(n_episodes):\n",
        "            episode_reward = 0\n",
        "            command = sample_command(init_replay_buffer, last_few)\n",
        "            if log_to_tensorboard: writer.add_scalar('Command desired reward/Episode', command[0], i_episode)    # write loss to a graph\n",
        "            if log_to_tensorboard: writer.add_scalar('Command horizon/Episode', command[1], i_episode)    # write loss to a graph\n",
        "            observation = env.reset()\n",
        "            episode_mem = {'observation':[],\n",
        "                           'action':[],\n",
        "                           'reward':[],}\n",
        "            done=False\n",
        "            while not done:\n",
        "                action = policy(torch.tensor([observation]).double(), torch.tensor([command]).double())\n",
        "                new_observation, reward, done, info = env.step(action)\n",
        "                \n",
        "                episode_mem['observation'].append(observation)\n",
        "                episode_mem['action'].append(action)\n",
        "                # episode_mem['reward'].append(reward)\n",
        "\n",
        "                episode_reward += reward\n",
        "                if not done:\n",
        "                    episode_mem['reward'].append(0) # sparse lunar lander\n",
        "                    command[0] -= 0\n",
        "                else:\n",
        "                    episode_mem['reward'].append(episode_reward)     # sparse lunar lander \n",
        "                    command[0] -= episode_reward\n",
        "                \n",
        "                observation=new_observation\n",
        "                # command[0]-= reward\n",
        "                command[1] = max(1, command[1]-1)\n",
        "\n",
        "            episode_mem['return']=sum(episode_mem['reward'])\n",
        "            episode_mem['episode_len']=len(episode_mem['observation'])\n",
        "            replay_buffer.append(episode_mem)\n",
        "            i_episode+=1\n",
        "            if log_to_tensorboard: writer.add_scalar('Return/Episode', sum(episode_mem['reward']), i_episode)    # write loss to a graph\n",
        "            print(\"Episode {} finished after {} timesteps. Return = {}\".format(i_episode, len(episode_mem['observation']), sum(episode_mem['reward'])))\n",
        "        env.close()\n",
        "    except KeyboardInterrupt:\n",
        "        env.close()\n",
        "    replay_buffer = sorted(replay_buffer, key=lambda x:x['return'])[-replay_size:]\n",
        "    return replay_buffer\n",
        "\n",
        "#Sample exploratory command\n",
        "def sample_command(replay_buffer, last_few):\n",
        "    if len(replay_buffer)==0:\n",
        "        return [1, 1]\n",
        "    else:\n",
        "        command_samples = replay_buffer[-last_few:]\n",
        "        lengths = [mem['episode_len'] for mem in command_samples]\n",
        "        returns = [mem['return'] for mem in command_samples]\n",
        "        mean_return, std_return = np.mean(returns), np.std(returns)\n",
        "        command_horizon = np.mean(lengths)\n",
        "        desired_reward = np.random.uniform(mean_return, mean_return+std_return)\n",
        "        return [desired_reward, command_horizon]\n",
        "\n",
        "#Improve behviour function by training on replay buffer\n",
        "# def train_net(policy_net, replay_buffer, n_updates=100, batch_size=64, log_to_tensorboard=True):\n",
        "def train_net(policy_net, replay_buffer, n_updates=100, batch_size=64):\n",
        "    # global i_updates\n",
        "    all_costs = []\n",
        "    for i in range(n_updates):\n",
        "        batch_observations = np.zeros((batch_size, np.prod(env.observation_space.shape)))\n",
        "        batch_commands = np.zeros((batch_size, 2))\n",
        "        batch_label = np.zeros((batch_size))\n",
        "        for b in range(batch_size):\n",
        "            sample_episode = np.random.randint(0, len(replay_buffer))\n",
        "            sample_t1 = np.random.randint(0, len(replay_buffer[sample_episode]['observation']))\n",
        "            sample_t2 = len(replay_buffer[sample_episode]['observation'])\n",
        "            ##sample_t2 = np.random.randint(sample_t1+1, len(replay_buffer[sample_episode]['observation'])+1)\n",
        "            sample_horizon = sample_t2-sample_t1\n",
        "            sample_mem = replay_buffer[sample_episode]['observation'][sample_t1]\n",
        "            sample_desired_reward = sum(replay_buffer[sample_episode]['reward'][sample_t1:sample_t2])\n",
        "            network_input = np.append(sample_mem, [sample_desired_reward, sample_horizon])\n",
        "            label = replay_buffer[sample_episode]['action'][sample_t1]\n",
        "            batch_observations[b] = sample_mem\n",
        "            batch_commands[b] = [sample_desired_reward, sample_horizon]\n",
        "            batch_label[b] = label\n",
        "        batch_observations = torch.tensor(batch_observations).double()\n",
        "        batch_commands = torch.tensor(batch_commands).double()\n",
        "        batch_label = torch.tensor(batch_label).long()\n",
        "        pred = policy_net(batch_observations, batch_commands)\n",
        "        cost = F.cross_entropy(pred, batch_label)\n",
        "        # if log_to_tensorboard: writer.add_scalar('Cost/NN update', cost.item() , i_updates)    # write loss to a graph\n",
        "        wandb.log({\"cost.item()\": cost.item()})\n",
        "        all_costs.append(cost.item())\n",
        "        cost.backward()\n",
        "        policy_net.optimizer.step()\n",
        "        policy_net.optimizer.zero_grad()\n",
        "        # i_updates+=1\n",
        "    return np.mean(all_costs)\n",
        "\n",
        "#Return a greedy policy from a given network\n",
        "def create_greedy_policy(policy_network):\n",
        "    def policy(obs, command):\n",
        "        action_logits = policy_network(obs, command)\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        action = np.argmax(action_probs.detach().numpy())\n",
        "        return action\n",
        "    return policy\n",
        "\n",
        "#Return a stochastic policy from a given network\n",
        "def create_stochastic_policy(policy_network):\n",
        "    def policy(obs, command):\n",
        "        action_logits = policy_network(obs, command)\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        action = torch.distributions.Categorical(action_probs).sample().item()\n",
        "        return action\n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Initialize vars\n",
        "i_episode=0 #number of episodes trained so far\n",
        "# i_updates=0 #number of parameter updates to the neural network so far\n",
        "replay_buffer = []\n",
        "# log_to_tensorboard = False\n",
        "\n",
        "## HYPERPARAMS\n",
        "replay_size = 700\n",
        "last_few = 50\n",
        "batch_size = 256\n",
        "n_warm_up_episodes = 50\n",
        "n_episodes_per_iter = 25\n",
        "n_updates_per_iter = 15\n",
        "command_scale = 0.02\n",
        "lr = 0.001\n",
        "\n",
        "# Initialize behaviour function\n",
        "agent = FCNN_AGENT(command_scale).double()\n",
        "agent.create_optimizer(lr)\n",
        "\n",
        "stochastic_policy = create_stochastic_policy(agent)\n",
        "greedy_policy = create_greedy_policy(agent)\n",
        "\n",
        "# # SET UP TRAINING VISUALISATION\n",
        "# if log_to_tensorboard: from torch.utils.tensorboard import SummaryWriter\n",
        "# if log_to_tensorboard: writer = SummaryWriter() # we will use this to show our models performance on a graph using tensorboard\n",
        "\n",
        "#Collect warm up episodes\n",
        "replay_buffer = collect_experience(random_policy, replay_buffer, replay_size, last_few, n_warm_up_episodes, log_to_tensorboard)\n",
        "# train_net(agent, replay_buffer, n_updates_per_iter, batch_size, log_to_tensorboard)\n",
        "train_net(agent, replay_buffer, n_updates_per_iter, batch_size)\n",
        "\n",
        "#Collect experience and train behaviour function for given number of iterations\n",
        "n_iters = 1000\n",
        "for i in range(n_iters):\n",
        "    replay_buffer = collect_experience(stochastic_policy, replay_buffer, replay_size, last_few, n_episodes_per_iter, log_to_tensorboard)\n",
        "    # train_net(agent, replay_buffer, n_updates_per_iter, batch_size, log_to_tensorboard)\n",
        "    train_net(agent, replay_buffer, n_updates_per_iter, batch_size)\n"
      ],
      "metadata": {
        "id": "a8IZYRl7GUuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"model.pth\"\n",
        "torch.save(agent.state_dict(), name)\n"
      ],
      "metadata": {
        "id": "Z-BJRZT0HJ73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise final trained agent with greedy policy\n",
        "visualise_agent(greedy_policy, command=[150, 400], n=5)\n",
        "\n",
        "#Visualise final trained agent with stochastic policy\n",
        "visualise_agent(stochastic_policy, command=[150, 400], n=5)\n"
      ],
      "metadata": {
        "id": "42pFMY4VHE3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/minGPT_play_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBmoZfW8SF7Z"
      },
      "source": [
        "#### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX4HsX73QTOh",
        "outputId": "8b327849-2c3f-417c-d5a0-06c2a92fd8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'minGPT' already exists and is not an empty directory.\n",
            "/content/minGPT\n"
          ]
        }
      ],
      "source": [
        "# # set up logging\n",
        "# import logging\n",
        "# logging.basicConfig(\n",
        "#         format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "#         datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "#         level=logging.INFO,\n",
        "# )\n",
        "\n",
        "# char-transformer: chop up to individual characters and train,\n",
        "\n",
        "# https://colab.research.google.com/github/karpathy/minGPT/blob/master/play_char.ipynb\n",
        "# # make deterministic\n",
        "# from mingpt.utils import set_seed\n",
        "# set_seed(42)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "%cd /content/minGPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wutRcUgqSLtH"
      },
      "source": [
        "#### functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Du6nhkQTOk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"arrange data and targets so that the first i elements of x will be asked to predict the i-th element of y.\n",
        "        Notice that the eventual language model will actually make block_size individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward pass of the network.\n",
        "        So for example if block_size is 4, then we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwi09ZnhQTOn",
        "outputId": "41576676-44a4-4426-e7a3-1784cb792a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data has 132637 characters, 95 unique.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "block_size = 128 # spatial extent of the model for its context\n",
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "import urllib.request\n",
        "url = 'https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt'\n",
        "urllib.request.urlretrieve(url, \"input.txt\")\n",
        "\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95WB0TXSoaN"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZftOd0HQTOq"
      },
      "outputs": [],
      "source": [
        "# og mingpt\n",
        "from mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=4, n_head=4, n_embd=128) # 8 8 512\n",
        "model = GPT(mconf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_4z3ROKfw7V"
      },
      "source": [
        "#### gpt class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yisTKwAf0qX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "        # logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "        elif isinstance(module, GPT):\n",
        "            torch.nn.init.normal_(module.pos_emb, mean=0.0, std=0.02)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\"\"\"\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        print(\"in model\",idx.shape)\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRPHfXWxf1-u"
      },
      "source": [
        "#### perceiverio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI9y630_XHGQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install perceiver-pytorch\n",
        "import torch\n",
        "from perceiver_pytorch import PerceiverIO\n",
        "# https://github.com/lucidrains/perceiver-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cptzKUS9XMwm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=4, n_head=4, n_embd=128) # 8 8 512\n",
        "\n",
        "model = PerceiverIO(\n",
        "    dim = train_dataset.vocab_size,                    # dimension of sequence to be encoded\n",
        "    queries_dim = n_labels,            # dimension of decoder queries\n",
        "    logits_dim = None,            # dimension of final logits\n",
        "    depth = 6,                   # depth of net\n",
        "    num_latents = 128,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "    latent_dim = 128,            # latent dimension\n",
        "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
        "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
        "    weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        ")#.to(device)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BC1MuKSm1Y"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "73GQFkObQTOr",
        "outputId": "44f94124-a16a-4b8b-da34-a09dcd5e7394"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1 iter 258: train loss 1.42896. lr 3.001423e-04: 100%|██████████| 259/259 [47:01<00:00, 10.89s/it]\n",
            "epoch 2 iter 258: train loss 1.12922. lr 6.000000e-05: 100%|██████████| 259/259 [46:37<00:00, 10.80s/it]\n"
          ]
        }
      ],
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "# tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size, num_workers=4)\n",
        "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size, num_workers=0)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7AthNlpSjN2"
      },
      "source": [
        "#### eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwTYEnZmQTOs",
        "outputId": "ea3f1575-1bd1-4bda-fa47-704b74518525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O God, O God! that e'er this tongue of mine,\n",
            "That laid the sentence of dread banishment\n",
            "On yon proud man, should take it off again\n",
            "With words of sooth! O that I were as great\n",
            "As is my grief, or lesser than my name!\n",
            "Or that I could forget\n",
            "With Richmond, I'll tell you what I am,\n",
            "The Lord Aumerle, .\n",
            "\n",
            "CLAUDIO:\n",
            "The prenzie Angelo!\n",
            "\n",
            "ISABELLA:\n",
            "O, 'tis the cunning livery of hell,\n",
            "The damned'st body to invest and cover\n",
            "In prenzie guards! Dost thou think, Claudio?\n",
            "If I would yield him my virginity,\n",
            "Thou mightst be freed.\n",
            "\n",
            "CLAUDIO:\n",
            "O heavens! it cannot be.\n",
            "\n",
            "ISABELLA:\n",
            "Yes, he would give't thee, from this rank offence,\n",
            "So to offend him still. This night's the time\n",
            "That I should do what I abhor to name,\n",
            "Or else thou diest to-morrow.\n",
            "\n",
            "CLAUDIO:\n",
            "Thou shalt not do't.\n",
            "\n",
            "ISABELLA:\n",
            "O, were it but my life,\n",
            "I'ld throw it down for your deliverance\n",
            "As frankly as a pin.\n",
            "\n",
            "CLAUDIO:\n",
            "Thanks, dear Isabel.\n",
            "\n",
            "ISABELLA:\n",
            "Be ready, Claudio, for your death tomorrow.\n",
            "\n",
            "CLAUDIO:\n",
            "Yes. Has he affections\n",
            "That profit us.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "By the world they see the word in 's doom.\n",
            "\n",
            "ANGELO:\n",
            "Thou art the like, thus hate the course in heaven.\n",
            "What foul hath bled the wheel or at wild,\n",
            "And wash him fresh again with true-love tears.\n",
            "Ah, thou, the model where old Troy did stand,\n",
            "Thou map of honour, thou King Richard's tomb,\n",
            "And not King Richard; thou most beauteous inn,\n",
            "Why should hard-favour'd grief be lodged in thee,\n",
            "When triumph is become an alehouse guest?\n",
            "\n",
            "KING RICHARD II:\n",
            "Join not with grief, fair woman, do not so,\n",
            "To make my end too sudden: learn, good soul,\n",
            "To think our former state a happy dream;\n",
            "From which awaked, the truth of what we are\n",
            "Shows us but this: I am sworn brother, sweet,\n",
            "To grim Necessity, and he and I\n",
            "Will keep a league till death. Hie thee to France\n",
            "And cloister thee in some religious house:\n",
            "Our holy lives must win a new world's crown,\n",
            "Which our profane hours here have stricken down.\n",
            "\n",
            "QUEEN:\n",
            "What, is my Richard both in shape and mind\n",
            "Transform'd and weaken'd? hath Bolingbroke deposed\n",
            "Thine intellect? hath h\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# alright, let's sample some character-level Shakespeare\n",
        "from mingpt.utils import sample\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RBmoZfW8SF7Z",
        "wutRcUgqSLtH",
        "ZRPHfXWxf1-u"
      ],
      "name": "minGPT-play_char.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
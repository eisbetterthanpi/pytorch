{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/minGPT_play_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBmoZfW8SF7Z"
      },
      "source": [
        "#### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX4HsX73QTOh",
        "outputId": "6b9fd9fb-51d8-4e10-933c-4826cc14097e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minGPT'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 386 (delta 44), reused 76 (delta 28), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (386/386), 1.42 MiB | 1.55 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n",
            "/content/minGPT\n"
          ]
        }
      ],
      "source": [
        "# # set up logging\n",
        "# import logging\n",
        "# logging.basicConfig(\n",
        "#         format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "#         datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "#         level=logging.INFO,\n",
        "# )\n",
        "\n",
        "# char-transformer: chop up to individual characters and train,\n",
        "\n",
        "# https://colab.research.google.com/github/karpathy/minGPT/blob/master/play_char.ipynb\n",
        "# # make deterministic\n",
        "# from mingpt.utils import set_seed\n",
        "# set_seed(42)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "%cd /content/minGPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wutRcUgqSLtH"
      },
      "source": [
        "#### functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A1Du6nhkQTOk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"arrange data and targets so that the first i elements of x will be asked to predict the i-th element of y.\n",
        "        Notice that the eventual language model will actually make block_size individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward pass of the network.\n",
        "        So for example if block_size is 4, then we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
        "so nothing in this file really has anything to do with GPT specifically.\"\"\"\n",
        "\n",
        "import math\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_dataset, test_dataset, config):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.config = config\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
        "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = raw_model.configure_optimizers(config)\n",
        "        def run_epoch(loader, is_train):\n",
        "            model.train(is_train)\n",
        "            losses = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            for it, (x, y) in pbar:\n",
        "                # place data on the correct device\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    logits, loss = model(x, y)\n",
        "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "                    losses.append(loss.item())\n",
        "                if is_train:\n",
        "                    # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # decay the learning rate based on our progress\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
        "                        else:\n",
        "                            # cosine learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "                    # report progress\n",
        "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "            if not is_train:\n",
        "                test_loss = float(np.mean(losses))\n",
        "                logger.info(\"test loss: %f\", test_loss)\n",
        "                return test_loss\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        self.tokens = 0 # counter used for learning rate decay\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=config.num_workers\n",
        "        )\n",
        "        if self.test_dataset is not None:\n",
        "            test_loader = DataLoader(\n",
        "                self.test_dataset,\n",
        "                shuffle=True,\n",
        "                pin_memory=True,\n",
        "                batch_size=config.batch_size,\n",
        "                num_workers=config.num_workers\n",
        "            )\n",
        "\n",
        "        for epoch in range(config.max_epochs):\n",
        "            run_epoch(train_loader, is_train=True)\n",
        "            if self.test_dataset is not None:\n",
        "                test_loss = run_epoch(test_loader, is_train=False)\n",
        "\n",
        "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
        "            good_model = self.test_dataset is None or test_loss < best_loss\n",
        "            if self.config.ckpt_path is not None and good_model:\n",
        "                best_loss = test_loss\n",
        "                self.save_checkpoint()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JSYecm20IFfU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwi09ZnhQTOn",
        "outputId": "a6a5183b-ad8d-4241-d552-edb7f9438062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 137366 characters, 95 unique.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "block_size = 128 # spatial extent of the model for its context\n",
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "import urllib.request\n",
        "url = 'https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt'\n",
        "urllib.request.urlretrieve(url, \"input.txt\")\n",
        "\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95WB0TXSoaN"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aZftOd0HQTOq"
      },
      "outputs": [],
      "source": [
        "# og mingpt\n",
        "from mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=4, n_head=4, n_embd=128) # 8 8 512\n",
        "model = GPT(mconf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_4z3ROKfw7V"
      },
      "source": [
        "#### gpt class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3yisTKwAf0qX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "        # logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "        elif isinstance(module, GPT):\n",
        "            torch.nn.init.normal_(module.pos_emb, mean=0.0, std=0.02)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\"\"\"\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        print(\"in model\",idx.shape)\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRPHfXWxf1-u"
      },
      "source": [
        "#### perceiverio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JI9y630_XHGQ",
        "outputId": "f2f24b23-2da4-4f02-aef1-5c739640c9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting perceiver-pytorch\n",
            "  Downloading perceiver_pytorch-0.8.3-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from perceiver-pytorch) (1.11.0+cu113)\n",
            "Collecting einops>=0.3\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->perceiver-pytorch) (4.1.1)\n",
            "Installing collected packages: einops, perceiver-pytorch\n",
            "Successfully installed einops-0.4.1 perceiver-pytorch-0.8.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install perceiver-pytorch\n",
        "import torch\n",
        "from perceiver_pytorch import PerceiverIO\n",
        "# https://github.com/lucidrains/perceiver-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cptzKUS9XMwm",
        "outputId": "b8f9aa9a-bf14-4364-eb74-1a4810b88b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1407a2d7c535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# print(x.shape, y.shape) #0-94? [64, 128],[64, 128]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[64, 128, 95] , [] singu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1407a2d7c535>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# x = self.drop(token_embeddings + position_embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# x = self.model(token_embeddings + position_embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/perceiver_pytorch/perceiver_io.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, mask, queries)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# cross attend from decoder queries to latents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mlatents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_cross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# optional decoder feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/perceiver_pytorch/perceiver_io.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2484\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         )\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[128], expected input with shape [*, 128], but got input of size[64, 128, 32]"
          ]
        }
      ],
      "source": [
        "\n",
        "# mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=4, n_head=4, n_embd=128) # 8 8 512\n",
        "\n",
        "# print(train_dataset.vocab_size, train_dataset.block_size) # 95 128\n",
        "\n",
        "\n",
        "\n",
        "# class TrainerConfig:\n",
        "#     # optimization parameters\n",
        "#     max_epochs = 10\n",
        "#     batch_size = 64\n",
        "#     learning_rate = 3e-4\n",
        "#     betas = (0.9, 0.95)\n",
        "#     grad_norm_clip = 1.0\n",
        "#     weight_decay = 0.1 # only applied on matmul weights\n",
        "#     # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "#     lr_decay = False\n",
        "#     warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "#     final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "#     # checkpoint settings\n",
        "#     ckpt_path = None\n",
        "#     num_workers = 0 # for DataLoader\n",
        "#     def __init__(self, **kwargs):\n",
        "#         for k,v in kwargs.items():\n",
        "#             setattr(self, k, v)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "train_loader = DataLoader(train_dataset, #self.train_dataset\n",
        "            shuffle=True, pin_memory=True,\n",
        "            batch_size=64, #64 config.batch_size\n",
        "            num_workers=0)\n",
        "\n",
        "# seq = torch.randn(5, 1, 28*28, device=device)\n",
        "# # queries = torch.randn(128, 32)\n",
        "# queries = torch.randn(1, 10, device=device)\n",
        "# logits = model(seq, queries = queries) # (1, 128, 100) - (batch, decoder seq, logits dim)\n",
        "# print(logits.shape)\n",
        "# if logits.dim()==3:\n",
        "#     logits=logits.squeeze(dim=1)\n",
        "#     # logits=logits.squeeze()\n",
        "# print(logits.shape)\n",
        "# print(logits)\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "block_size = train_dataset.block_size\n",
        "\n",
        "class perceiverioGPT(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_layer=4, n_head=4, n_embd=128):\n",
        "        super(perceiverioGPT, self).__init__()\n",
        "        self.model = PerceiverIO(\n",
        "            dim = n_embd,            # dimension of sequence to be encoded\n",
        "            queries_dim = block_size,    # dimension of decoder queries\n",
        "            logits_dim = None,           # dimension of final logits\n",
        "            depth = 1,                   # depth of net\n",
        "            num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "            latent_dim = 32,            # latent dimension\n",
        "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
        "            latent_heads = 4,            # number of heads for latent self attention, 8\n",
        "            cross_dim_head = 8,         # number of dimensions per cross attention head\n",
        "            latent_dim_head = 8,        # number of dimensions per latent self attention head\n",
        "            weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        )\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd) # vocab_size num diff words, embedded into n_embd size vector\n",
        "        # self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.pos_emb = torch.zeros(1, block_size, n_embd)\n",
        "        # self.drop = nn.Dropout(config.embd_pdrop) # randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. for regularization and preventing the co-adaptation of neurons\n",
        "        self.queries = torch.zeros(128, 32)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        # assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        # x = self.drop(token_embeddings + position_embeddings)\n",
        "        # x = self.model(token_embeddings + position_embeddings)\n",
        "        logits = self.model(token_embeddings + position_embeddings, queries = self.queries)\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "# def gpt_pre(x, y): [64, 128],[64, 128]\n",
        "\n",
        "# def gpt_post(x, y):\n",
        "\n",
        "\n",
        "\n",
        "model = perceiverioGPT(vocab_size, block_size, n_layer=4, n_head=4, n_embd=128)\n",
        "\n",
        "(x, y) = next(iter(train_loader))\n",
        "# print(x.shape, y.shape) #0-94? [64, 128],[64, 128]\n",
        "logits, loss = model(x, y)\n",
        "print(logits, loss) #[64, 128, 95] , [] singu\n",
        "\n",
        "\n",
        "# model.train(is_train)\n",
        "#     losses = []\n",
        "#     pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "#     for it, (x, y) in pbar:\n",
        "#         x = x.to(self.device)\n",
        "#         y = y.to(self.device)\n",
        "#         # forward the model\n",
        "#         with torch.set_grad_enabled(is_train):\n",
        "#             logits, loss = model(x, y)\n",
        "#             loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "#             losses.append(loss.item())\n",
        "\n",
        "# def forward(self, idx, targets=None):\n",
        "#     b, t = idx.size()\n",
        "#     assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "#     # forward the GPT model\n",
        "#     token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "#     position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "#     x = self.drop(token_embeddings + position_embeddings)\n",
        "#     x = self.blocks(x)\n",
        "#     x = self.ln_f(x)\n",
        "#     logits = self.head(x)\n",
        "#     # if we are given some desired targets also calculate the loss\n",
        "#     loss = None\n",
        "#     if targets is not None:\n",
        "#         loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "#     return logits, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BC1MuKSm1Y"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73GQFkObQTOr",
        "outputId": "2085fed4-58b6-44c3-ee99-874ab001fc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 267: train loss 1.38706. lr 3.001374e-04: 100%|██████████| 268/268 [01:08<00:00,  3.89it/s]\n",
            "epoch 2 iter 267: train loss 1.11898. lr 6.000000e-05: 100%|██████████| 268/268 [01:10<00:00,  3.79it/s]\n"
          ]
        }
      ],
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "# tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size, num_workers=4)\n",
        "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size, num_workers=0)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7AthNlpSjN2"
      },
      "source": [
        "#### eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwTYEnZmQTOs",
        "outputId": "ea3f1575-1bd1-4bda-fa47-704b74518525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O God, O God! that e'er this tongue of mine,\n",
            "That laid the sentence of dread banishment\n",
            "On yon proud man, should take it off again\n",
            "With words of sooth! O that I were as great\n",
            "As is my grief, or lesser than my name!\n",
            "Or that I could forget\n",
            "With Richmond, I'll tell you what I am,\n",
            "The Lord Aumerle, .\n",
            "\n",
            "CLAUDIO:\n",
            "The prenzie Angelo!\n",
            "\n",
            "ISABELLA:\n",
            "O, 'tis the cunning livery of hell,\n",
            "The damned'st body to invest and cover\n",
            "In prenzie guards! Dost thou think, Claudio?\n",
            "If I would yield him my virginity,\n",
            "Thou mightst be freed.\n",
            "\n",
            "CLAUDIO:\n",
            "O heavens! it cannot be.\n",
            "\n",
            "ISABELLA:\n",
            "Yes, he would give't thee, from this rank offence,\n",
            "So to offend him still. This night's the time\n",
            "That I should do what I abhor to name,\n",
            "Or else thou diest to-morrow.\n",
            "\n",
            "CLAUDIO:\n",
            "Thou shalt not do't.\n",
            "\n",
            "ISABELLA:\n",
            "O, were it but my life,\n",
            "I'ld throw it down for your deliverance\n",
            "As frankly as a pin.\n",
            "\n",
            "CLAUDIO:\n",
            "Thanks, dear Isabel.\n",
            "\n",
            "ISABELLA:\n",
            "Be ready, Claudio, for your death tomorrow.\n",
            "\n",
            "CLAUDIO:\n",
            "Yes. Has he affections\n",
            "That profit us.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "By the world they see the word in 's doom.\n",
            "\n",
            "ANGELO:\n",
            "Thou art the like, thus hate the course in heaven.\n",
            "What foul hath bled the wheel or at wild,\n",
            "And wash him fresh again with true-love tears.\n",
            "Ah, thou, the model where old Troy did stand,\n",
            "Thou map of honour, thou King Richard's tomb,\n",
            "And not King Richard; thou most beauteous inn,\n",
            "Why should hard-favour'd grief be lodged in thee,\n",
            "When triumph is become an alehouse guest?\n",
            "\n",
            "KING RICHARD II:\n",
            "Join not with grief, fair woman, do not so,\n",
            "To make my end too sudden: learn, good soul,\n",
            "To think our former state a happy dream;\n",
            "From which awaked, the truth of what we are\n",
            "Shows us but this: I am sworn brother, sweet,\n",
            "To grim Necessity, and he and I\n",
            "Will keep a league till death. Hie thee to France\n",
            "And cloister thee in some religious house:\n",
            "Our holy lives must win a new world's crown,\n",
            "Which our profane hours here have stricken down.\n",
            "\n",
            "QUEEN:\n",
            "What, is my Richard both in shape and mind\n",
            "Transform'd and weaken'd? hath Bolingbroke deposed\n",
            "Thine intellect? hath h\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# alright, let's sample some character-level Shakespeare\n",
        "from mingpt.utils import sample\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RBmoZfW8SF7Z",
        "wutRcUgqSLtH",
        "ZRPHfXWxf1-u"
      ],
      "name": "minGPT-play_char.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/curiousity/curiousity_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQjuJIepAvA"
      },
      "source": [
        "#### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1GD7lk8H13h",
        "outputId": "4db9c5a5-5830-4b3c-b5be-adee2ba88673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "# # https://github.com/kimhc6028/pytorch-noreward-rl\n",
        "# https://stackoverflow.com/questions/67808779/running-gym-atari-in-google-colab\n",
        "%pip install -U gym\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "# import gym\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "!pip install colabgymrender\n",
        "!pip install perceiver-pytorch\n",
        "\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            self.total_rewards = 0\n",
        "            return observation, 0, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        # print(\"MarioSparse\",life,score)\n",
        "        # if done: return observation, self.total_rewards, done, info\n",
        "        if life<2: return observation, score, True, info # lost one life, end env\n",
        "        else:\n",
        "            # self.total_score = 0\n",
        "            return observation, score, False, info\n",
        "    def reset(self):\n",
        "        # self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "log=False\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.login() # \n",
        "# wandb.init(project=\"curiousity_simple\", entity=\"bobdole\")\n",
        "# log=True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            self.total_rewards = 0\n",
        "            return observation, 0, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        if done: return observation, reward, done, info\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            return observation, score, True, info # lost one life, end env\n",
        "        else:\n",
        "            # self.total_score = 0\n",
        "            return observation, score, False, info\n",
        "    def reset(self):\n",
        "        # self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        if done: return observation, reward, done, info\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            return observation, reward, True, info # early stop\n",
        "        else:\n",
        "            return observation, reward, False, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "qdSVAFKY6RA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidkEuaA8HvK"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGpJGeDM8HvU"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/model.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, in_shape, action_space):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.in_dim = in_shape # mario (240, 256)\n",
        "        self.conv = nn.Sequential( # embed pi\n",
        "            nn.Conv2d(in_shape[0], 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1), nn.ELU(), # added for RuntimeError: Input batch size 2 doesn't match hidden0 batch size 1\n",
        "            )\n",
        "        self.lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        num_outputs = action_space.n\n",
        "        self.actor_linear = nn.Linear(256, num_outputs) # -> action\n",
        "        self.critic_linear = nn.Linear(256, 1) # -> value\n",
        "\n",
        "        self.inv_lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        self.fwd_lstm = nn.LSTMCell(in_shape[1], 256)\n",
        "        self.inv_linear = nn.Sequential( # inv learning, predict at\n",
        "            nn.Linear(in_shape[1] + in_shape[1], 256), nn.ReLU(),\n",
        "            nn.Linear(256, num_outputs), nn.Softmax()\n",
        "            ) # cat(phi(st), phi(st+1)) -> athat\n",
        "        self.fwd_linear = nn.Sequential( # predict phi st+1\n",
        "            nn.Linear(in_shape[1] + num_outputs, 256), nn.ReLU(),\n",
        "            nn.Linear(256, in_shape[1])\n",
        "            ) # cat(phi(st), at) -> phihat(st+1)\n",
        "\n",
        "    def forward(self, inputs, icm):\n",
        "        if icm == False: #A3C\n",
        "            st, (a3c_hx, a3c_cx) = inputs # [1, 210, 160, 3], ([1, 256], [1, 256])\n",
        "            vec_st = self.conv(st).view(-1, self.in_dim[1])\n",
        "            a3c_hx1, a3c_cx1 = self.lstm(vec_st, (a3c_hx, a3c_cx))\n",
        "            critic = self.critic_linear(a3c_hx1)\n",
        "            actor = self.actor_linear(a3c_hx1)\n",
        "            # print(\"forward A3C \",critic.shape, actor.shape, a3c_hx.shape, a3c_cx.shape)\n",
        "            return critic, actor, (a3c_hx1, a3c_cx1) # [1, 1], [1, 12], ([1, 256], [1, 256])\n",
        "\n",
        "        else: #icm\n",
        "            (inv_hx, inv_cx), (fwd_hx, fwd_cx), st1, at = inputs\n",
        "            vec_st1 = self.conv(st1).view(-1, self.in_dim[1])\n",
        "            inv_hx1, inv_cx1 = self.inv_lstm(vec_st1, (inv_hx, inv_cx)) # inv model\n",
        "            fwd_hx1, fwd_cx1 = self.fwd_lstm(vec_st1, (fwd_hx, fwd_cx)) # world model\n",
        "\n",
        "            inv_vec = torch.cat((inv_hx, vec_st1), 1) # predict at\n",
        "            fwd_vec = torch.cat((fwd_hx, at), 1) # predict vec_st1\n",
        "            inverse = self.inv_linear(inv_vec)\n",
        "            forward = self.fwd_linear(fwd_vec)\n",
        "            # print(\"forward icm \",vec_st1.shape, inverse.shape, forward.shape)\n",
        "            return vec_st1, inverse, forward, (inv_hx1, inv_cx1), (fwd_hx1, fwd_cx1) # [1, 320], [1, 18], [1, 320], ()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpPda4YEv13"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erf14x8KEv2A"
      },
      "outputs": [],
      "source": [
        "# train.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/train.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def train(env, args, model, optimizer=None):\n",
        "    # torch.manual_seed(seed)\n",
        "    # model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "    if optimizer is None:\n",
        "        optimizer = torch.optim.Adam(shared_model.parameters(), lr)\n",
        "    model.train()\n",
        "    for x in range(num_episodes):\n",
        "        # model.load_state_dict(shared_model.state_dict()) # Sync with the shared model\n",
        "        hx = torch.zeros(1, 256).to(device)\n",
        "        cx = torch.zeros(1, 256).to(device)\n",
        "        vec_st = torch.zeros(1, 256).to(device)\n",
        "        inv_latent = (torch.zeros(1, 256).to(device), torch.zeros(1, 256).to(device))\n",
        "        fwd_latent = (torch.zeros(1, 256).to(device), torch.zeros(1, 256).to(device))\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropies = []\n",
        "        inverses = []\n",
        "        forwards = []\n",
        "        actions = []\n",
        "        vec_st1s = []\n",
        "        episode_length = 0\n",
        "\n",
        "        state = env.reset()\n",
        "        # state=state[:,:,0]\n",
        "        state = torch.from_numpy(state.copy()).type(torch.float).to(device) # i added, change from int to float\n",
        "        st1 = state.float()\n",
        "        # print(\"#####www####\",state.dtype,hx.dtype)\n",
        "        while True:\n",
        "            episode_length += 1\n",
        "            value, logit, (hx, cx) = model((state.unsqueeze(0), (hx, cx)), icm = False)\n",
        "            prob = F.softmax(logit, dim=1)\n",
        "            log_prob = F.log_softmax(logit, dim=1)\n",
        "            entropy = -(log_prob * prob).sum(1)\n",
        "            entropies.append(entropy.cpu())\n",
        "            action = prob.multinomial(1).data\n",
        "            log_prob = log_prob.gather(1, action)\n",
        "            oh_action = torch.zeros(1, env.action_space.n)\n",
        "            oh_action[0][action.item()] = 1.0\n",
        "            at = oh_action\n",
        "            actions.append(oh_action)\n",
        "            state, reward, done, _ = env.step(action.item())\n",
        "            state = torch.from_numpy(state.copy()).type(torch.float).to(device)\n",
        "            # state=state[:,:,0]\n",
        "            # print(\"reward\",reward)\n",
        "            done = done or episode_length >= max_episode_length\n",
        "            # reward = max(min(reward, 1), -1) #why clip rewards?\n",
        "            st = st1\n",
        "            st1 = state.float()          \n",
        "            vec_st1, inverse, forward, inv_latent, fwd_latent = model((inv_latent, fwd_latent, st1.unsqueeze(0), at.to(device)), icm = True)            \n",
        "\n",
        "            reward_intrinsic = eta * ((vec_st1 - forward).pow(2)).sum(1) / 2.\n",
        "            #reward_intrinsic = eta * ((vec_st1 - forward).pow(2)).sum(1).sqrt() / 2.\n",
        "            # print(\"reward_intrinsic\", reward_intrinsic)\n",
        "            reward_intrinsic = reward_intrinsic.item()\n",
        "            # print(\"ep \",x,\", rwd ext: \", reward, \" ,rwd int: \", reward_intrinsic.item())\n",
        "            reward += reward_intrinsic\n",
        "            values.append(value.cpu())\n",
        "            log_probs.append(log_prob.cpu())\n",
        "            rewards.append(reward)\n",
        "            vec_st1s.append(vec_st1.cpu())\n",
        "            inverses.append(inverse.cpu())\n",
        "            forwards.append(forward.cpu())\n",
        "            if done:\n",
        "                print(episode_length)\n",
        "                episode_length = 0\n",
        "                break\n",
        "        R = torch.zeros(1, 1)\n",
        "        if not done:\n",
        "            value, _, _ = model((state.unsqueeze(0), (hx, cx)), icm = False)\n",
        "            R = value.data\n",
        "        values.append(R)\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "        inverse_loss = 0\n",
        "        forward_loss = 0\n",
        "        gae = torch.zeros(1, 1)\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = gamma * R + rewards[i]\n",
        "            advantage = R - values[i]\n",
        "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
        "            # Generalized Advantage Estimataion\n",
        "            # delta_t = rewards[i] + gamma * values[i + 1].data - values[i].data\n",
        "            delta_t = torch.tensor(rewards[i]) + gamma * values[i + 1].data - values[i].data\n",
        "            gae = gae * gamma * tau + delta_t\n",
        "            policy_loss = policy_loss - log_probs[i] * gae - 0.01 * entropies[i]\n",
        "            cross_entropy = - (actions[i] * torch.log(inverses[i] + 1e-15)).sum(1)\n",
        "            inverse_loss = inverse_loss + cross_entropy\n",
        "            forward_err = forwards[i] - vec_st1s[i]\n",
        "            forward_loss = forward_loss + 0.5 * (forward_err.pow(2)).sum(1)\n",
        "        optimizer.zero_grad()\n",
        "        # ((1-beta) * inverse_loss + beta * forward_loss).backward(retain_variables=True)\n",
        "        inv_loss = (1-beta) * inverse_loss + beta * forward_loss\n",
        "        pol_loss = lmbda * (policy_loss + 0.5 * value_loss)\n",
        "        (inv_loss + pol_loss).backward()\n",
        "        # (inv_loss + 0*pol_loss).backward()\n",
        "        # (((1-beta) * inverse_loss + beta * forward_loss) + lmbda * (policy_loss + 0.5 * value_loss)).backward()\n",
        "        print(''.join([str(torch.argmax(a).item()) for a in actions]))\n",
        "        print(\"inv_loss: \", inv_loss.item(), \" ,pol_loss: \", pol_loss.item())\n",
        "        # if log: \n",
        "        #     wandb.log({\"inv_loss\": inv_loss.item(), \"pol_loss\": pol_loss.item()})\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 40)\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHpcDteZdLS"
      },
      "source": [
        "#### test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja6KZf13p--B"
      },
      "outputs": [],
      "source": [
        "# test.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/test.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def test(env, args, model):\n",
        "    # torch.manual_seed(seed)\n",
        "    # model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "    # model.load_state_dict(shared_model.state_dict())\n",
        "    model.eval()\n",
        "    state = env.reset()\n",
        "    state = torch.from_numpy(state.copy()).type(torch.float)\n",
        "    reward_sum = 0\n",
        "    start_time = time.time()\n",
        "    actions = []\n",
        "    episode_length = 0\n",
        "    result = []\n",
        "    cx = torch.zeros(1, 256).to(device)\n",
        "    hx = torch.zeros(1, 256).to(device)\n",
        "    while True:\n",
        "        episode_length += 1\n",
        "        # value, logit, (hx, cx) = model((state.unsqueeze(0), (hx, cx)), icm = False)\n",
        "        value, logit, (hx, cx) = model((state.unsqueeze(0).to(device), (hx, cx)), icm = False)\n",
        "        # action = prob.max(1)[1].data.numpy() #stupid from test\n",
        "        prob = F.softmax(logit, dim=1) #from train\n",
        "        action = prob.multinomial(1).data\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "        state = torch.from_numpy(state.copy()).type(torch.float)\n",
        "\n",
        "        done = done or episode_length >= max_episode_length\n",
        "        # print(\"rwd ext: \", reward)\n",
        "        reward_sum += reward\n",
        "        actions.append(action[0])\n",
        "        if done:\n",
        "            end_time = time.time()\n",
        "            print(\"Time {}, episode reward {}, episode length {}\".format(\n",
        "                time.strftime(\"%Hh %Mm %Ss\", time.gmtime(end_time - start_time)), reward_sum, episode_length))\n",
        "            result.append((reward_sum, end_time - start_time))\n",
        "            torch.save(model.state_dict(), 'model.pth')\n",
        "            # print(''.join([str(a.item()) for a in actions]))\n",
        "            print([a.item() for a in actions])\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj3tv7XHZmD9"
      },
      "source": [
        "#### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjPLqIBgH9xJ",
        "outputId": "92dca849-51bd-43b8-fb20-ed13dcc29e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:565: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/main.py\n",
        "# import os, sys, cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "\n",
        "lr=0.001\n",
        "gamma=0.99\n",
        "tau=1.00\n",
        "seed=1\n",
        "num_processes=4\n",
        "num_steps=20\n",
        "max_episode_length=500 # 10000\n",
        "# env_name='PongDeterministic-v4'\n",
        "# env_name='LunarLander-v2'\n",
        "# env_name='MontezumaRevengeDeterministic-v4'\n",
        "# env_name='MontezumaRevengeDeterministic-ram-v4'\n",
        "\n",
        "no_shared=False\n",
        "eta=0.01\n",
        "beta=0.2\n",
        "lmbda=0.1\n",
        "outdir=\"output\"\n",
        "record='store_true'\n",
        "num_episodes=10#100\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "# env = gym.make(env_name)\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = MarioEarlyStop(env)\n",
        "# query_environment(\"MountainCar-v0\")\n",
        "\n",
        "# print(env.observation_space.shape, env.action_space) # (210, 160, 3) Discrete(18)\n",
        "\n",
        "shared_model = ActorCritic(env.observation_space.shape, env.action_space).to(device)\n",
        "# shared_model.share_memory()\n",
        "if no_shared:\n",
        "    optimizer = None\n",
        "else:\n",
        "    optimizer = torch.optim.Adam(shared_model.parameters(), lr=lr)\n",
        "    # optimizer.share_memory()\n",
        "args=None\n",
        "# train(0, args, shared_model, optimizer)\n",
        "\n",
        "# processes = []\n",
        "# import torch.multiprocessing as mp\n",
        "# p = mp.Process(target=test, args=(num_processes, args, shared_model))\n",
        "# p.start()\n",
        "# processes.append(p)\n",
        "# for rank in range(0, num_processes):\n",
        "#     p = mp.Process(target=train, args=(rank, args, shared_model, optimizer))\n",
        "#     p.start()\n",
        "#     processes.append(p)\n",
        "# for p in processes:\n",
        "#     p.join()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFrRKvOwhYM_"
      },
      "source": [
        "#### wwwwwwwww"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KuzIfVWPBIg"
      },
      "outputs": [],
      "source": [
        "max_episode_length=1000 # 10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHLw7ewldCLm"
      },
      "outputs": [],
      "source": [
        "# train(env, args, shared_model)\n",
        "\n",
        "for x in range(25):\n",
        "    train(env, args, shared_model, optimizer)\n",
        "test(env, args, shared_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlV2MvSK-aL_"
      },
      "source": [
        "#### save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-XW-LvZ8Xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/curious/\" # for saving to google drive\n",
        "# PATH=\"/content\" # for saving on colab only\n",
        "name='model_mario_lstm.pth'\n",
        "model=shared_model\n",
        "# torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# shared_model=model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6PNxVu-W6K"
      },
      "source": [
        "#### video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fkhEcBB3tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# # env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "# model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "# model.load_state_dict(shared_model.state_dict())\n",
        "# model.eval()\n",
        "cx = torch.zeros(1, 256)\n",
        "hx = torch.zeros(1, 256)\n",
        "# torch.manual_seed(6)\n",
        "x=0\n",
        "\n",
        "# [11, 0, 11, 7, 9, 10, 9, 11, 1, 1, 4, 10, 1, 5, 11, 5, 1, 4, 7, 5, 11, 11, 4, 1, 9, 0, 1, 11, 1, 1, 1, 10, 1, 1, 1, 5, 9, 9, 9, 11, 9, 10, 11, 4, 10, 1, 10, 1, 1, 9, 1, 1, 11, 9, 1, 1, 10, 10, 9, 10, 11, 11, 9, 3, 10, 9, 3, 1, 9, 0, 9, 5, 8, 9, 8, 5, 3, 1, 5, 11, 1, 11, 10, 1, 9, 5, 11, 7, 3, 4, 2, 3, 0, 3, 9, 3, 1, 11, 1, 1, 3, 3, 9, 9, 9, 9, 9, 0, 9, 9, 9, 1, 4, 4, 1, 10, 10, 9, 10, 0, 4, 1, 11, 9, 1, 4, 2, 1, 9, 4, 1, 8, 0, 9, 9, 1, 2, 5, 3, 4, 6, 4, 3, 2, 2, 5, 10, 6, 10, 10, 11, 11, 10, 9, 9, 11, 0, 9, 10, 11, 4, 4, 2, 2, 10, 9, 11, 2, 9, 4, 10, 9, 8, 1, 1, 9, 3, 1, 8, 11, 9, 9, 9, 9, 0, 7, 1, 4, 4, 11, 1, 5, 1, 4, 10, 2, 5, 1, 11, 4, 4, 4, 1, 10, 3, 1, 1, 1, 11, 11, 1, 11, 11, 1, 1, 10, 9, 1, 1, 9, 5, 10, 3, 4, 11, 2, 9, 4, 1, 9, 1, 1, 1, 9, 2, 4, 1, 1, 11, 1, 4, 2, 5, 2, 9, 1, 11, 1, 9, 11, 9, 0, 1, 1, 4, 10, 7, 7, 9, 3, 11, 10, 1, 1, 1, 10, 11, 1, 2, 10, 10, 1, 9, 3, 11, 1, 5, 3, 1, 7, 9, 1, 1, 4, 1, 9, 5, 4, 1, 5, 3, 4, 11, 1, 11, 1, 4, 1, 4, 10, 11, 1, 10, 1, 11, 10, 2, 3, 4, 1, 9, 1, 9, 5, 1, 4, 9, 5, 5, 4, 11, 1, 10, 9, 10, 0, 9, 9, 9, 1, 4, 9, 1, 1, 1, 9, 4, 7, 9, 4, 10, 9, 4, 1, 5, 4, 9, 11, 10, 9, 9, 9, 1]\n",
        "# [5, 0, 9, 4, 4, 4, 5, 11, 0, 8, 1, 2, 7, 11, 10, 4, 11, 11, 4, 4, 11, 4, 9, 4, 4, 1, 4, 10, 4, 0, 6, 5, 0, 6, 10, 11, 5, 11, 9, 7, 0, 4, 9, 8, 10, 4, 10, 11, 0, 2, 6, 7, 10, 2, 11, 0, 10, 2, 10, 0, 10, 4, 8, 10, 10, 4, 4, 4, 8, 4, 11, 4, 7, 9, 4, 11, 2, 4, 4, 11, 4, 10, 4, 4, 0, 4, 2, 10, 11, 4, 4, 10, 10, 4, 2, 1, 5, 0, 9, 10, 10, 0, 0, 4, 0, 4, 4, 2, 10, 0, 11, 4, 10, 8, 4, 0, 7, 7, 8, 10, 2, 4, 4, 9, 4, 4, 10, 2, 1, 8, 2, 4, 4, 4, 4, 11, 4, 4, 10, 11, 10, 0, 4, 11, 10, 3, 2, 6, 4, 4, 7, 4, 4, 4, 1, 9, 11, 4, 8, 11, 4, 4, 2, 4, 4, 7, 4, 9, 9, 11, 4, 10, 7, 11, 8, 0, 11, 2, 9, 10, 4, 11, 0, 10, 7, 4, 9, 4, 1, 11, 4, 10, 9, 4, 3, 4, 11, 0, 4, 10, 4, 11, 10, 4, 3, 0, 11, 7, 4, 4, 0, 4, 4, 10, 0, 4, 2, 11, 4, 2, 3, 11, 2, 7, 4, 9, 4, 10, 11, 11, 11, 1, 0, 10, 4, 3, 7, 8, 2, 2, 4, 5, 4, 9, 4, 4, 0, 4, 0, 11, 0, 6, 4, 11, 11, 4, 0, 0, 0, 7, 11, 0, 10, 10, 0, 10, 4, 9, 7, 4, 6, 8, 9, 4, 9, 4, 7, 2, 4, 7, 2, 8, 0, 9, 4, 6, 4, 10, 9, 7, 4, 4, 4, 8, 9, 4, 4, 9, 4, 10, 7, 4, 9, 6, 10, 11, 0, 4, 4, 0, 4, 11, 2, 0, 0, 3, 11, 0, 10, 4, 8, 10, 10, 2, 10, 10, 4, 4, 4, 1, 4, 4, 9, 0, 1, 4, 10, 4, 0, 10, 4, 0, 4, 9, 4, 7, 10, 10, 4, 4, 1, 9, 7, 7, 10, 4, 3, 10, 9, 4, 4, 4, 9, 10, 4, 8, 1, 11, 4, 9, 4, 4, 10, 1, 0, 0, 11, 1, 4, 4, 7, 9, 11, 10, 4, 1, 3, 7, 2, 4, 11, 2, 4, 8, 8, 4, 4, 3, 4, 7, 11, 0, 11, 10, 4, 11, 5, 1, 9, 4, 1, 3, 2, 9, 2, 2, 9, 4, 4, 11, 4, 7, 0, 0, 2, 2, 8, 11, 7, 6, 10, 8, 4, 0, 4, 2, 11, 0, 4, 8, 4, 4, 2, 0, 10, 7, 4, 4, 11, 8, 8, 5, 0, 0, 0, 4, 9, 7, 10, 11, 4, 9, 0, 11, 6, 8, 7, 7, 10, 4, 10, 10, 10, 0, 11, 11, 8, 6, 4, 10, 10, 9, 4, 6, 10, 4, 4, 2, 4, 11, 4, 4, 7, 2, 4, 4, 10, 4, 7, 0]\n",
        "# [7, 11, 9, 7, 5, 5, 4, 1, 4, 7, 11, 5, 7, 1, 7, 6, 10, 7, 10, 10, 4, 10, 5, 7, 10, 4, 5, 2, 6, 5, 11, 4, 6, 4, 8, 5, 10, 10, 9, 10, 2, 7, 1, 6, 4, 10, 10, 2, 4, 4, 10, 3, 9, 10, 5, 5, 9, 0, 8, 10, 4, 10, 0, 7, 1, 4, 5, 9, 4, 11, 7, 10, 2, 10, 1, 1, 10, 5, 10, 10, 2, 4, 2, 10, 2, 2, 6, 1, 3, 10, 10, 4, 4, 2, 1, 9, 1, 10, 9, 4, 10, 7, 6, 2, 11, 4, 1, 4, 5, 10, 7, 10, 0, 10, 7, 10, 4, 4, 4, 3, 1, 10, 5, 10, 10, 10, 7, 10, 5, 7, 11, 10, 4, 0, 10, 7, 6, 7, 10, 4, 3, 4, 7, 5, 1, 7, 7, 7, 10, 7, 1, 4, 11, 11, 2, 8, 11, 5, 10, 2, 1, 4, 10, 6, 4, 10, 4, 1, 6, 4, 5, 6, 10, 1, 7, 10, 1, 0, 4, 7, 10, 8, 4, 4, 10, 2, 2, 10, 4, 3, 4, 8, 5, 10, 4, 2, 6, 7, 10, 5, 5, 4, 1, 10, 11, 10, 8, 4, 11, 7, 11, 7, 5, 7, 11, 10, 2, 11, 4, 10, 2, 7, 6, 6, 4, 5, 5, 1, 10, 4, 9, 8, 10, 4, 9, 2, 10, 10, 10, 2, 2, 3, 10, 10, 1, 7, 4, 10, 6, 7, 10, 10, 3, 9, 10, 10, 0, 8, 11, 7, 5, 11, 5, 5, 4, 4, 1, 2, 9, 2, 7, 7, 4, 5, 1, 4, 11, 6, 1, 3, 3, 6, 10, 10, 0, 10, 10, 7, 1, 5, 0, 4, 5, 5, 9, 7, 4, 10, 10, 5, 7, 10, 7, 4, 11, 5, 2, 2, 5, 0, 9, 3, 9, 6, 4, 9, 9, 4, 5, 1, 4, 7, 6, 9, 7, 6, 10, 4, 4, 5, 11, 7, 5, 4, 3, 10, 10, 11, 8, 5, 10, 10, 10, 11, 9, 1, 6, 5, 5, 10, 11, 10, 10, 1, 6, 2, 0, 1, 8, 5, 7, 11, 5, 2, 4, 0, 10, 4, 11, 4, 6, 3, 4, 4, 5, 4, 5, 10, 7, 7, 5, 8, 7, 1, 0, 0, 7, 7, 1, 1, 5, 5, 9, 3, 2, 4, 2, 6, 6, 10, 5, 2, 4, 7, 4, 10, 10, 4, 10, 8, 1, 10, 7, 10, 8, 10, 3, 5, 6, 4, 2, 2, 2, 7, 10, 10, 4, 9, 7, 1, 4, 6, 4, 2, 5, 7, 10, 10, 1, 11, 4, 10, 5, 1, 4, 4, 4, 8, 5, 10, 11, 4, 5, 7, 10, 2, 10, 4, 4, 6, 3, 10, 10, 10, 7, 5, 10, 2, 10, 8, 10, 9, 10, 0, 8, 5, 2, 8, 10, 5, 5, 6, 1, 11, 10, 9, 8, 7, 8, 2, 2, 1, 4, 10, 10, 3, 11, 10, 4, 5]\n",
        "# acts= [10, 2, 11, 7, 2, 1, 4, 10, 1, 10, 2, 0, 2, 1, 1, 1, 10, 7, 1, 1, 6, 4, 2, 10, 4, 6, 0, 3, 6, 10, 2, 2, 11, 7, 10, 4, 10, 9, 4, 1, 4, 4, 5, 0, 1, 6, 0, 10, 7, 5, 11, 5, 4, 6, 7, 1, 1, 8, 10, 2, 5, 5, 11, 1, 1, 10, 10, 1, 0, 5, 11, 6, 2, 1, 6, 8, 4, 10, 4, 6, 2, 1, 4, 1, 7, 6, 2, 4, 1, 2, 10, 11, 2, 10, 11, 0, 1, 1, 1, 5, 1, 2, 1, 0, 3, 2, 10, 4, 1, 8, 4, 1, 9, 11, 11, 2, 2, 4, 5, 6, 1, 4, 8, 1, 10, 6, 5, 11, 2, 10, 2, 2, 2, 4, 6, 9, 10, 2, 2, 2, 6, 1, 1, 2, 4, 10, 1, 11, 2, 1, 10, 7, 1, 1, 0, 8, 2, 2, 0, 11, 1, 6, 1, 0, 2, 1, 1, 11, 0, 7, 0, 2, 4, 5, 9, 9, 2, 1, 5, 7, 0, 9, 1, 0, 10, 0, 2, 1, 2, 11, 10, 10, 6, 3, 10, 2, 10, 1, 10, 10, 7, 10, 10, 11, 0, 1, 0, 1, 4, 11, 1, 2, 10, 7, 4, 3, 0, 2, 2, 5, 2, 4, 1, 9, 3, 10, 10, 2, 1, 7, 2, 10, 2, 5, 1, 1, 4, 10, 6, 1, 9, 10, 1, 7, 10, 0, 2, 11, 7, 1, 10, 4, 11, 2, 10, 7, 1, 11, 11, 6, 0, 6, 2, 1, 0, 4, 5, 2, 1, 1, 2, 10, 11, 10, 0, 2, 2, 0, 3, 1, 0, 10, 9, 5, 1, 6, 2, 1, 4, 10, 6, 1, 4, 0, 7, 5, 9, 2, 3, 6, 7, 6, 1, 6, 10, 8, 2, 1, 7, 1, 10, 1, 10, 7, 4, 1, 1, 1, 4, 2, 2, 4, 1, 1, 10, 2, 7, 1, 1, 10, 5, 1, 1, 7, 10, 11, 4, 1, 2, 1, 0, 10, 10, 1, 11, 2, 1, 10, 8, 9, 10, 10, 9, 2, 4, 4, 7, 10, 2, 0, 1, 1, 4, 6, 5, 11, 3, 10, 7, 10, 5, 8, 11, 0, 11, 7, 1, 10, 8, 1, 9, 10, 4, 7, 7, 7, 3, 7, 3, 2, 2, 11, 10, 10, 1, 0, 10, 10, 3, 7, 10, 9, 1, 1, 5, 6, 7, 9, 11, 10, 10, 4, 10, 8, 1, 2, 11, 4, 5, 1, 5, 6, 6, 0, 1, 3, 4, 5, 1, 8, 11, 4, 10, 10, 7, 4, 1, 10, 10, 1, 11, 11, 0, 0, 7, 11, 5, 2, 6, 5, 10, 5, 1, 6, 1, 10, 11, 11, 11, 4, 0, 4, 4, 1, 7, 6, 10, 10, 4, 2, 7, 0, 7, 6, 4, 2, 10, 1, 10, 1, 2, 6, 0, 1, 10, 11, 10, 10, 10, 6, 2, 10, 6, 4, 2, 8, 11, 10, 5, 0]\n",
        "# [11, 2, 11, 5, 7, 10, 11, 11, 11, 0, 2, 4, 0, 11, 10, 1, 4, 9, 6, 2, 1, 2, 7, 7, 5, 11, 2, 11, 2, 4, 11, 7, 6, 0, 11, 4, 11, 4, 10, 1, 6, 2, 5, 0, 7, 7, 11, 2, 1, 4, 2, 10, 4, 2, 2, 1, 4, 11, 0, 3, 11, 2, 2, 0, 2, 4, 11, 2, 0, 5, 2, 2, 2, 2, 4, 10, 2, 1, 2, 0, 11, 4, 10, 11, 1, 1, 10, 2, 2, 2, 2, 2, 11, 6, 11, 11, 11, 11, 6, 11, 11, 9, 6, 11, 2, 11, 11, 1, 1, 2, 4, 7, 6, 1, 2, 0, 11, 4, 7, 5, 2, 4, 4, 1, 10, 11, 2, 3, 4, 7, 4, 2, 2, 7, 11, 11, 7, 7, 1, 11, 2, 10, 11, 2, 10, 11, 11, 10, 7, 9, 1, 1, 11, 6, 2, 9, 2, 4, 2, 9, 11, 7, 2, 2, 2, 10, 11, 1, 11, 5, 10, 2, 2, 0, 11, 7, 3, 7, 10, 6, 0, 11, 5, 4, 5, 4, 11, 11, 7, 11, 2, 11, 7, 10, 4, 6, 4, 7, 2, 5, 5, 2, 2, 2, 7, 1, 6, 8, 11, 4, 10, 11, 0, 1, 4, 4, 8, 1, 2, 10, 2, 1, 7, 11, 4, 0, 9, 8, 5, 4, 4, 2, 7, 9, 1, 0, 4, 11, 11, 6, 10, 2, 5, 2, 2, 11, 2, 8, 4, 0, 2, 1, 0, 11, 4, 4, 9, 4, 7, 6, 4, 10, 11, 0, 2, 2, 5, 10, 11, 11, 2, 1, 11, 2, 5, 1, 4, 2, 0, 7, 5, 6, 7, 2, 2, 7, 2, 8, 9, 2, 9, 4, 1, 1, 4, 11, 2, 3, 2, 11, 6, 7, 11, 6, 5, 4, 7, 6, 2, 2, 4, 4, 10, 2, 7, 2, 2, 11, 3, 2, 11, 11, 2, 11, 0, 2, 7, 11, 2, 8, 7, 11, 2, 10, 11, 6, 11, 7, 6, 7, 2, 11, 2, 7, 11, 7, 2, 4, 0, 9, 2, 11, 1, 7, 7, 7, 1, 4, 11, 11, 11, 2, 0, 0, 0, 2, 4, 2, 2, 5, 2, 7, 11, 5, 10, 4, 11, 5, 11, 4, 6, 10, 2, 7, 2, 6, 2, 7, 2, 4, 11, 2, 2, 9, 3, 11, 6, 6, 2, 4, 1, 6, 10, 10, 7, 2, 1, 5, 10, 11, 2, 2, 11, 5, 7, 2, 2, 4, 5, 1, 4, 5, 4, 7, 0, 10, 4, 4, 7, 2, 8, 5, 0, 11, 0, 4, 11, 4, 2, 5, 6, 4, 11, 11, 11, 4, 1, 2, 6, 2, 10, 4, 2, 2, 11, 2, 8, 11, 5, 6, 6, 11, 6, 2, 4, 2, 1, 11, 5, 7, 7, 2, 5, 11, 1, 11, 2, 9, 2, 2, 7, 11, 5, 11, 7, 10, 11, 2, 0, 8, 11, 4, 11, 4, 5, 10, 2, 2, 6, 10]\n",
        "# [9, 4, 2, 7, 1, 8, 2, 2, 11, 2, 1, 1, 11, 4, 2, 5, 1, 11, 2, 2, 1, 9, 7, 2, 2, 8, 1, 2, 2, 6, 6, 2, 2, 6, 8, 2, 2, 8, 1, 2, 2, 10, 2, 2, 6, 10, 2, 11, 11, 2, 10, 6, 10, 2, 2, 4, 2, 1, 8, 2, 10, 8, 8, 7, 2, 2, 1, 11, 1, 9, 2, 2, 5, 2, 2, 2, 6, 2, 6, 5, 2, 2, 6, 5, 10, 2, 2, 6, 1, 9, 11, 2, 9, 2, 2, 1, 2, 10, 2, 1, 8, 2, 2, 2, 2, 10, 2, 2, 2, 0, 2, 1, 10, 2, 2, 2, 2, 1, 2, 6, 6, 1, 1, 5, 2, 2, 2, 2, 2, 1, 7, 11, 8, 2, 2, 2, 6, 2, 2, 6, 2, 6, 10, 1, 2, 4, 2, 5, 7, 6, 2, 4, 2, 2, 6, 2, 2, 2, 5, 2, 4, 11, 5, 4, 2, 0, 2, 10, 9, 2, 7, 5, 2, 2, 2, 1, 2, 7, 10, 11, 8, 10, 10, 2, 6, 10, 1, 1, 3, 2, 11, 7, 2, 10, 2, 4, 0, 1, 2, 2, 6, 7, 0, 1, 2, 6, 8, 2, 8, 2, 6, 7, 2, 11, 11, 2, 1, 8, 8, 2, 5, 2, 7, 9, 3, 2, 2, 5, 2, 2, 2, 11, 7, 5, 2, 5, 7, 8, 2, 2, 6, 7, 10, 2, 1, 10, 2, 2, 2, 2, 10, 6, 5, 2, 5, 1, 1, 2, 1, 9, 2, 2, 2, 2, 11, 2, 6, 8, 2, 6, 8, 6, 1, 11, 2, 8, 10, 1, 2, 6, 2, 1, 2, 2, 7, 1, 3, 2, 2, 8, 2, 5, 2, 0, 10, 6, 0, 5, 2, 6, 6, 0, 6, 9, 5, 5, 11, 2, 2, 2, 2, 2, 1, 6, 2, 4, 6, 1, 2, 11, 2, 7, 2, 5, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 8, 2, 2, 1, 1, 8, 2, 7, 2, 6, 5, 4, 2, 11, 2, 2, 11, 1, 2, 6, 2, 2, 8, 2, 1, 2, 0, 2, 11, 2, 2, 10, 4, 2, 1, 2, 1, 5, 2, 1, 2, 2, 7, 2, 5, 2, 6, 2, 10, 2, 8, 11, 1, 1, 2, 2, 1, 2, 1, 6, 2, 2, 10, 7, 2, 2, 10, 2, 10, 2, 2, 10, 1, 5, 6, 2, 10, 6, 2, 6, 5, 2, 10, 2, 1, 7, 5, 4, 2, 6, 2, 8, 10, 11, 2, 2, 5, 2, 2, 1, 1, 2, 2, 2, 2, 2, 7, 4, 2, 2, 2, 2, 8, 1, 2, 4, 0, 8, 2, 2, 8, 7, 4, 2, 2, 6, 8, 2, 0, 8, 1, 6, 6, 2, 2, 10, 2, 2, 10, 2, 4, 11, 7, 8, 2, 2, 7, 5, 5, 6, 5, 1, 2, 5, 10, 2, 4, 1, 2, 2, 11, 6, 0, 1]\n",
        "# [2, 2, 2, 2, 2, 2, 4, 1, 1, 2, 2, 2, 2, 2, 7, 6, 2, 2, 2, 2, 2, 6, 2, 2, 5, 2, 2, 0, 2, 3, 4, 2, 2, 10, 1, 10, 2, 0, 2, 2, 2, 2, 1, 10, 11, 5, 2, 1, 0, 2, 2, 1, 2, 0, 2, 5, 2, 2, 2, 2, 2, 2, 11, 2, 8, 2, 6, 2, 2, 2, 0, 2, 0, 11, 2, 2, 2, 2, 10, 4, 8, 2, 2, 2, 2, 0, 0, 0, 11, 2, 0, 1, 2, 2, 2, 5, 7, 0, 2, 2, 2, 0, 0, 9, 4, 2, 2, 0, 7, 2, 0, 8, 0, 8, 2, 2, 2, 2, 2, 2, 11, 11, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 5, 11, 11, 1, 2, 2, 1, 0, 10, 11, 2, 2, 9, 2, 10, 2, 7, 2, 2, 11, 4, 6, 2, 1, 7, 2, 2, 2, 8, 2, 5, 0, 10, 0, 11, 7, 10, 2, 3, 1, 2, 1, 8, 6, 2, 0, 2, 2, 2, 6, 2, 2, 2, 7, 0, 7, 2, 11, 0, 1, 2, 2, 7, 0, 11, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 7, 2, 2, 11, 2, 2, 11, 2, 2, 1, 1, 2, 4, 2, 2, 2, 0, 11, 2, 2, 5, 2, 2, 2, 2, 11, 2, 8, 2, 1, 0, 2, 9, 1, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 4, 1, 1, 2, 6, 2, 2, 2, 2, 0, 9, 2, 2, 2, 2, 0, 1, 2, 2, 11, 0, 2, 6, 2, 2, 2, 7, 2, 11, 4, 2, 2, 0, 3, 11, 2, 3, 2, 0, 0, 2, 1, 0, 2, 11, 2, 2, 1, 2, 5, 5, 2, 2, 2, 2, 2, 2, 2, 0, 11, 11, 6, 2, 2, 10, 0, 2, 1, 2, 2, 7, 11, 2, 9, 1, 0, 8, 2, 2, 1, 2, 2, 0, 2, 2, 5, 0, 2, 5, 1, 6, 2, 4, 2, 0, 2, 2, 2, 2, 11, 0, 2, 2, 2, 2, 10, 0, 2, 8, 2, 4, 7, 2, 2, 0, 2, 2, 1, 2, 2, 5, 2, 2, 7, 0, 2, 0, 1, 11, 11, 2, 6, 2, 2, 0, 8, 2, 0, 1, 4, 1, 2, 0, 0, 2, 11, 2, 2, 0, 1, 2, 11, 2, 2, 2, 0, 2, 2, 2, 11, 4, 2, 2, 0, 0, 8, 2, 11, 0, 7, 4, 2, 1, 1, 10, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 7, 5, 2, 5, 5, 5, 10, 2, 2, 2, 2, 0, 2, 6, 2, 4, 1, 2, 2, 10, 6, 2, 2, 1, 2, 0, 11, 2, 7, 0, 6, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 4, 11, 1, 0, 11, 2, 1, 8, 0, 2, 0, 5, 2, 1, 2, 2, 2, 7, 10, 2, 0]\n",
        "# [2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 0, 2, 2, 2, 4, 4, 2, 0, 4, 2, 10, 2, 2, 6, 4, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 0, 2, 1, 8, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 6, 2, 2, 7, 2, 7, 2, 0, 2, 2, 0, 1, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 2, 2, 2, 9, 2, 2, 11, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 7, 1, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 2, 1, 2, 7, 2, 4, 11, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 10, 7, 1, 7, 0, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 0, 2, 2, 2, 2, 3, 2, 8, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 8, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 10, 2, 0, 2, 2, 2, 2, 9, 2, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 11, 2, 2, 2, 2, 0, 2, 11, 4, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 0, 0, 5, 2, 2, 4, 5, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 2, 2, 2, 2, 2, 7, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
        "# [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 5, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 10, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 7, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 7, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 11, 2, 2, 5, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 9, 2, 2, 2, 1, 2, 11, 2, 2, 2, 3, 2, 2, 2, 2, 10, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 11, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 2, 10, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 10, 2, 2, 2, 2]\n",
        "acts=[5, 2, 6, 3, 2, 3, 2, 2, 2, 2, 2, 5, 3, 2, 2, 1, 2, 2, 2, 11, 11, 2, 11, 2, 2, 2, 2, 2, 5, 11, 7, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 3, 2, 1, 2, 2, 2, 3, 2, 2, 4, 2, 2, 2, 6, 2, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 3, 2, 10, 5, 2, 7, 2, 2, 2, 2, 2, 2, 2, 3, 2, 11, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 10, 2, 2, 2, 2, 2, 2, 1, 3, 2, 3, 2, 2, 2, 2, 2, 2, 8, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 8, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 4, 3, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 10, 2, 2, 2, 2, 5, 2, 2, 10, 3, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 2, 2, 7, 3, 2, 2, 2, 2, 2, 9, 2, 2, 10, 2, 2, 3, 2, 2, 0, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 7, 2, 2, 2, 7, 2, 2, 5, 2, 2, 2, 5, 2, 7, 2, 2, 2, 2, 2, 3, 2, 2, 0, 10, 8, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 3, 2, 0, 7, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 8, 7, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 7, 2, 2, 2, 2, 11, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 1, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 7, 2, 5, 2, 3, 2, 2, 2, 2, 5, 2, 2, 1, 10, 11, 2, 7, 2, 2, 4, 2, 2, 2, 2, 2, 7, 11, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 7, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 11, 2, 7, 2, 1, 2, 5, 6, 2, 6, 2, 2, 2, 2, 0, 2, 2, 5, 2, 0, 1, 2, 2, 2, 2, 2, 4, 2, 11, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 5, 6, 2, 2, 2, 5, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 2, 3, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 10, 2, 7, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 10, 2, 2, 2, 2, 3, 1, 2, 2, 2, 10, 2, 6, 8, 2, 2, 3, 2, 2, 2, 2, 1, 2, 8, 2, 3, 2, 1, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 6, 2, 7, 3, 7, 2, 2, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 7, 2, 2, 7, 7, 2, 2, 2, 5, 2, 10, 2, 1, 2, 2, 2, 9, 2, 2, 2, 2, 3, 2, 10, 0, 2, 6, 3, 3, 2, 3, 2, 2, 10, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 6, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 3, 2, 1, 2, 1, 11, 6, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 7, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 5, 5, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 0, 7, 2, 7, 2, 2, 2, 2, 6, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 7, 2, 2, 2, 7, 1, 2, 2, 2, 2, 2, 0, 10, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 0, 1, 7, 2, 7, 11, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 7, 2, 2, 2, 3, 2, 2, 2, 2, 2, 7, 2, 2, 7, 3, 3, 3, 2, 2, 2, 2, 2, 7, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 10, 3, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 7, 2, 1]\n",
        "\n",
        "\n",
        "while True:\n",
        "    # state = torch.from_numpy(state.copy()).type(torch.float)\n",
        "    # value, logit, (hx, cx) = model((state.unsqueeze(0), (hx, cx)), icm = False)\n",
        "    # prob = F.softmax(logit, dim=1) #from train\n",
        "    # action = prob.multinomial(1).data\n",
        "    # state, reward, done, _ = env.step(action.item())\n",
        "    try:\n",
        "        action=int(acts[x])\n",
        "    except:\n",
        "        action = 10\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "    x+=1\n",
        "    if done: break\n",
        "env.play()\n",
        "print(x)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-qQjuJIepAvA",
        "SPCCve3p2bL-",
        "sbpPda4YEv13",
        "GCHpcDteZdLS",
        "fj3tv7XHZmD9",
        "wFrRKvOwhYM_",
        "KlV2MvSK-aL_"
      ],
      "name": "curiousity_lstm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
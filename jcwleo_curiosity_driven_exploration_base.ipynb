{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jcwleo-curiosity-driven-exploration-base.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMth3cK4b0hEusCKBgq/Ppe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/jcwleo_curiosity_driven_exploration_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "ezjFR5N1HT94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://github.com/jcwleo/curiosity-driven-exploration-pytorch\n",
        "!pip install gym_super_mario_bros nes_py\n",
        "!pip install tensorboardX\n",
        "# https://stackoverflow.com/questions/67808779/running-gym-atari-in-google-colab\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]\n"
      ],
      "metadata": {
        "id": "tWNUebWtGx4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### config"
      ],
      "metadata": {
        "id": "LzppoJah6gBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "# # https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/config.py\n",
        "# env_type = \"atari\"\n",
        "env_id = \"BreakoutNoFrameskip-v4\"\n",
        "max_step_per_episode = 4500\n",
        "learning_rate = 1e-4\n",
        "num_worker = 16\n",
        "num_step = 128\n",
        "gamma = 0.99\n",
        "lam = 0.95\n",
        "use_gae = True\n",
        "use_cuda = True\n",
        "use_noisy_net = False\n",
        "clip_grad_norm = 0.5\n",
        "entropy_coef = 0.001\n",
        "epoch=Epoch = 3\n",
        "mini_batch = 8\n",
        "ppo_eps = 0.1\n",
        "life_done = False\n",
        "pre_obs_norm_step = 10000\n",
        "eta=ETA = 1.\n"
      ],
      "metadata": {
        "id": "569WzGoSmbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### utils"
      ],
      "metadata": {
        "id": "advj-ZV96ef8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/utils.py\n",
        "import numpy as np\n",
        "\n",
        "def make_train_data(reward, done, value, gamma, num_step, num_worker):\n",
        "    discounted_return = np.empty([num_worker, num_step])\n",
        "    # Discounted Return\n",
        "    if use_gae:\n",
        "        gae = np.zeros_like([num_worker, ])\n",
        "        for t in range(num_step - 1, -1, -1):\n",
        "            delta = reward[:, t] + gamma * value[:, t + 1] * (1 - done[:, t]) - value[:, t]\n",
        "            gae = delta + gamma * lam * (1 - done[:, t]) * gae\n",
        "            discounted_return[:, t] = gae + value[:, t]\n",
        "            # For Actor\n",
        "        adv = discounted_return - value[:, :-1]\n",
        "\n",
        "    else:\n",
        "        running_add = value[:, -1]\n",
        "        for t in range(num_step - 1, -1, -1):\n",
        "            running_add = reward[:, t] + gamma * running_add * (1 - done[:, t])\n",
        "            discounted_return[:, t] = running_add\n",
        "        # For Actor\n",
        "        adv = discounted_return - value[:, :-1]\n",
        "    return discounted_return.reshape([-1]), adv.reshape([-1])\n",
        "\n",
        "\n",
        "class RunningMeanStd(object):\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * (self.count)\n",
        "        m_b = batch_var * (batch_count)\n",
        "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
        "        new_var = M2 / (self.count + batch_count)\n",
        "        new_count = batch_count + self.count\n",
        "        self.mean = new_mean\n",
        "        self.var = new_var\n",
        "        self.count = new_count\n",
        "\n",
        "\n",
        "class RewardForwardFilter(object):\n",
        "    def __init__(self, gamma):\n",
        "        self.rewems = None\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def update(self, rews):\n",
        "        if self.rewems is None:\n",
        "            self.rewems = rews\n",
        "        else:\n",
        "            self.rewems = self.rewems * self.gamma + rews\n",
        "        return self.rewems\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis]  # dito\n",
        "    return e_x / div\n"
      ],
      "metadata": {
        "id": "jJR3qCEkmhaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model"
      ],
      "metadata": {
        "id": "04hfoQXu6hUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/model.py\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.nn import init\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "    \"\"\"Factorised Gaussian NoisyNet\"\"\"\n",
        "    def __init__(self, in_features, out_features, sigma0=0.5):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.noisy_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.noisy_bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.noise_std = sigma0 / math.sqrt(self.in_features)\n",
        "        self.reset_parameters()\n",
        "        self.register_noise()\n",
        "\n",
        "    def register_noise(self):\n",
        "        in_noise = torch.FloatTensor(self.in_features)\n",
        "        out_noise = torch.FloatTensor(self.out_features)\n",
        "        noise = torch.FloatTensor(self.out_features, self.in_features)\n",
        "        self.register_buffer('in_noise', in_noise)\n",
        "        self.register_buffer('out_noise', out_noise)\n",
        "        self.register_buffer('noise', noise)\n",
        "\n",
        "    def sample_noise(self):\n",
        "        self.in_noise.normal_(0, self.noise_std)\n",
        "        self.out_noise.normal_(0, self.noise_std)\n",
        "        self.noise = torch.mm(\n",
        "            self.out_noise.view(-1, 1), self.in_noise.view(1, -1))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.noisy_weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "            self.noisy_bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Note: noise will be updated if x is not volatile\"\"\"\n",
        "        normal_y = nn.functional.linear(x, self.weight, self.bias)\n",
        "        if self.training:\n",
        "            # update the noise once per update\n",
        "            self.sample_noise()\n",
        "        noisy_weight = self.noisy_weight * self.noise\n",
        "        noisy_bias = self.noisy_bias * self.out_noise\n",
        "        noisy_y = nn.functional.linear(x, noisy_weight, noisy_bias)\n",
        "        return noisy_y + normal_y\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'in_features=' + str(self.in_features) \\\n",
        "               + ', out_features=' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class CnnActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size, use_noisy_net=False):\n",
        "        super(CnnActorCriticNetwork, self).__init__()\n",
        "        if use_noisy_net:\n",
        "            print('use NoisyNet')\n",
        "            linear = NoisyLinear\n",
        "        else:\n",
        "            linear = nn.Linear\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            linear(7 * 7 * 64, 512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            linear(512, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            linear(512, output_size)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            linear(512, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            linear(512, 1)\n",
        "        )\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "        for i in range(len(self.actor)):\n",
        "            if type(self.actor[i]) == nn.Linear:\n",
        "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
        "                self.actor[i].bias.data.zero_()\n",
        "        for i in range(len(self.critic)):\n",
        "            if type(self.critic[i]) == nn.Linear:\n",
        "                init.orthogonal_(self.critic[i].weight, 0.01)\n",
        "                self.critic[i].bias.data.zero_()\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.feature(state)\n",
        "        policy = self.actor(x)\n",
        "        value = self.critic(x)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "class ICMModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, use_cuda=True):\n",
        "        super(ICMModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "        feature_output = 7 * 7 * 64\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            nn.Linear(feature_output, 512)\n",
        "        )\n",
        "\n",
        "        self.inverse_net = nn.Sequential(\n",
        "            nn.Linear(512 * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_size)\n",
        "        )\n",
        "        self.residual = [nn.Sequential(\n",
        "            nn.Linear(output_size + 512, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "        ).to(self.device)] * 8\n",
        "        self.forward_net_1 = nn.Sequential(\n",
        "            nn.Linear(output_size + 512, 512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.forward_net_2 = nn.Sequential(\n",
        "            nn.Linear(output_size + 512, 512),\n",
        "        )\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.kaiming_uniform_(p.weight)\n",
        "                p.bias.data.zero_()\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.kaiming_uniform_(p.weight, a=1.0)\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        state, next_state, action = inputs\n",
        "        encode_state = self.feature(state)\n",
        "        encode_next_state = self.feature(next_state)\n",
        "        # get pred action\n",
        "        pred_action = torch.cat((encode_state, encode_next_state), 1)\n",
        "        pred_action = self.inverse_net(pred_action)\n",
        "        # ---------------------\n",
        "        # get pred next state\n",
        "        pred_next_state_feature_orig = torch.cat((encode_state, action), 1)\n",
        "        pred_next_state_feature_orig = self.forward_net_1(pred_next_state_feature_orig)\n",
        "        # residual\n",
        "        for i in range(4):\n",
        "            pred_next_state_feature = self.residual[i * 2](torch.cat((pred_next_state_feature_orig, action), 1))\n",
        "            pred_next_state_feature_orig = self.residual[i * 2 + 1](\n",
        "                torch.cat((pred_next_state_feature, action), 1)) + pred_next_state_feature_orig\n",
        "        pred_next_state_feature = self.forward_net_2(torch.cat((pred_next_state_feature_orig, action), 1))\n",
        "        real_next_state_feature = encode_next_state\n",
        "        return real_next_state_feature, pred_next_state_feature, pred_action\n",
        "\n"
      ],
      "metadata": {
        "id": "dOYA7QP3n6sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### agents"
      ],
      "metadata": {
        "id": "skp0AbrR6g3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# agents\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/agents.py\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "class ICMAgent(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            output_size,\n",
        "            num_env,\n",
        "            num_step,\n",
        "            gamma,\n",
        "            lam=0.95,\n",
        "            learning_rate=1e-4,\n",
        "            ent_coef=0.01,\n",
        "            clip_grad_norm=0.5,\n",
        "            epoch=3,\n",
        "            batch_size=128,\n",
        "            ppo_eps=0.1,\n",
        "            eta=0.01,\n",
        "            use_gae=True,\n",
        "            use_cuda=False,\n",
        "            use_noisy_net=False):\n",
        "        self.model = CnnActorCriticNetwork(input_size, output_size, use_noisy_net)\n",
        "        self.num_env = num_env\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size\n",
        "        self.num_step = num_step\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gae = use_gae\n",
        "        self.ent_coef = ent_coef\n",
        "        self.eta = eta\n",
        "        self.ppo_eps = ppo_eps\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "        self.icm = ICMModel(input_size, output_size, use_cuda)\n",
        "        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.icm.parameters()), lr=learning_rate)\n",
        "        self.icm = self.icm.to(self.device)\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.Tensor(state).to(self.device)\n",
        "        state = state.float()\n",
        "        policy, value = self.model(state)\n",
        "        action_prob = F.softmax(policy, dim=-1).data.cpu().numpy()\n",
        "        action = self.random_choice_prob_index(action_prob)\n",
        "        return action, value.data.cpu().numpy().squeeze(), policy.detach()\n",
        "\n",
        "    @staticmethod\n",
        "    def random_choice_prob_index(p, axis=1):\n",
        "        r = np.expand_dims(np.random.rand(p.shape[1 - axis]), axis=axis)\n",
        "        return (p.cumsum(axis=axis) > r).argmax(axis=axis)\n",
        "\n",
        "    def compute_intrinsic_reward(self, state, next_state, action):\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        action = torch.LongTensor(action).to(self.device)\n",
        "        action_onehot = torch.FloatTensor(len(action), self.output_size).to(self.device)\n",
        "        action_onehot.zero_()\n",
        "        action_onehot.scatter_(1, action.view(len(action), -1), 1)\n",
        "        real_next_state_feature, pred_next_state_feature, pred_action = self.icm([state, next_state, action_onehot])\n",
        "        intrinsic_reward = self.eta * F.mse_loss(real_next_state_feature, pred_next_state_feature, reduction='none').mean(-1)\n",
        "        return intrinsic_reward.data.cpu().numpy()\n",
        "\n",
        "    def train_model(self, s_batch, next_s_batch, target_batch, y_batch, adv_batch, old_policy):\n",
        "        s_batch = torch.FloatTensor(s_batch).to(self.device)\n",
        "        next_s_batch = torch.FloatTensor(next_s_batch).to(self.device)\n",
        "        target_batch = torch.FloatTensor(target_batch).to(self.device)\n",
        "        y_batch = torch.LongTensor(y_batch).to(self.device)\n",
        "        adv_batch = torch.FloatTensor(adv_batch).to(self.device)\n",
        "        sample_range = np.arange(len(s_batch))\n",
        "        ce = nn.CrossEntropyLoss()\n",
        "        forward_mse = nn.MSELoss()\n",
        "        with torch.no_grad():\n",
        "            policy_old_list = torch.stack(old_policy).permute(1, 0, 2).contiguous().view(-1, self.output_size).to(self.device)\n",
        "            m_old = Categorical(F.softmax(policy_old_list, dim=-1))\n",
        "            log_prob_old = m_old.log_prob(y_batch)\n",
        "            # ------------------------------------------------------------\n",
        "        for i in range(self.epoch):\n",
        "            print(\"agent train i \",i)\n",
        "            np.random.shuffle(sample_range)\n",
        "            for j in range(int(len(s_batch) / self.batch_size)):\n",
        "                sample_idx = sample_range[self.batch_size * j:self.batch_size * (j + 1)]\n",
        "                # --------------------------------------------------------------------------------\n",
        "                # for Curiosity-driven\n",
        "                action_onehot = torch.FloatTensor(self.batch_size, self.output_size).to(self.device)\n",
        "                action_onehot.zero_()\n",
        "                action_onehot.scatter_(1, y_batch[sample_idx].view(-1, 1), 1)\n",
        "                real_next_state_feature, pred_next_state_feature, pred_action = self.icm(\n",
        "                    [s_batch[sample_idx], next_s_batch[sample_idx], action_onehot])\n",
        "                inverse_loss = ce(pred_action, y_batch[sample_idx])\n",
        "                forward_loss = forward_mse(pred_next_state_feature, real_next_state_feature.detach())\n",
        "                # ---------------------------------------------------------------------------------\n",
        "                policy, value = self.model(s_batch[sample_idx])\n",
        "                m = Categorical(F.softmax(policy, dim=-1))\n",
        "                log_prob = m.log_prob(y_batch[sample_idx])\n",
        "                ratio = torch.exp(log_prob - log_prob_old[sample_idx])\n",
        "                surr1 = ratio * adv_batch[sample_idx]\n",
        "                surr2 = torch.clamp(\n",
        "                    ratio,\n",
        "                    1.0 - self.ppo_eps,\n",
        "                    1.0 + self.ppo_eps) * adv_batch[sample_idx]\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_loss = F.mse_loss(value.sum(1), target_batch[sample_idx])\n",
        "                entropy = m.entropy().mean()\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = (actor_loss + 0.5 * critic_loss - 0.001 * entropy) + forward_loss + inverse_loss\n",
        "                print(\"agent train loss \",loss)\n",
        "                loss.backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaYJferjmQNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### envs"
      ],
      "metadata": {
        "id": "_6iGwOSX6h0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# envs\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/envs.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "from abc import abstractmethod\n",
        "from collections import deque\n",
        "from copy import copy\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "from torch.multiprocessing import Pipe, Process\n",
        "from PIL import Image\n",
        "\n",
        "class Environment(Process):\n",
        "    @abstractmethod\n",
        "    def run(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def pre_proc(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_init_state(self, x):\n",
        "        pass\n",
        "\n",
        "\n",
        "def unwrap(env):\n",
        "    if hasattr(env, \"unwrapped\"):\n",
        "        return env.unwrapped\n",
        "    elif hasattr(env, \"env\"):\n",
        "        return unwrap(env.env)\n",
        "    elif hasattr(env, \"leg_env\"):\n",
        "        return unwrap(env.leg_env)\n",
        "    else:\n",
        "        return env\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            # noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, is_render, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "        self.is_render = is_render\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if self.is_render:\n",
        "                self.env.render()\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class MontezumaInfoWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, room_address):\n",
        "        super(MontezumaInfoWrapper, self).__init__(env)\n",
        "        self.room_address = room_address\n",
        "        self.visited_rooms = set()\n",
        "\n",
        "    def get_current_room(self):\n",
        "        ram = unwrap(self.env).ale.getRAM()\n",
        "        assert len(ram) == 128\n",
        "        return int(ram[self.room_address])\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.visited_rooms.add(self.get_current_room())\n",
        "        if done:\n",
        "            if 'episode' not in info:\n",
        "                info['episode'] = {}\n",
        "            info['episode'].update(visited_rooms=copy(self.visited_rooms))\n",
        "            self.visited_rooms.clear()\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "\n",
        "class AtariEnvironment(Environment):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env_id,\n",
        "            is_render,\n",
        "            env_idx,\n",
        "            child_conn,\n",
        "            history_size=4,\n",
        "            h=84,\n",
        "            w=84,\n",
        "            life_done=True):\n",
        "        super(AtariEnvironment, self).__init__()\n",
        "        self.daemon = True\n",
        "        self.env = MaxAndSkipEnv(NoopResetEnv(gym.make(env_id)), is_render)\n",
        "        if 'Montezuma' in env_id:\n",
        "            self.env = MontezumaInfoWrapper(self.env, room_address=3 if 'Montezuma' in env_id else 1)\n",
        "        self.env_id = env_id\n",
        "        self.is_render = is_render\n",
        "        self.env_idx = env_idx\n",
        "        self.steps = 0\n",
        "        self.episode = 0\n",
        "        self.rall = 0\n",
        "        self.recent_rlist = deque(maxlen=100)\n",
        "        self.child_conn = child_conn\n",
        "        self.history_size = history_size\n",
        "        self.history = np.zeros([history_size, h, w])\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.reset()\n",
        "\n",
        "    def run(self):\n",
        "        super(AtariEnvironment, self).run()\n",
        "        while True:\n",
        "            action = self.child_conn.recv()\n",
        "            if 'Breakout' in self.env_id:\n",
        "                action += 1\n",
        "            s, reward, done, info = self.env.step(action)\n",
        "            if max_step_per_episode < self.steps:\n",
        "                done = True\n",
        "            log_reward = reward\n",
        "            force_done = done\n",
        "            self.history[:3, :, :] = self.history[1:, :, :]\n",
        "            self.history[3, :, :] = self.pre_proc(s)\n",
        "            self.rall += reward\n",
        "            self.steps += 1\n",
        "            if done:\n",
        "                self.recent_rlist.append(self.rall)\n",
        "                print(\"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Visited Room: [{}]\".format(\n",
        "                    self.episode, self.env_idx, self.steps, self.rall, np.mean(self.recent_rlist),\n",
        "                    info.get('episode', {}).get('visited_rooms', {})))\n",
        "                self.history = self.reset()\n",
        "            self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_action = 0\n",
        "        self.steps = 0\n",
        "        self.episode += 1\n",
        "        self.rall = 0\n",
        "        s = self.env.reset()\n",
        "        self.get_init_state(\n",
        "            self.pre_proc(s))\n",
        "        return self.history[:, :, :]\n",
        "\n",
        "    def pre_proc(self, X):\n",
        "        X = np.array(Image.fromarray(X).convert('L')).astype('float32')\n",
        "        x = cv2.resize(X, (self.h, self.w))\n",
        "        return x\n",
        "\n",
        "    def get_init_state(self, s):\n",
        "        for i in range(self.history_size):\n",
        "            self.history[i, :, :] = self.pre_proc(s)\n",
        "\n",
        "\n",
        "class MarioEnvironment(Process):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env_id,\n",
        "            is_render,\n",
        "            env_idx,\n",
        "            child_conn,\n",
        "            history_size=4,\n",
        "            life_done=True,\n",
        "            h=84,\n",
        "            w=84, movement=COMPLEX_MOVEMENT, sticky_action=True,\n",
        "            p=0.25):\n",
        "        super(MarioEnvironment, self).__init__()\n",
        "        self.daemon = True\n",
        "        self.env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "        self.is_render = is_render\n",
        "        self.env_idx = env_idx\n",
        "        self.steps = 0\n",
        "        self.episode = 0\n",
        "        self.rall = 0\n",
        "        self.recent_rlist = deque(maxlen=100)\n",
        "        self.child_conn = child_conn\n",
        "        self.life_done = life_done\n",
        "        self.history_size = history_size\n",
        "        self.history = np.zeros([history_size, h, w])\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.reset()\n",
        "\n",
        "    def run(self):\n",
        "        super(MarioEnvironment, self).run()\n",
        "        while True:\n",
        "            action = self.child_conn.recv()\n",
        "            if self.is_render:\n",
        "                self.env.render()\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            # when Mario loses life, changes the state to the terminal state.\n",
        "            if self.life_done:\n",
        "                if self.lives > info['life'] and info['life'] > 0:\n",
        "                    force_done = True\n",
        "                    self.lives = info['life']\n",
        "                else:\n",
        "                    force_done = done\n",
        "                    self.lives = info['life']\n",
        "            else:\n",
        "                force_done = done\n",
        "            # reward range -15 ~ 15\n",
        "            log_reward = reward / 15\n",
        "            self.rall += log_reward\n",
        "            r = log_reward\n",
        "            self.history[:3, :, :] = self.history[1:, :, :]\n",
        "            self.history[3, :, :] = self.pre_proc(obs)\n",
        "            self.steps += 1\n",
        "            if done:\n",
        "                self.recent_rlist.append(self.rall)\n",
        "                print(\"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Stage: {} current x:{}   max x:{}\".format(self.episode, self.env_idx, self.steps, self.rall, np.mean(self.recent_rlist), info['stage'], info['x_pos'], self.max_pos))\n",
        "                self.history = self.reset()\n",
        "            self.child_conn.send([self.history[:, :, :], r, force_done, done, log_reward])\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_action = 0\n",
        "        self.steps = 0\n",
        "        self.episode += 1\n",
        "        self.rall = 0\n",
        "        self.lives = 3\n",
        "        self.stage = 1\n",
        "        self.max_pos = 0\n",
        "        self.get_init_state(self.env.reset())\n",
        "        return self.history[:, :, :]\n",
        "\n",
        "    def pre_proc(self, X):\n",
        "        x = cv2.cvtColor(X, cv2.COLOR_RGB2GRAY) # grayscaling\n",
        "        x = cv2.resize(x, (self.h, self.w)) # resize\n",
        "        return x\n",
        "\n",
        "    def get_init_state(self, s):\n",
        "        for i in range(self.history_size):\n",
        "            self.history[i, :, :] = self.pre_proc(s)\n",
        "\n"
      ],
      "metadata": {
        "id": "u6A7eQqUmfmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/train.py\n",
        "from torch.multiprocessing import Pipe\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "# def main():\n",
        "if env_type == 'mario':\n",
        "    env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "elif env_type == 'atari':\n",
        "    env = gym.make(env_id)\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "input_size = env.observation_space.shape  # 4\n",
        "output_size = env.action_space.n  # 2\n",
        "\n",
        "if 'Breakout' in env_id:\n",
        "    output_size -= 1\n",
        "\n",
        "env.close()\n",
        "\n",
        "is_load_model = False\n",
        "is_render = False\n",
        "# model_path = 'models/{}.model'.format(env_id)\n",
        "# icm_path = 'models/{}.icm'.format(env_id)\n",
        "model_path = '{}.model'.format(env_id)\n",
        "icm_path = '{}.icm'.format(env_id)\n",
        "writer = SummaryWriter()\n",
        "batch_size = int(num_step * num_worker / mini_batch)\n",
        "reward_rms = RunningMeanStd()\n",
        "obs_rms = RunningMeanStd(shape=(1, 4, 84, 84))\n",
        "# pre_obs_norm_step = int(default_config['ObsNormStep'])\n",
        "discounted_reward = RewardForwardFilter(gamma)\n",
        "agent = ICMAgent\n",
        "\n",
        "if env_type == 'atari':\n",
        "    env_t = AtariEnvironment\n",
        "elif env_type == 'mario':\n",
        "    env_t = MarioEnvironment\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "agent = agent(\n",
        "    input_size,\n",
        "    output_size,\n",
        "    num_worker,\n",
        "    num_step,\n",
        "    gamma,\n",
        "    lam=lam,\n",
        "    learning_rate=learning_rate,\n",
        "    ent_coef=entropy_coef,\n",
        "    clip_grad_norm=clip_grad_norm,\n",
        "    epoch=epoch,\n",
        "    batch_size=batch_size,\n",
        "    ppo_eps=ppo_eps,\n",
        "    eta=eta,\n",
        "    use_cuda=use_cuda,\n",
        "    use_gae=use_gae,\n",
        "    use_noisy_net=use_noisy_net\n",
        ")\n",
        "\n",
        "if is_load_model:\n",
        "    if use_cuda:\n",
        "        agent.model.load_state_dict(torch.load(model_path))\n",
        "    else:\n",
        "        agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "\n",
        "works = []\n",
        "parent_conns = []\n",
        "child_conns = []\n",
        "for idx in range(num_worker):\n",
        "    parent_conn, child_conn = Pipe()\n",
        "    work = env_t(env_id, is_render, idx, child_conn)\n",
        "    work.start()\n",
        "    works.append(work)\n",
        "    parent_conns.append(parent_conn)\n",
        "    child_conns.append(child_conn)\n",
        "states = np.zeros([num_worker, 4, 84, 84])\n",
        "\n",
        "sample_episode = 0\n",
        "sample_rall = 0\n",
        "sample_step = 0\n",
        "sample_env_idx = 0\n",
        "sample_i_rall = 0\n",
        "global_update = 0\n",
        "global_step = 0\n",
        "\n",
        "# normalize obs\n",
        "print('Start to initailize observation normalization parameter.....')\n",
        "next_obs = []\n",
        "steps = 0\n",
        "while steps < pre_obs_norm_step:\n",
        "    steps += num_worker\n",
        "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
        "    for parent_conn, action in zip(parent_conns, actions):\n",
        "        parent_conn.send(action)\n",
        "    for parent_conn in parent_conns:\n",
        "        s, r, d, rd, lr = parent_conn.recv()\n",
        "        next_obs.append(s[:])\n",
        "next_obs = np.stack(next_obs)\n",
        "obs_rms.update(next_obs)\n",
        "print('End to initalize...')\n",
        "\n",
        "while True:\n",
        "    total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_values, total_policy = \\\n",
        "        [], [], [], [], [], [], [], [], []\n",
        "    global_step += (num_worker * num_step)\n",
        "    global_update += 1\n",
        "    # Step 1. n-step rollout\n",
        "    for _ in range(num_step):\n",
        "        actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
        "        for parent_conn, action in zip(parent_conns, actions):\n",
        "            parent_conn.send(action)\n",
        "        next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
        "        for parent_conn in parent_conns:\n",
        "            s, r, d, rd, lr = parent_conn.recv()\n",
        "            next_states.append(s)\n",
        "            rewards.append(r)\n",
        "            dones.append(d)\n",
        "            real_dones.append(rd)\n",
        "            log_rewards.append(lr)\n",
        "        next_states = np.stack(next_states)\n",
        "        rewards = np.hstack(rewards)\n",
        "        dones = np.hstack(dones)\n",
        "        real_dones = np.hstack(real_dones)\n",
        "        # total reward = int reward\n",
        "        intrinsic_reward = agent.compute_intrinsic_reward(\n",
        "            (states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "            (next_states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "            actions)\n",
        "        sample_i_rall += intrinsic_reward[sample_env_idx]\n",
        "        total_int_reward.append(intrinsic_reward)\n",
        "        total_state.append(states)\n",
        "        total_next_state.append(next_states)\n",
        "        total_reward.append(rewards)\n",
        "        total_done.append(dones)\n",
        "        total_action.append(actions)\n",
        "        total_values.append(value)\n",
        "        total_policy.append(policy)\n",
        "        states = next_states[:, :, :, :]\n",
        "        sample_rall += log_rewards[sample_env_idx]\n",
        "\n",
        "        sample_step += 1\n",
        "        if real_dones[sample_env_idx]:\n",
        "            sample_episode += 1\n",
        "            writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
        "            writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
        "            writer.add_scalar('data/step', sample_step, sample_episode)\n",
        "            sample_rall = 0\n",
        "            sample_step = 0\n",
        "            sample_i_rall = 0\n",
        "\n",
        "    # calculate last next value\n",
        "    _, value, _ = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
        "    total_values.append(value)\n",
        "    # --------------------------------------------------\n",
        "    total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "    total_next_state = np.stack(total_next_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "    total_action = np.stack(total_action).transpose().reshape([-1])\n",
        "    total_done = np.stack(total_done).transpose()\n",
        "    total_values = np.stack(total_values).transpose()\n",
        "    total_logging_policy = torch.stack(total_policy).view(-1, output_size).cpu().numpy()\n",
        "\n",
        "    # Step 2. calculate intrinsic reward\n",
        "    # running mean intrinsic reward\n",
        "    total_int_reward = np.stack(total_int_reward).transpose()\n",
        "    total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
        "                                        total_int_reward.T])\n",
        "    mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
        "    reward_rms.update_from_moments(mean, std ** 2, count)\n",
        "\n",
        "    # normalize intrinsic reward\n",
        "    total_int_reward /= np.sqrt(reward_rms.var)\n",
        "    writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
        "    writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
        "    # -------------------------------------------------------------------------------------------\n",
        "    # logging Max action probability\n",
        "    writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
        "\n",
        "    # Step 3. make target and advantage\n",
        "    target, adv = make_train_data(total_int_reward,\n",
        "                                    np.zeros_like(total_int_reward),\n",
        "                                    total_values,\n",
        "                                    gamma,\n",
        "                                    num_step,\n",
        "                                    num_worker)\n",
        "\n",
        "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Step 5. Training!\n",
        "    agent.train_model((total_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "                        (total_next_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "                        target, total_action,\n",
        "                        adv,\n",
        "                        total_policy)\n",
        "\n",
        "    if global_step % (num_worker * num_step * 100) == 0:\n",
        "        print('Now Global Step :{}'.format(global_step))\n",
        "        torch.save(agent.model.state_dict(), model_path)\n",
        "        torch.save(agent.icm.state_dict(), icm_path)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_0RT0iLwk2kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train"
      ],
      "metadata": {
        "id": "FD0PkqWp6iPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/train.py\n",
        "from torch.multiprocessing import Pipe\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "env = gym.make(env_id)\n",
        "env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "input_size = env.observation_space.shape  # 4\n",
        "output_size = env.action_space.n  # 2\n",
        "if 'Breakout' in env_id: output_size -= 1\n",
        "env.close()\n",
        "\n",
        "is_load_model = False\n",
        "is_render = False\n",
        "model_path = 'models/{}.model'.format(env_id)\n",
        "icm_path = 'models/{}.icm'.format(env_id)\n",
        "writer = SummaryWriter()\n",
        "batch_size = int(num_step * num_worker / mini_batch)\n",
        "reward_rms = RunningMeanStd()\n",
        "obs_rms = RunningMeanStd(shape=(1, 4, 84, 84))\n",
        "discounted_reward = RewardForwardFilter(gamma)\n",
        "agent = ICMAgent\n",
        "\n",
        "env = AtariEnvironment\n",
        "# env = MarioEnvironment\n",
        "\n",
        "agent = agent(\n",
        "    input_size,\n",
        "    output_size,\n",
        "    num_worker,\n",
        "    num_step,\n",
        "    gamma,\n",
        "    lam=lam,\n",
        "    learning_rate=learning_rate,\n",
        "    ent_coef=entropy_coef,\n",
        "    clip_grad_norm=clip_grad_norm,\n",
        "    epoch=epoch,\n",
        "    batch_size=batch_size,\n",
        "    ppo_eps=ppo_eps,\n",
        "    eta=eta,\n",
        "    use_cuda=use_cuda,\n",
        "    use_gae=use_gae,\n",
        "    use_noisy_net=use_noisy_net\n",
        ")\n",
        "\n",
        "if is_load_model:\n",
        "    if use_cuda:\n",
        "        agent.model.load_state_dict(torch.load(model_path))\n",
        "    else:\n",
        "        agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "\n",
        "works = []\n",
        "parent_conns = []\n",
        "child_conns = []\n",
        "for idx in range(num_worker):\n",
        "    parent_conn, child_conn = Pipe()\n",
        "    work = env(env_id, is_render, idx, child_conn)\n",
        "    work.start()\n",
        "    works.append(work)\n",
        "    parent_conns.append(parent_conn)\n",
        "    child_conns.append(child_conn)\n",
        "states = np.zeros([num_worker, 4, 84, 84])\n",
        "\n",
        "sample_episode = 0\n",
        "sample_rall = 0\n",
        "sample_step = 0\n",
        "sample_env_idx = 0\n",
        "sample_i_rall = 0\n",
        "global_update = 0\n",
        "global_step = 0\n",
        "\n",
        "# normalize obs\n",
        "print('Start to initailize observation normalization parameter.....')\n",
        "next_obs = []\n",
        "steps = 0\n",
        "while steps < pre_obs_norm_step:\n",
        "    steps += num_worker\n",
        "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
        "    for parent_conn, action in zip(parent_conns, actions):\n",
        "        parent_conn.send(action)\n",
        "    for parent_conn in parent_conns:\n",
        "        s, r, d, rd, lr = parent_conn.recv()\n",
        "        next_obs.append(s[:])\n",
        "next_obs = np.stack(next_obs)\n",
        "obs_rms.update(next_obs)\n",
        "print('End to initalize...')\n",
        "\n",
        "# while True:\n",
        "for x in range(1):\n",
        "    total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_values, total_policy = \\\n",
        "        [], [], [], [], [], [], [], [], []\n",
        "    global_step += (num_worker * num_step)\n",
        "    global_update += 1\n",
        "    # Step 1. n-step rollout\n",
        "    for _ in range(num_step):\n",
        "        actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
        "        for parent_conn, action in zip(parent_conns, actions):\n",
        "            parent_conn.send(action)\n",
        "        next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
        "        for parent_conn in parent_conns:\n",
        "            s, r, d, rd, lr = parent_conn.recv()\n",
        "            next_states.append(s)\n",
        "            rewards.append(r)\n",
        "            dones.append(d)\n",
        "            real_dones.append(rd)\n",
        "            log_rewards.append(lr)\n",
        "        next_states = np.stack(next_states)\n",
        "        rewards = np.hstack(rewards)\n",
        "        dones = np.hstack(dones)\n",
        "        real_dones = np.hstack(real_dones)\n",
        "        # total reward = int reward\n",
        "        intrinsic_reward = agent.compute_intrinsic_reward((states - obs_rms.mean) / np.sqrt(obs_rms.var), (next_states - obs_rms.mean) / np.sqrt(obs_rms.var), actions)\n",
        "        sample_i_rall += intrinsic_reward[sample_env_idx]\n",
        "        total_int_reward.append(intrinsic_reward)\n",
        "        total_state.append(states)\n",
        "        total_next_state.append(next_states)\n",
        "        total_reward.append(rewards)\n",
        "        total_done.append(dones)\n",
        "        total_action.append(actions)\n",
        "        total_values.append(value)\n",
        "        total_policy.append(policy)\n",
        "        states = next_states[:, :, :, :]\n",
        "        sample_rall += log_rewards[sample_env_idx]\n",
        "        sample_step += 1\n",
        "        if real_dones[sample_env_idx]:\n",
        "            sample_episode += 1\n",
        "            writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
        "            writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
        "            writer.add_scalar('data/step', sample_step, sample_episode)\n",
        "            sample_rall = 0\n",
        "            sample_step = 0\n",
        "            sample_i_rall = 0\n",
        "\n",
        "    # calculate last next value\n",
        "    _, value, _ = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
        "    total_values.append(value)\n",
        "    # --------------------------------------------------\n",
        "    total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "    total_next_state = np.stack(total_next_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "    total_action = np.stack(total_action).transpose().reshape([-1])\n",
        "    total_done = np.stack(total_done).transpose()\n",
        "    total_values = np.stack(total_values).transpose()\n",
        "    total_logging_policy = torch.stack(total_policy).view(-1, output_size).cpu().numpy()\n",
        "\n",
        "    # Step 2. calculate intrinsic reward\n",
        "    # running mean intrinsic reward\n",
        "    total_int_reward = np.stack(total_int_reward).transpose()\n",
        "    total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in total_int_reward.T])\n",
        "    mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
        "    reward_rms.update_from_moments(mean, std ** 2, count)\n",
        "\n",
        "    # normalize intrinsic reward\n",
        "    total_int_reward /= np.sqrt(reward_rms.var)\n",
        "    writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
        "    writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
        "    # -------------------------------------------------------------------------------------------\n",
        "    # logging Max action probability\n",
        "    writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
        "\n",
        "    # Step 3. make target and advantage\n",
        "    target, adv = make_train_data(total_int_reward, np.zeros_like(total_int_reward),\n",
        "                                    total_values, gamma, num_step, num_worker)\n",
        "\n",
        "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Step 5. Training!\n",
        "    agent.train_model((total_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "                        (total_next_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "                        target, total_action, adv, total_policy)\n",
        "\n",
        "    if global_step % (num_worker * num_step * 100) == 0:\n",
        "        print('Now Global Step :{}'.format(global_step))\n",
        "        torch.save(agent.model.state_dict(), model_path)\n",
        "        torch.save(agent.icm.state_dict(), icm_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n8Xg1ERAotDt",
        "outputId": "447f749f-b773-4a9f-fe8f-25a356832568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n",
            "Process AtariEnvironment-18:\n",
            "Traceback (most recent call last):\n",
            "Process AtariEnvironment-19:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "Process AtariEnvironment-22:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "Traceback (most recent call last):\n",
            "Process AtariEnvironment-24:\n",
            "IndexError: list index out of range\n",
            "Process AtariEnvironment-26:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "Process AtariEnvironment-29:\n",
            "Process AtariEnvironment-28:\n",
            "Process AtariEnvironment-30:\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "Process AtariEnvironment-27:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process AtariEnvironment-25:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to initailize observation normalization parameter.....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  File \"<ipython-input-6-b0698bc31eff>\", line 168, in run\n",
            "    s, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 88, in step\n",
            "    obs, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 72, in step\n",
            "    return self.env.step(ac)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
            "    observation, reward, done, info = self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "IndexError: list index out of range\n",
            "IndexError: list index out of range\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "IndexError: list index out of range\n",
            "IndexError: list index out of range\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "IndexError: list index out of range\n",
            "IndexError: list index out of range\n",
            "IndexError: list index out of range\n",
            "IndexError: list index out of range\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\", line 223, in step\n",
            "    action = self._action_set[action_ind]\n",
            "IndexError: list index out of range\n",
            "Process AtariEnvironment-20:\n",
            "Process AtariEnvironment-17:\n",
            "Process AtariEnvironment-21:\n",
            "Process AtariEnvironment-23:\n",
            "Process AtariEnvironment-31:\n",
            "Traceback (most recent call last):\n",
            "Process AtariEnvironment-32:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 183, in run\n",
            "    self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 183, in run\n",
            "    self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 183, in run\n",
            "    self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 183, in run\n",
            "    self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 183, in run\n",
            "    self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ab30af840add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparent_conn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparent_conns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mnext_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 206, in send\n",
            "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
            "    self._send(buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
            "    self._send(buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
            "    self._send(buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
            "    self._send(buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-6-b0698bc31eff>\", line 165, in run\n",
            "    action = self.child_conn.recv()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 398, in _send_bytes\n",
            "    self._send(buf)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### eval\n"
      ],
      "metadata": {
        "id": "ZjHSWERB6kP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/eval.py\n",
        "from torch.multiprocessing import Pipe\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "# env = gym.make(env_id)\n",
        "input_size = env.observation_space.shape  # 4\n",
        "output_size = env.action_space.n  # 2\n",
        "if 'Breakout' in env_id: output_size -= 1\n",
        "\n",
        "env.close()\n",
        "is_render = True\n",
        "model_path = 'models/{}.model'.format(env_id)\n",
        "predictor_path = 'models/{}.pred'.format(env_id)\n",
        "target_path = 'models/{}.target'.format(env_id)\n",
        "use_cuda = False\n",
        "num_worker = 1\n",
        "batch_size = int(num_step * num_worker / mini_batch)\n",
        "sticky_action = False\n",
        "\n",
        "agent = RNDAgent\n",
        "\n",
        "# env = AtariEnvironment\n",
        "env = MarioEnvironment\n",
        "\n",
        "\n",
        "agent = agent(\n",
        "    input_size,\n",
        "    output_size,\n",
        "    num_worker,\n",
        "    num_step,\n",
        "    gamma,\n",
        "    lam=lam,\n",
        "    learning_rate=learning_rate,\n",
        "    ent_coef=entropy_coef,\n",
        "    clip_grad_norm=clip_grad_norm,\n",
        "    epoch=epoch,\n",
        "    batch_size=batch_size,\n",
        "    ppo_eps=ppo_eps,\n",
        "    use_cuda=use_cuda,\n",
        "    use_gae=use_gae,\n",
        "    use_noisy_net=use_noisy_net\n",
        ")\n",
        "\n",
        "print('Loading Pre-trained model....')\n",
        "if use_cuda:\n",
        "    agent.model.load_state_dict(torch.load(model_path))\n",
        "    agent.rnd.predictor.load_state_dict(torch.load(predictor_path))\n",
        "    agent.rnd.target.load_state_dict(torch.load(target_path))\n",
        "else:\n",
        "    agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    agent.rnd.predictor.load_state_dict(torch.load(predictor_path, map_location='cpu'))\n",
        "    agent.rnd.target.load_state_dict(torch.load(target_path, map_location='cpu'))\n",
        "print('End load...')\n",
        "works = []\n",
        "parent_conns = []\n",
        "child_conns = []\n",
        "for idx in range(num_worker):\n",
        "    parent_conn, child_conn = Pipe()\n",
        "    work = env(env_id, is_render, idx, child_conn)\n",
        "    work.start()\n",
        "    works.append(work)\n",
        "    parent_conns.append(parent_conn)\n",
        "    child_conns.append(child_conn)\n",
        "states = np.zeros([num_worker, 4, 84, 84])\n",
        "steps = 0\n",
        "rall = 0\n",
        "rd = False\n",
        "intrinsic_reward_list = []\n",
        "while not rd:\n",
        "    steps += 1\n",
        "    actions, value_ext, value_int, policy = agent.get_action(np.float32(states) / 255.)\n",
        "    for parent_conn, action in zip(parent_conns, actions):\n",
        "        parent_conn.send(action)\n",
        "    next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
        "    for parent_conn in parent_conns:\n",
        "        s, r, d, rd, lr = parent_conn.recv()\n",
        "        rall += r\n",
        "        next_states = s.reshape([1, 4, 84, 84])\n",
        "        next_obs = s[3, :, :].reshape([1, 1, 84, 84])\n",
        "\n",
        "    # total reward = int reward + ext Reward\n",
        "    intrinsic_reward = agent.compute_intrinsic_reward(next_obs)\n",
        "    intrinsic_reward_list.append(intrinsic_reward)\n",
        "    states = next_states[:, :, :, :]\n",
        "\n",
        "    if rd:\n",
        "        intrinsic_reward_list = (intrinsic_reward_list - np.mean(intrinsic_reward_list)) / np.std(\n",
        "            intrinsic_reward_list)\n",
        "        with open('int_reward', 'wb') as f:\n",
        "            pickle.dump(intrinsic_reward_list, f)\n",
        "        steps = 0\n",
        "        rall = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "7GDUPTqPmgeA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
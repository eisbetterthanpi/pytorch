{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jcwleo-curiosity-driven-exploration-base.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/pytorch/blob/main/jcwleo_curiosity_driven_exploration_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "ezjFR5N1HT94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://github.com/jcwleo/curiosity-driven-exploration-pytorch\n",
        "!pip install gym_super_mario_bros nes_py\n",
        "!pip install tensorboardX\n",
        "# https://stackoverflow.com/questions/67808779/running-gym-atari-in-google-colab\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]\n"
      ],
      "metadata": {
        "id": "tWNUebWtGx4k",
        "outputId": "b0cead3e-1d67-4448-c2c2-c146405ed9b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym_super_mario_bros\n",
            "  Downloading gym_super_mario_bros-7.3.3-py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting nes_py\n",
            "  Downloading nes_py-8.1.9.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (4.64.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes_py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.9-cp37-cp37m-linux_x86_64.whl size=435703 sha256=947e5e5cbbee8aa8ef4520b638d224eb1688b3e61dc6f5aa2eedf65697307eca\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/9e/0284da6e21df2cf30d5ed201fe070a462a3b0e607f7e28efb1\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.3 nes-py-8.1.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=5252722e0ad023f94b1c5e9c048e57e61b25f2c26aefd164dabf01b6c1d52e9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### config"
      ],
      "metadata": {
        "id": "LzppoJah6gBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "# # https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/config.py\n",
        "# env_type = \"atari\"\n",
        "env_id = \"BreakoutNoFrameskip-v4\"\n",
        "max_step_per_episode = 4500\n",
        "learning_rate = 1e-4\n",
        "num_worker = 16\n",
        "num_step = 128\n",
        "gamma = 0.99\n",
        "lam = 0.95\n",
        "use_gae = True\n",
        "use_cuda = True\n",
        "use_noisy_net = False\n",
        "clip_grad_norm = 0.5\n",
        "entropy_coef = 0.001\n",
        "epoch=Epoch = 3\n",
        "mini_batch = 8\n",
        "ppo_eps = 0.1\n",
        "life_done = False\n",
        "pre_obs_norm_step = 10000\n",
        "eta=ETA = 1.\n"
      ],
      "metadata": {
        "id": "569WzGoSmbqt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### utils"
      ],
      "metadata": {
        "id": "advj-ZV96ef8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/utils.py\n",
        "import numpy as np\n",
        "\n",
        "def make_train_data(reward, done, value, gamma, num_step, num_worker):\n",
        "    discounted_return = np.empty([num_worker, num_step])\n",
        "    # Discounted Return\n",
        "    if use_gae:\n",
        "        gae = np.zeros_like([num_worker, ])\n",
        "        for t in range(num_step - 1, -1, -1):\n",
        "            delta = reward[:, t] + gamma * value[:, t + 1] * (1 - done[:, t]) - value[:, t]\n",
        "            gae = delta + gamma * lam * (1 - done[:, t]) * gae\n",
        "            discounted_return[:, t] = gae + value[:, t]\n",
        "            # For Actor\n",
        "        adv = discounted_return - value[:, :-1]\n",
        "\n",
        "    else:\n",
        "        running_add = value[:, -1]\n",
        "        for t in range(num_step - 1, -1, -1):\n",
        "            running_add = reward[:, t] + gamma * running_add * (1 - done[:, t])\n",
        "            discounted_return[:, t] = running_add\n",
        "        # For Actor\n",
        "        adv = discounted_return - value[:, :-1]\n",
        "    return discounted_return.reshape([-1]), adv.reshape([-1])\n",
        "\n",
        "\n",
        "class RunningMeanStd(object):\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * (self.count)\n",
        "        m_b = batch_var * (batch_count)\n",
        "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
        "        new_var = M2 / (self.count + batch_count)\n",
        "        new_count = batch_count + self.count\n",
        "        self.mean = new_mean\n",
        "        self.var = new_var\n",
        "        self.count = new_count\n",
        "\n",
        "\n",
        "class RewardForwardFilter(object):\n",
        "    def __init__(self, gamma):\n",
        "        self.rewems = None\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def update(self, rews):\n",
        "        if self.rewems is None:\n",
        "            self.rewems = rews\n",
        "        else:\n",
        "            self.rewems = self.rewems * self.gamma + rews\n",
        "        return self.rewems\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis]  # dito\n",
        "    return e_x / div\n"
      ],
      "metadata": {
        "id": "jJR3qCEkmhaX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model"
      ],
      "metadata": {
        "id": "04hfoQXu6hUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/model.py\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.nn import init\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "    \"\"\"Factorised Gaussian NoisyNet\"\"\"\n",
        "    def __init__(self, in_features, out_features, sigma0=0.5):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.noisy_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.noisy_bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.noise_std = sigma0 / math.sqrt(self.in_features)\n",
        "        self.reset_parameters()\n",
        "        self.register_noise()\n",
        "\n",
        "    def register_noise(self):\n",
        "        in_noise = torch.FloatTensor(self.in_features)\n",
        "        out_noise = torch.FloatTensor(self.out_features)\n",
        "        noise = torch.FloatTensor(self.out_features, self.in_features)\n",
        "        self.register_buffer('in_noise', in_noise)\n",
        "        self.register_buffer('out_noise', out_noise)\n",
        "        self.register_buffer('noise', noise)\n",
        "\n",
        "    def sample_noise(self):\n",
        "        self.in_noise.normal_(0, self.noise_std)\n",
        "        self.out_noise.normal_(0, self.noise_std)\n",
        "        self.noise = torch.mm(\n",
        "            self.out_noise.view(-1, 1), self.in_noise.view(1, -1))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.noisy_weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "            self.noisy_bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Note: noise will be updated if x is not volatile\"\"\"\n",
        "        normal_y = nn.functional.linear(x, self.weight, self.bias)\n",
        "        if self.training:\n",
        "            # update the noise once per update\n",
        "            self.sample_noise()\n",
        "        noisy_weight = self.noisy_weight * self.noise\n",
        "        noisy_bias = self.noisy_bias * self.out_noise\n",
        "        noisy_y = nn.functional.linear(x, noisy_weight, noisy_bias)\n",
        "        return noisy_y + normal_y\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'in_features=' + str(self.in_features) \\\n",
        "               + ', out_features=' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class CnnActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size, use_noisy_net=False):\n",
        "        super(CnnActorCriticNetwork, self).__init__()\n",
        "        if use_noisy_net:\n",
        "            print('use NoisyNet')\n",
        "            linear = NoisyLinear\n",
        "        else:\n",
        "            linear = nn.Linear\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            linear(7 * 7 * 64, 512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            linear(512, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            linear(512, output_size)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            linear(512, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            linear(512, 1)\n",
        "        )\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "        for i in range(len(self.actor)):\n",
        "            if type(self.actor[i]) == nn.Linear:\n",
        "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
        "                self.actor[i].bias.data.zero_()\n",
        "        for i in range(len(self.critic)):\n",
        "            if type(self.critic[i]) == nn.Linear:\n",
        "                init.orthogonal_(self.critic[i].weight, 0.01)\n",
        "                self.critic[i].bias.data.zero_()\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.feature(state)\n",
        "        policy = self.actor(x)\n",
        "        value = self.critic(x)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "class ICMModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, use_cuda=True):\n",
        "        super(ICMModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "        feature_output = 7 * 7 * 64\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.LeakyReLU(),\n",
        "            Flatten(),\n",
        "            nn.Linear(feature_output, 512)\n",
        "        )\n",
        "\n",
        "        self.inverse_net = nn.Sequential(\n",
        "            nn.Linear(512 * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_size)\n",
        "        )\n",
        "        self.residual = [nn.Sequential(\n",
        "            nn.Linear(output_size + 512, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "        ).to(self.device)] * 8\n",
        "        self.forward_net_1 = nn.Sequential(\n",
        "            nn.Linear(output_size + 512, 512),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.forward_net_2 = nn.Sequential(\n",
        "            nn.Linear(output_size + 512, 512),\n",
        "        )\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.kaiming_uniform_(p.weight)\n",
        "                p.bias.data.zero_()\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.kaiming_uniform_(p.weight, a=1.0)\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        state, next_state, action = inputs\n",
        "        encode_state = self.feature(state)\n",
        "        encode_next_state = self.feature(next_state)\n",
        "        # get pred action\n",
        "        pred_action = torch.cat((encode_state, encode_next_state), 1)\n",
        "        pred_action = self.inverse_net(pred_action)\n",
        "        # ---------------------\n",
        "        # get pred next state\n",
        "        pred_next_state_feature_orig = torch.cat((encode_state, action), 1)\n",
        "        pred_next_state_feature_orig = self.forward_net_1(pred_next_state_feature_orig)\n",
        "        # residual\n",
        "        for i in range(4):\n",
        "            pred_next_state_feature = self.residual[i * 2](torch.cat((pred_next_state_feature_orig, action), 1))\n",
        "            pred_next_state_feature_orig = self.residual[i * 2 + 1](\n",
        "                torch.cat((pred_next_state_feature, action), 1)) + pred_next_state_feature_orig\n",
        "        pred_next_state_feature = self.forward_net_2(torch.cat((pred_next_state_feature_orig, action), 1))\n",
        "        real_next_state_feature = encode_next_state\n",
        "        return real_next_state_feature, pred_next_state_feature, pred_action\n",
        "\n"
      ],
      "metadata": {
        "id": "dOYA7QP3n6sK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### agents"
      ],
      "metadata": {
        "id": "skp0AbrR6g3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# agents\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/agents.py\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "class ICMAgent(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            output_size,\n",
        "            num_env,\n",
        "            num_step,\n",
        "            gamma,\n",
        "            lam=0.95,\n",
        "            learning_rate=1e-4,\n",
        "            ent_coef=0.01,\n",
        "            clip_grad_norm=0.5,\n",
        "            epoch=3,\n",
        "            batch_size=128,\n",
        "            ppo_eps=0.1,\n",
        "            eta=0.01,\n",
        "            use_gae=True,\n",
        "            use_cuda=False,\n",
        "            use_noisy_net=False):\n",
        "        self.model = CnnActorCriticNetwork(input_size, output_size, use_noisy_net)\n",
        "        self.num_env = num_env\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size\n",
        "        self.num_step = num_step\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gae = use_gae\n",
        "        self.ent_coef = ent_coef\n",
        "        self.eta = eta\n",
        "        self.ppo_eps = ppo_eps\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "        self.icm = ICMModel(input_size, output_size, use_cuda)\n",
        "        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.icm.parameters()), lr=learning_rate)\n",
        "        self.icm = self.icm.to(self.device)\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.Tensor(state).to(self.device)\n",
        "        state = state.float()\n",
        "        policy, value = self.model(state)\n",
        "        action_prob = F.softmax(policy, dim=-1).data.cpu().numpy()\n",
        "        action = self.random_choice_prob_index(action_prob)\n",
        "        return action, value.data.cpu().numpy().squeeze(), policy.detach()\n",
        "\n",
        "    @staticmethod\n",
        "    def random_choice_prob_index(p, axis=1):\n",
        "        r = np.expand_dims(np.random.rand(p.shape[1 - axis]), axis=axis)\n",
        "        return (p.cumsum(axis=axis) > r).argmax(axis=axis)\n",
        "\n",
        "    def compute_intrinsic_reward(self, state, next_state, action):\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        action = torch.LongTensor(action).to(self.device)\n",
        "        action_onehot = torch.FloatTensor(len(action), self.output_size).to(self.device)\n",
        "        action_onehot.zero_()\n",
        "        action_onehot.scatter_(1, action.view(len(action), -1), 1)\n",
        "        real_next_state_feature, pred_next_state_feature, pred_action = self.icm([state, next_state, action_onehot])\n",
        "        intrinsic_reward = self.eta * F.mse_loss(real_next_state_feature, pred_next_state_feature, reduction='none').mean(-1)\n",
        "        return intrinsic_reward.data.cpu().numpy()\n",
        "\n",
        "    def train_model(self, s_batch, next_s_batch, target_batch, y_batch, adv_batch, old_policy):\n",
        "        s_batch = torch.FloatTensor(s_batch).to(self.device)\n",
        "        next_s_batch = torch.FloatTensor(next_s_batch).to(self.device)\n",
        "        target_batch = torch.FloatTensor(target_batch).to(self.device)\n",
        "        y_batch = torch.LongTensor(y_batch).to(self.device)\n",
        "        adv_batch = torch.FloatTensor(adv_batch).to(self.device)\n",
        "        sample_range = np.arange(len(s_batch))\n",
        "        ce = nn.CrossEntropyLoss()\n",
        "        forward_mse = nn.MSELoss()\n",
        "        with torch.no_grad():\n",
        "            policy_old_list = torch.stack(old_policy).permute(1, 0, 2).contiguous().view(-1, self.output_size).to(self.device)\n",
        "            m_old = Categorical(F.softmax(policy_old_list, dim=-1))\n",
        "            log_prob_old = m_old.log_prob(y_batch)\n",
        "            # ------------------------------------------------------------\n",
        "        for i in range(self.epoch):\n",
        "            print(\"agent train i \",i)\n",
        "            np.random.shuffle(sample_range)\n",
        "            for j in range(int(len(s_batch) / self.batch_size)):\n",
        "                sample_idx = sample_range[self.batch_size * j:self.batch_size * (j + 1)]\n",
        "                # --------------------------------------------------------------------------------\n",
        "                # for Curiosity-driven\n",
        "                action_onehot = torch.FloatTensor(self.batch_size, self.output_size).to(self.device)\n",
        "                action_onehot.zero_()\n",
        "                action_onehot.scatter_(1, y_batch[sample_idx].view(-1, 1), 1)\n",
        "                real_next_state_feature, pred_next_state_feature, pred_action = self.icm(\n",
        "                    [s_batch[sample_idx], next_s_batch[sample_idx], action_onehot])\n",
        "                inverse_loss = ce(pred_action, y_batch[sample_idx])\n",
        "                forward_loss = forward_mse(pred_next_state_feature, real_next_state_feature.detach())\n",
        "                # ---------------------------------------------------------------------------------\n",
        "                policy, value = self.model(s_batch[sample_idx])\n",
        "                m = Categorical(F.softmax(policy, dim=-1))\n",
        "                log_prob = m.log_prob(y_batch[sample_idx])\n",
        "                ratio = torch.exp(log_prob - log_prob_old[sample_idx])\n",
        "                surr1 = ratio * adv_batch[sample_idx] \n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.ppo_eps, 1.0 + self.ppo_eps) * adv_batch[sample_idx]\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_loss = F.mse_loss(value.sum(1), target_batch[sample_idx])\n",
        "                entropy = m.entropy().mean()\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = (actor_loss + 0.5 * critic_loss - 0.001 * entropy) + forward_loss + inverse_loss\n",
        "                print(\"agent train loss \",loss)\n",
        "                loss.backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "xaYJferjmQNX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### envs"
      ],
      "metadata": {
        "id": "_6iGwOSX6h0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# envs\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/envs.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "from abc import abstractmethod\n",
        "from collections import deque\n",
        "from copy import copy\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "from torch.multiprocessing import Pipe, Process\n",
        "from PIL import Image\n",
        "\n",
        "class Environment(Process):\n",
        "    @abstractmethod\n",
        "    def run(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def pre_proc(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_init_state(self, x):\n",
        "        pass\n",
        "\n",
        "\n",
        "def unwrap(env):\n",
        "    if hasattr(env, \"unwrapped\"):\n",
        "        return env.unwrapped\n",
        "    elif hasattr(env, \"env\"):\n",
        "        return unwrap(env.env)\n",
        "    elif hasattr(env, \"leg_env\"):\n",
        "        return unwrap(env.leg_env)\n",
        "    else:\n",
        "        return env\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            # noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, is_render, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "        self.is_render = is_render\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if self.is_render:\n",
        "                self.env.render()\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class MontezumaInfoWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, room_address):\n",
        "        super(MontezumaInfoWrapper, self).__init__(env)\n",
        "        self.room_address = room_address\n",
        "        self.visited_rooms = set()\n",
        "\n",
        "    def get_current_room(self):\n",
        "        ram = unwrap(self.env).ale.getRAM()\n",
        "        assert len(ram) == 128\n",
        "        return int(ram[self.room_address])\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.visited_rooms.add(self.get_current_room())\n",
        "        if done:\n",
        "            if 'episode' not in info:\n",
        "                info['episode'] = {}\n",
        "            info['episode'].update(visited_rooms=copy(self.visited_rooms))\n",
        "            self.visited_rooms.clear()\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "\n",
        "class AtariEnvironment(Environment):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env_id,\n",
        "            is_render,\n",
        "            env_idx,\n",
        "            child_conn,\n",
        "            history_size=4,\n",
        "            h=84,\n",
        "            w=84,\n",
        "            life_done=True):\n",
        "        super(AtariEnvironment, self).__init__()\n",
        "        self.daemon = True\n",
        "        self.env = MaxAndSkipEnv(NoopResetEnv(gym.make(env_id)), is_render)\n",
        "        if 'Montezuma' in env_id:\n",
        "            self.env = MontezumaInfoWrapper(self.env, room_address=3 if 'Montezuma' in env_id else 1)\n",
        "        self.env_id = env_id\n",
        "        self.is_render = is_render\n",
        "        self.env_idx = env_idx\n",
        "        self.steps = 0\n",
        "        self.episode = 0\n",
        "        self.rall = 0\n",
        "        self.recent_rlist = deque(maxlen=100)\n",
        "        self.child_conn = child_conn\n",
        "        self.history_size = history_size\n",
        "        self.history = np.zeros([history_size, h, w])\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.reset()\n",
        "\n",
        "    def run(self):\n",
        "        super(AtariEnvironment, self).run()\n",
        "        while True:\n",
        "            action = self.child_conn.recv()\n",
        "            if 'Breakout' in self.env_id:\n",
        "                action += 1\n",
        "            s, reward, done, info = self.env.step(action)\n",
        "            if max_step_per_episode < self.steps:\n",
        "                done = True\n",
        "            log_reward = reward\n",
        "            force_done = done\n",
        "            self.history[:3, :, :] = self.history[1:, :, :]\n",
        "            self.history[3, :, :] = self.pre_proc(s)\n",
        "            self.rall += reward\n",
        "            self.steps += 1\n",
        "            if done:\n",
        "                self.recent_rlist.append(self.rall)\n",
        "                print(\"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Visited Room: [{}]\".format(\n",
        "                    self.episode, self.env_idx, self.steps, self.rall, np.mean(self.recent_rlist),\n",
        "                    info.get('episode', {}).get('visited_rooms', {})))\n",
        "                self.history = self.reset()\n",
        "            self.child_conn.send([self.history[:, :, :], reward, force_done, done, log_reward])\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_action = 0\n",
        "        self.steps = 0\n",
        "        self.episode += 1\n",
        "        self.rall = 0\n",
        "        s = self.env.reset()\n",
        "        self.get_init_state(self.pre_proc(s))\n",
        "        return self.history[:, :, :]\n",
        "\n",
        "    def pre_proc(self, X):\n",
        "        X = np.array(Image.fromarray(X).convert('L')).astype('float32')\n",
        "        x = cv2.resize(X, (self.h, self.w))\n",
        "        return x\n",
        "\n",
        "    def get_init_state(self, s):\n",
        "        for i in range(self.history_size):\n",
        "            self.history[i, :, :] = self.pre_proc(s)\n",
        "\n",
        "\n",
        "class MarioEnvironment(Process):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env_id,\n",
        "            is_render,\n",
        "            env_idx,\n",
        "            child_conn,\n",
        "            history_size=4,\n",
        "            life_done=True,\n",
        "            h=84,\n",
        "            w=84, movement=COMPLEX_MOVEMENT, sticky_action=True,\n",
        "            p=0.25):\n",
        "        super(MarioEnvironment, self).__init__()\n",
        "        self.daemon = True\n",
        "        self.env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "        self.is_render = is_render\n",
        "        self.env_idx = env_idx\n",
        "        self.steps = 0\n",
        "        self.episode = 0\n",
        "        self.rall = 0\n",
        "        self.recent_rlist = deque(maxlen=100)\n",
        "        self.child_conn = child_conn\n",
        "        self.life_done = life_done\n",
        "        self.history_size = history_size\n",
        "        self.history = np.zeros([history_size, h, w])\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.reset()\n",
        "\n",
        "    def run(self):\n",
        "        super(MarioEnvironment, self).run()\n",
        "        while True:\n",
        "            action = self.child_conn.recv()\n",
        "            if self.is_render:\n",
        "                self.env.render()\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            # when Mario loses life, changes the state to the terminal state.\n",
        "            if self.life_done:\n",
        "                if self.lives > info['life'] and info['life'] > 0:\n",
        "                    force_done = True\n",
        "                    self.lives = info['life']\n",
        "                else:\n",
        "                    force_done = done\n",
        "                    self.lives = info['life']\n",
        "            else:\n",
        "                force_done = done\n",
        "            # reward range -15 ~ 15\n",
        "            log_reward = reward / 15\n",
        "            self.rall += log_reward\n",
        "            r = log_reward\n",
        "            self.history[:3, :, :] = self.history[1:, :, :]\n",
        "            self.history[3, :, :] = self.pre_proc(obs)\n",
        "            self.steps += 1\n",
        "            if done:\n",
        "                self.recent_rlist.append(self.rall)\n",
        "                print(\"[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Stage: {} current x:{}   max x:{}\".format(self.episode, self.env_idx, self.steps, self.rall, np.mean(self.recent_rlist), info['stage'], info['x_pos'], self.max_pos))\n",
        "                self.history = self.reset()\n",
        "            self.child_conn.send([self.history[:, :, :], r, force_done, done, log_reward])\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_action = 0\n",
        "        self.steps = 0\n",
        "        self.episode += 1\n",
        "        self.rall = 0\n",
        "        self.lives = 3\n",
        "        self.stage = 1\n",
        "        self.max_pos = 0\n",
        "        self.get_init_state(self.env.reset())\n",
        "        return self.history[:, :, :]\n",
        "\n",
        "    def pre_proc(self, X):\n",
        "        x = cv2.cvtColor(X, cv2.COLOR_RGB2GRAY) # grayscaling\n",
        "        x = cv2.resize(x, (self.h, self.w)) # resize\n",
        "        return x\n",
        "\n",
        "    def get_init_state(self, s):\n",
        "        for i in range(self.history_size):\n",
        "            self.history[i, :, :] = self.pre_proc(s)\n",
        "\n"
      ],
      "metadata": {
        "id": "u6A7eQqUmfmm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train"
      ],
      "metadata": {
        "id": "FD0PkqWp6iPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/train.py\n",
        "from torch.multiprocessing import Pipe\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "env = gym.make(env_id)\n",
        "# env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "input_size = env.observation_space.shape  # 4\n",
        "output_size = env.action_space.n  # 2\n",
        "if 'Breakout' in env_id: output_size -= 1\n",
        "env.close()\n",
        "\n",
        "is_load_model = False\n",
        "is_render = False\n",
        "model_path = 'models/{}.model'.format(env_id)\n",
        "icm_path = 'models/{}.icm'.format(env_id)\n",
        "writer = SummaryWriter()\n",
        "batch_size = int(num_step * num_worker / mini_batch)\n",
        "reward_rms = RunningMeanStd()\n",
        "obs_rms = RunningMeanStd(shape=(1, 4, 84, 84))\n",
        "discounted_reward = RewardForwardFilter(gamma)\n",
        "agent = ICMAgent\n",
        "\n",
        "env = AtariEnvironment\n",
        "# env = MarioEnvironment\n",
        "\n",
        "agent = agent(\n",
        "    input_size,\n",
        "    output_size,\n",
        "    num_worker,\n",
        "    num_step,\n",
        "    gamma,\n",
        "    lam=lam,\n",
        "    learning_rate=learning_rate,\n",
        "    ent_coef=entropy_coef,\n",
        "    clip_grad_norm=clip_grad_norm,\n",
        "    epoch=epoch,\n",
        "    batch_size=batch_size,\n",
        "    ppo_eps=ppo_eps,\n",
        "    eta=eta,\n",
        "    use_cuda=use_cuda,\n",
        "    use_gae=use_gae,\n",
        "    use_noisy_net=use_noisy_net\n",
        ")\n",
        "\n",
        "if is_load_model:\n",
        "    if use_cuda:\n",
        "        agent.model.load_state_dict(torch.load(model_path))\n",
        "    else:\n",
        "        agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "\n",
        "works = []\n",
        "parent_conns = []\n",
        "child_conns = []\n",
        "for idx in range(num_worker):\n",
        "    parent_conn, child_conn = Pipe()\n",
        "    work = env(env_id, is_render, idx, child_conn)\n",
        "    work.start()\n",
        "    works.append(work)\n",
        "    parent_conns.append(parent_conn)\n",
        "    child_conns.append(child_conn)\n",
        "states = np.zeros([num_worker, 4, 84, 84])\n",
        "\n",
        "sample_episode = 0\n",
        "sample_rall = 0\n",
        "sample_step = 0\n",
        "sample_env_idx = 0\n",
        "sample_i_rall = 0\n",
        "global_update = 0\n",
        "global_step = 0\n",
        "\n",
        "# normalize obs\n",
        "print('Start to initailize observation normalization parameter.....')\n",
        "next_obs = []\n",
        "steps = 0\n",
        "while steps < pre_obs_norm_step:\n",
        "    steps += num_worker\n",
        "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
        "    for parent_conn, action in zip(parent_conns, actions):\n",
        "        parent_conn.send(action)\n",
        "    for parent_conn in parent_conns:\n",
        "        s, r, d, rd, lr = parent_conn.recv()\n",
        "        next_obs.append(s[:])\n",
        "next_obs = np.stack(next_obs)\n",
        "obs_rms.update(next_obs)\n",
        "print('End to initalize...')\n",
        "\n",
        "# while True:\n",
        "for x in range(1):\n",
        "    total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_values, total_policy = \\\n",
        "        [], [], [], [], [], [], [], [], []\n",
        "    global_step += (num_worker * num_step)\n",
        "    global_update += 1\n",
        "    # Step 1. n-step rollout\n",
        "    for _ in range(num_step):\n",
        "        actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
        "        for parent_conn, action in zip(parent_conns, actions):\n",
        "            parent_conn.send(action)\n",
        "        next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
        "        for parent_conn in parent_conns:\n",
        "            s, r, d, rd, lr = parent_conn.recv()\n",
        "            next_states.append(s)\n",
        "            rewards.append(r)\n",
        "            dones.append(d)\n",
        "            real_dones.append(rd)\n",
        "            log_rewards.append(lr)\n",
        "        next_states = np.stack(next_states)\n",
        "        rewards = np.hstack(rewards)\n",
        "        dones = np.hstack(dones)\n",
        "        real_dones = np.hstack(real_dones)\n",
        "        # total reward = int reward\n",
        "        intrinsic_reward = agent.compute_intrinsic_reward((states - obs_rms.mean) / np.sqrt(obs_rms.var), (next_states - obs_rms.mean) / np.sqrt(obs_rms.var), actions)\n",
        "        sample_i_rall += intrinsic_reward[sample_env_idx]\n",
        "        total_int_reward.append(intrinsic_reward)\n",
        "        total_state.append(states)\n",
        "        total_next_state.append(next_states)\n",
        "        total_reward.append(rewards)\n",
        "        total_done.append(dones)\n",
        "        total_action.append(actions)\n",
        "        total_values.append(value)\n",
        "        total_policy.append(policy)\n",
        "        states = next_states[:, :, :, :]\n",
        "        sample_rall += log_rewards[sample_env_idx]\n",
        "        sample_step += 1\n",
        "        if real_dones[sample_env_idx]:\n",
        "            sample_episode += 1\n",
        "            writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
        "            writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
        "            writer.add_scalar('data/step', sample_step, sample_episode)\n",
        "            sample_rall = 0\n",
        "            sample_step = 0\n",
        "            sample_i_rall = 0\n",
        "\n",
        "    # calculate last next value\n",
        "    _, value, _ = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
        "    total_values.append(value)\n",
        "    # --------------------------------------------------\n",
        "    total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "    total_next_state = np.stack(total_next_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\n",
        "    total_action = np.stack(total_action).transpose().reshape([-1])\n",
        "    total_done = np.stack(total_done).transpose()\n",
        "    total_values = np.stack(total_values).transpose()\n",
        "    total_logging_policy = torch.stack(total_policy).view(-1, output_size).cpu().numpy()\n",
        "\n",
        "    # Step 2. calculate intrinsic reward\n",
        "    # running mean intrinsic reward\n",
        "    total_int_reward = np.stack(total_int_reward).transpose()\n",
        "    total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in total_int_reward.T])\n",
        "    mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
        "    reward_rms.update_from_moments(mean, std ** 2, count)\n",
        "\n",
        "    # normalize intrinsic reward\n",
        "    total_int_reward /= np.sqrt(reward_rms.var)\n",
        "    writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
        "    writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
        "    # -------------------------------------------------------------------------------------------\n",
        "    # logging Max action probability\n",
        "    writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
        "\n",
        "    # Step 3. make target and advantage\n",
        "    target, adv = make_train_data(total_int_reward, np.zeros_like(total_int_reward),\n",
        "                                    total_values, gamma, num_step, num_worker)\n",
        "\n",
        "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Step 5. Training!\n",
        "    agent.train_model((total_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "                        (total_next_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
        "                        target, total_action, adv, total_policy)\n",
        "\n",
        "    if global_step % (num_worker * num_step * 100) == 0:\n",
        "        print('Now Global Step :{}'.format(global_step))\n",
        "        torch.save(agent.model.state_dict(), model_path)\n",
        "        torch.save(agent.icm.state_dict(), icm_path)\n",
        "\n",
        "\n",
        "# [Episode 3(5)] Step: 133  Reward: 0.0  Recent Reward: 1.66  Visited Room: [{}]\n",
        "# End to initalize...\n",
        "# [Episode 4(3)] Step: 214  Reward: 2.0  Recent Reward: 1.25  Visited Room: [{}]\n",
        "# agent train i  0\n",
        "# agent train loss  tensor(583851.4375, device='cuda:0', grad_fn=<AddBackward0>)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8Xg1ERAotDt",
        "outputId": "37565cb9-920c-4ae9-f219-d7ad64f46f63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to initailize observation normalization parameter.....\n",
            "[Episode 1(2)] Step: 129  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(4)] Step: 132  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(6)] Step: 136  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(9)] Step: 137  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(5)] Step: 140  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(14)] Step: 152  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(1)] Step: 154  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(10)] Step: 156  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(0)] Step: 161  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(12)] Step: 164  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(13)] Step: 171  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(3)] Step: 180  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(11)] Step: 181  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 1(8)] Step: 213  Reward: 2.0  Recent Reward: 2.0  Visited Room: [{}]\n",
            "[Episode 1(7)] Step: 253  Reward: 3.0  Recent Reward: 3.0  Visited Room: [{}]\n",
            "[Episode 2(2)] Step: 140  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 1(15)] Step: 279  Reward: 3.0  Recent Reward: 3.0  Visited Room: [{}]\n",
            "[Episode 2(1)] Step: 127  Reward: 0.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(0)] Step: 131  Reward: 0.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(14)] Step: 141  Reward: 0.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(5)] Step: 158  Reward: 1.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(12)] Step: 141  Reward: 0.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(6)] Step: 175  Reward: 1.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(3)] Step: 139  Reward: 0.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 2(8)] Step: 139  Reward: 0.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 2(4)] Step: 267  Reward: 3.0  Recent Reward: 1.5  Visited Room: [{}]\n",
            "[Episode 3(2)] Step: 132  Reward: 0.0  Recent Reward: 0.0  Visited Room: [{}]\n",
            "[Episode 2(15)] Step: 125  Reward: 0.0  Recent Reward: 1.5  Visited Room: [{}]\n",
            "[Episode 2(10)] Step: 251  Reward: 3.0  Recent Reward: 1.5  Visited Room: [{}]\n",
            "[Episode 3(0)] Step: 126  Reward: 0.0  Recent Reward: 0.3333333333333333  Visited Room: [{}]\n",
            "[Episode 3(14)] Step: 130  Reward: 0.0  Recent Reward: 0.3333333333333333  Visited Room: [{}]\n",
            "[Episode 2(7)] Step: 170  Reward: 1.0  Recent Reward: 2.0  Visited Room: [{}]\n",
            "[Episode 3(6)] Step: 154  Reward: 1.0  Recent Reward: 0.6666666666666666  Visited Room: [{}]\n",
            "[Episode 3(3)] Step: 150  Reward: 1.0  Recent Reward: 0.6666666666666666  Visited Room: [{}]\n",
            "[Episode 2(13)] Step: 301  Reward: 4.0  Recent Reward: 2.5  Visited Room: [{}]\n",
            "[Episode 3(12)] Step: 177  Reward: 1.0  Recent Reward: 0.6666666666666666  Visited Room: [{}]\n",
            "[Episode 3(1)] Step: 214  Reward: 2.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 2(11)] Step: 332  Reward: 6.0  Recent Reward: 3.5  Visited Room: [{}]\n",
            "[Episode 2(9)] Step: 385  Reward: 6.0  Recent Reward: 3.0  Visited Room: [{}]\n",
            "[Episode 3(5)] Step: 226  Reward: 2.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 3(4)] Step: 132  Reward: 0.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 4(0)] Step: 123  Reward: 0.0  Recent Reward: 0.25  Visited Room: [{}]\n",
            "[Episode 4(14)] Step: 123  Reward: 0.0  Recent Reward: 0.25  Visited Room: [{}]\n",
            "[Episode 3(10)] Step: 142  Reward: 0.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 3(7)] Step: 131  Reward: 0.0  Recent Reward: 1.3333333333333333  Visited Room: [{}]\n",
            "[Episode 3(15)] Step: 163  Reward: 1.0  Recent Reward: 1.3333333333333333  Visited Room: [{}]\n",
            "[Episode 4(2)] Step: 181  Reward: 2.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "[Episode 4(12)] Step: 124  Reward: 0.0  Recent Reward: 0.5  Visited Room: [{}]\n",
            "End to initalize...\n",
            "[Episode 4(3)] Step: 180  Reward: 1.0  Recent Reward: 0.75  Visited Room: [{}]\n",
            "[Episode 3(9)] Step: 131  Reward: 0.0  Recent Reward: 2.0  Visited Room: [{}]\n",
            "[Episode 5(0)] Step: 125  Reward: 0.0  Recent Reward: 0.2  Visited Room: [{}]\n",
            "[Episode 4(1)] Step: 183  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 4(10)] Step: 129  Reward: 0.0  Recent Reward: 0.75  Visited Room: [{}]\n",
            "[Episode 4(6)] Step: 214  Reward: 3.0  Recent Reward: 1.25  Visited Room: [{}]\n",
            "[Episode 3(11)] Step: 167  Reward: 1.0  Recent Reward: 2.6666666666666665  Visited Room: [{}]\n",
            "[Episode 4(4)] Step: 155  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 4(5)] Step: 165  Reward: 1.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 4(15)] Step: 129  Reward: 0.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 4(7)] Step: 142  Reward: 0.0  Recent Reward: 1.0  Visited Room: [{}]\n",
            "[Episode 3(13)] Step: 262  Reward: 3.0  Recent Reward: 2.6666666666666665  Visited Room: [{}]\n",
            "[Episode 5(14)] Step: 189  Reward: 1.0  Recent Reward: 0.4  Visited Room: [{}]\n",
            "agent train i  0\n",
            "agent train loss  tensor(23614332., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(27641818., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(19460080., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(15277015., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(10530913., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(14870864., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(13929685., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(12812595., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train i  1\n",
            "agent train loss  tensor(13239339., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(8938019., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(11683875., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(11023595., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(15469097., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(8657640., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(10291980., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(9733784., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train i  2\n",
            "agent train loss  tensor(12565702., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(11459045., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(6609659., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(7283385., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(8822507., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(11295188., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(9986232., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "agent train loss  tensor(6340267.5000, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(agent.model.state_dict(), model_path)\n",
        "# torch.save(agent.icm.state_dict(), icm_path)\n",
        "torch.save(agent.model.state_dict(),\"agent.model\")\n",
        "torch.save(agent.icm.state_dict(),\"agent.icm\")"
      ],
      "metadata": {
        "id": "xqN2cnQaz1Li"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### eval\n"
      ],
      "metadata": {
        "id": "ZjHSWERB6kP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "# https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/master/eval.py\n",
        "from torch.multiprocessing import Pipe\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# env = JoypadSpace(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\n",
        "env = gym.make(env_id)\n",
        "input_size = env.observation_space.shape  # 4\n",
        "output_size = env.action_space.n  # 2\n",
        "if 'Breakout' in env_id: output_size -= 1\n",
        "env.close()\n",
        "\n",
        "is_render = True\n",
        "# model_path = 'models/{}.model'.format(env_id)\n",
        "# predictor_path = 'models/{}.pred'.format(env_id)\n",
        "# target_path = 'models/{}.target'.format(env_id)\n",
        "use_cuda = False\n",
        "num_worker = 1\n",
        "batch_size = int(num_step * num_worker / mini_batch)\n",
        "sticky_action = False\n",
        "\n",
        "# agent = RNDAgent\n",
        "agent = ICMAgent\n",
        "\n",
        "env = AtariEnvironment\n",
        "# env = MarioEnvironment\n",
        "\n",
        "agent = agent(\n",
        "    input_size,\n",
        "    output_size,\n",
        "    num_worker,\n",
        "    num_step,\n",
        "    gamma,\n",
        "    lam=lam,\n",
        "    learning_rate=learning_rate,\n",
        "    ent_coef=entropy_coef,\n",
        "    clip_grad_norm=clip_grad_norm,\n",
        "    epoch=epoch,\n",
        "    batch_size=batch_size,\n",
        "    ppo_eps=ppo_eps,\n",
        "    use_cuda=use_cuda,\n",
        "    use_gae=use_gae,\n",
        "    use_noisy_net=use_noisy_net\n",
        ")\n",
        "\n",
        "print('Loading Pre-trained model....')\n",
        "if use_cuda:\n",
        "    agent.model.load_state_dict(torch.load(model_path))\n",
        "    agent.rnd.predictor.load_state_dict(torch.load(predictor_path))\n",
        "    agent.rnd.target.load_state_dict(torch.load(target_path))\n",
        "else:\n",
        "    # agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    # agent.rnd.predictor.load_state_dict(torch.load(predictor_path, map_location='cpu'))\n",
        "    # agent.rnd.target.load_state_dict(torch.load(target_path, map_location='cpu'))\n",
        "    agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    agent.rnd.predictor.load_state_dict(torch.load(predictor_path, map_location='cpu'))\n",
        "    agent.rnd.target.load_state_dict(torch.load(target_path, map_location='cpu'))\n",
        "print('End load...')"
      ],
      "metadata": {
        "id": "7GDUPTqPmgeA",
        "outputId": "884db208-b556-47da-9770-9a973d3b1610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Pre-trained model....\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-17748fbf51f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/BreakoutNoFrameskip-v4.model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "works = []\n",
        "parent_conns = []\n",
        "child_conns = []\n",
        "for idx in range(num_worker):\n",
        "    parent_conn, child_conn = Pipe()\n",
        "    work = env(env_id, is_render, idx, child_conn)\n",
        "    work.start()\n",
        "    works.append(work)\n",
        "    parent_conns.append(parent_conn)\n",
        "    child_conns.append(child_conn)\n",
        "states = np.zeros([num_worker, 4, 84, 84])\n",
        "steps = 0\n",
        "rall = 0\n",
        "rd = False\n",
        "intrinsic_reward_list = []\n",
        "while not rd:\n",
        "    steps += 1\n",
        "    # actions, value_ext, value_int, policy = agent.get_action(np.float32(states) / 255.)\n",
        "    actions, value_ext, policy = agent.get_action(np.float32(states) / 255.)\n",
        "    for parent_conn, action in zip(parent_conns, actions):\n",
        "        parent_conn.send(action)\n",
        "    next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
        "    for parent_conn in parent_conns:\n",
        "        s, r, d, rd, lr = parent_conn.recv()\n",
        "        rall += r\n",
        "        next_states = s.reshape([1, 4, 84, 84])\n",
        "        next_obs = s[3, :, :].reshape([1, 1, 84, 84])\n",
        "\n",
        "    # total reward = int reward + ext Reward\n",
        "    intrinsic_reward = agent.compute_intrinsic_reward(next_obs)\n",
        "    intrinsic_reward_list.append(intrinsic_reward)\n",
        "    states = next_states[:, :, :, :]\n",
        "\n",
        "    if rd:\n",
        "        intrinsic_reward_list = (intrinsic_reward_list - np.mean(intrinsic_reward_list)) / np.std(\n",
        "            intrinsic_reward_list)\n",
        "        with open('int_reward', 'wb') as f:\n",
        "            pickle.dump(intrinsic_reward_list, f)\n",
        "        steps = 0\n",
        "        rall = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "7sFWornTviFt",
        "outputId": "0e59ba09-a6ca-49fc-e10b-0d03a3194253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6d9560b4c61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_conns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ]
    }
  ]
}